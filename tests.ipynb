{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import helper as hlp\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mpldates\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.colors import LogNorm\n",
    "%matplotlib inline\n",
    "\n",
    "import scipy.interpolate as sci\n",
    "import scipy.optimize as sco\n",
    "import scipy.stats as scs\n",
    "\n",
    "import json\n",
    "import datetime\n",
    "import pickle\n",
    "from astropy.time import Time as astrotime\n",
    "\n",
    "import sklearn.neighbors as skn\n",
    "import sklearn.model_selection as skms  # Newer version of grid_search\n",
    "\n",
    "from corner_hist import corner_hist\n",
    "from anapymods3.plots.general import split_axis, get_binmids, hist_marginalize\n",
    "\n",
    "# Some globals\n",
    "hoursindays = 24.\n",
    "secinday = hoursindays * 60. * 60. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Load IC86 data from epinat, which should be the usual IC86-I (2011) PS sample, but pull corrected and OneWeights corrected by number of events generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "exp, mc, livetime = hlp.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Get data livetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Generate from good run list as stated here:\n",
    "- http://icecube.wisc.edu/~coenders/html/build/html/ic86-bdt/muonL3.html\n",
    "- https://wiki.icecube.wisc.edu/index.php/IC86_I_Point_Source_Analysis/Data_and_Simulation\n",
    "\n",
    "It should be 332.61 days as stated by jefeintzeig and scoenders.\n",
    "We create one bin per included run, with exactly that width.\n",
    "Excluded runs are those with too high/low rate and without everything marked \"good\".\n",
    "\n",
    "Livetime ist a bit higher, because we used a newer runlist from iclive instead of the old non-json v1.4.\n",
    "See side test for that comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "run_list = hlp.get_run_list()\n",
    "run_dict = hlp.get_run_dict(run_list)\n",
    "inc_run_arr, _livetime = hlp.get_good_runs(run_dict)\n",
    "\n",
    "# We don't use this livetime, but the ones from runlist v1.4\n",
    "print(\"IC86-I livetime from iclive: \", _livetime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Bin BG according to runlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Each run is one bin in the bg rate vs time plot.\n",
    "The rate is normed to Hertz by dividing through the bin sizes in seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Store events in bins with run borders\n",
    "exp_times = exp[\"timeMJD\"]\n",
    "start_mjd = inc_run_arr[\"start_mjd\"]\n",
    "stop_mjd = inc_run_arr[\"stop_mjd\"]\n",
    "\n",
    "tot = 0\n",
    "evts_in_run = {}\n",
    "for start, stop , runid in zip(start_mjd, stop_mjd, inc_run_arr[\"runID\"]):\n",
    "    mask = (exp_times >= start) & (exp_times < stop)\n",
    "    evts_in_run[runid] = exp[mask]\n",
    "    tot += np.sum(mask)\n",
    "    \n",
    "# Crosscheck, if we got all events and counted nothing double\n",
    "print(\"Do we have all events? \", tot == len(exp))\n",
    "print(\"  Events selected : \", tot)\n",
    "print(\"  Events in exp   : \", len(exp))\n",
    "\n",
    "# Create binmids and histogram values in each bin\n",
    "binmids = 0.5 * (start_mjd + stop_mjd)\n",
    "h = np.zeros(len(binmids), dtype=np.float)\n",
    "\n",
    "for i, evts in enumerate(evts_in_run.values()):\n",
    "    h[i] = len(evts)\n",
    "    \n",
    "# Now remove the 120 runs with zero rate that come from the differences\n",
    "# in the runlist. See side_test for more\n",
    "m = (h > 0)\n",
    "print(\"\\nRuns with 0 events :\", np.sum(~m))\n",
    "print(\"Runtime in those runs: \", np.sum(inc_run_arr[\"stop_mjd\"][~m] -\n",
    "                                        inc_run_arr[\"start_mjd\"][~m]))\n",
    "\n",
    "# Remove all zero event runs (artifacts from new run list) and calc the rate\n",
    "stop_mjd, start_mjd = stop_mjd[m], start_mjd[m]\n",
    "h = h[m]\n",
    "\n",
    "print(\"\\nHave all events after removing zero rates? \", np.sum(h) == len(exp))\n",
    "print(\"  Events selected : \", int(np.sum(h)))\n",
    "print(\"  Events in exp   : \", len(exp))\n",
    "\n",
    "# Normalize to rate in Hz and calc yerrors for fitting later\n",
    "h /= ((stop_mjd - start_mjd) * secinday)\n",
    "binmids = binmids[m]\n",
    "yerr = np.sqrt(h) / np.sqrt((stop_mjd - start_mjd) * secinday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Plot runs\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "xerr = 0.5 * (stop_mjd - start_mjd)\n",
    "ax.errorbar(binmids, h, xerr=xerr, yerr=yerr, fmt=\",\")\n",
    "\n",
    "# Setup main axis\n",
    "ax.set_xlim(start_mjd[0], stop_mjd[-1])\n",
    "ax.set_ylim(0, None)\n",
    "ax.set_xlabel(\"MJD\")\n",
    "ax.set_ylabel(\"Rate in Hz\")\n",
    "# Rotate bottom labels if needed\n",
    "# def xlabels(x):\n",
    "#     return [\"{:5d}\".format(int(xi)) for xi in x]\n",
    "# ax.set_xticklabels(xlabels(ax.get_xticks()), rotation=60,\n",
    "#                    horizontalalignment=\"right\")\n",
    "\n",
    "# Second xaxis on top with month and year.\n",
    "# Convert MJD to datetimes, make dates for every month and convert to mjd\n",
    "# http://stackoverflow.com/questions/22696662/ \\\n",
    "#   python-list-of-first-day-of-month-for-given-period\n",
    "datetimes = astrotime(binmids, format=\"mjd\").to_datetime()\n",
    "dt, end = datetimes[0], datetimes[-1]\n",
    "datetimes_ticks = []\n",
    "while dt < end:\n",
    "    if not dt.month % 12:\n",
    "        dt = datetime.datetime(dt.year + 1, 1, 1)\n",
    "    else:\n",
    "        dt = datetime.datetime(dt.year, dt.month + 1, 1)\n",
    "    datetimes_ticks.append(dt)\n",
    "mjd_ticks = astrotime(datetimes_ticks, format=\"datetime\").mjd\n",
    "# New axis on top, make sure, we use the same range\n",
    "ax2 = ax.twiny()\n",
    "ax2.set_xlim(ax.get_xlim())\n",
    "ax2.set_xticks(mjd_ticks)\n",
    "ax2.set_xticklabels([dtt.strftime(\"%b '%y\") for dtt in datetimes_ticks],\n",
    "                    rotation=60, horizontalalignment=\"left\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Time dependent rate function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Note: I think it is unnecessary to use a time and declination dependent rate. The spatial part is injected from the data BG from KDE anyways. So we just need to have the rate to determine how much events we inject allsky.**\n",
    "\n",
    "Rate ist time dependent because of seasonal variation.\n",
    "We take this varariation into account by fitting a priodic function to the time resolved rate.\n",
    "\n",
    "The data is built by calculating the rate in each run as seen before.\n",
    "This rate is correctly normalized and smoothes local fluctuations.\n",
    "\n",
    "### Peridoc function with a weighted least squares fit\n",
    "\n",
    "See side_test for comparison to spline fits.\n",
    "The function is a simple sinus scalable by 4 parameters to fit the shape of the rates:\n",
    "\n",
    "$$\n",
    "    f(x) = a\\cdot \\sin(b\\cdot(x - c)) + d\n",
    "$$\n",
    "\n",
    "The least squares loss function is\n",
    "\n",
    "$$\n",
    "    R = \\sum_i (w_i(y_i - f(x_i)))^2\n",
    "$$\n",
    "\n",
    "Weights are standard deviations from poisson histogram error.\n",
    "\n",
    "$$\n",
    "    w_i = \\frac{1}{\\sigma_i}\n",
    "$$\n",
    "\n",
    "Seed values are estimated from plot rate vs time.\n",
    "\n",
    "- Period should be 365 days (MJD) because we have one year of data so we choose $b0 = 2\\pi/365$.\n",
    "- Amplitude is about $a_0=-0.0005$, because sinus seems to start with negative values.\n",
    "- The x-offset is choose as the first start date, to get the right order of magnitude.\n",
    "- The y-axis intersection $d$ schould be close to the weighted average, so we take this as a seed.\n",
    "\n",
    "The bounds are motivated as follows (and if we don't hit them, it's OK to use them).\n",
    "\n",
    "- Amplitude $a$ should be positive, this also resolves a degenracy between a-axis offset.\n",
    "- The period $b$ should scatter around one year, a period larger than +-1 half a year is unphysical.\n",
    "- The x-offset $c$ cannot be greater than the initial +- the period because we have a periodic function.\n",
    "- The y-axis offset $d$ is arbitrarily constrained, but as seen from the plot it should not exceed 0.1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def f(x, args):\n",
    "    a, b, c, d = args\n",
    "    return a * np.sin(b * (x - c)) + d\n",
    "\n",
    "def lstsq(pars, *args):\n",
    "    \"\"\"\n",
    "    Weighted leastsquares min sum((wi * (yi - fi))**2)\n",
    "    \"\"\"\n",
    "    # data x,y-values and weights are fixed\n",
    "    x, y, w = args[0], args[1], args[2]\n",
    "    # Params get fitted\n",
    "    a, b, c, d = pars[0], pars[1], pars[2], pars[3]\n",
    "    # Target function\n",
    "    f = a * np.sin(b * (x - c)) + d\n",
    "    # Least squares loss\n",
    "    return np.sum((w * (y - f))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Seed values from consideration above.\n",
    "a0 = -0.0005\n",
    "b0 = 2. * np.pi / 365.  # We could restrict the period to one yr exact.\n",
    "c0 = np.amin(start_mjd)\n",
    "d0 = np.average(h, weights=yerr**2)\n",
    "\n",
    "x0 = [a0, b0, c0, d0]\n",
    "# Bounds as explained above\n",
    "bounds = [[None, None], [0.5 * b0, 1.5 * b0], [c0 - b0, c0 + b0, ], [0, 0.01]]\n",
    "# x, y values, weights\n",
    "args = (binmids, h, 1. / yerr)\n",
    "\n",
    "res = sco.minimize(fun=lstsq, x0=x0, args=args, bounds=bounds)\n",
    "bf_pars = res.x\n",
    "\n",
    "print(\"Amplitude   : {: 13.5f} in Hz\".format(res.x[0]))\n",
    "print(\"Period (d)  : {: 13.5f} in days\".format(2 * np.pi / res.x[1]))\n",
    "print(\"Offset (MJD): {: 13.5f} in MJD\".format(res.x[2]))\n",
    "print(\"Avg. rate   : {: 13.5f} in Hz\".format(res.x[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Define the rate function:\n",
    "def rate(t):\n",
    "    return f(t, res.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Plot runs\n",
    "xerr = 0.5 * (stop_mjd - start_mjd)\n",
    "plt.errorbar(binmids, h, xerr=0, yerr=yerr, fmt=\",\")\n",
    "plt.ylim(0, None);\n",
    "\n",
    "# Plot fit\n",
    "x = np.linspace(start_mjd[0], stop_mjd[-1], 1000)\n",
    "y = rate(x)\n",
    "plt.plot(x, y, zorder=5)\n",
    "\n",
    "# Plot y shift dashed to see baseline or years average\n",
    "plt.axhline(bf_pars[3], 0, 1, color=\"C1\", ls=\"--\", label=\"\")\n",
    "\n",
    "plt.xlim(start_mjd[0], stop_mjd[-1])\n",
    "plt.xlabel(\"MJD\")\n",
    "plt.ylabel(\"Rate in Hz\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Let's make the BG pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Proceeding to section 6.3.1 Randomized BG Injection, p. 113.\n",
    "Mrichmann draws events by:\n",
    "\n",
    "1. Get number of bg events to be injected from a poisson distribution with expectation values drawn from the previously build bg temporal distribution.\n",
    "   $$\n",
    "   P_{\\langle n_B\\rangle}(N_m) = \\frac{\\langle n_B\\rangle^{N_m}}{N_m\\!}\\cdot \\exp(\\langle n_B\\rangle)\n",
    "   $$\n",
    "2. These events are then drawn from a 3D pdf in energy proxy, zenith proxy and sigma proxy.\n",
    "   He does it by dividing 10x10x10 bins, first selecting energy, then zenith in that energy bin, then sigma in that zenith bin.\n",
    "   \n",
    "Here we create a smooth PDF using a kernel density estimator and obtain a sample by running a MCMC chain to create a sample a priori.\n",
    "The bandwidth is set globally and cross validated to be robust.\n",
    "\n",
    "**Some note on `numpy.histogramdd`:**\n",
    "\n",
    "The input must be an array with shape (nDim, len(data)).\n",
    "\n",
    "Shape of h is the same as the number of bins in each dim: (50, 40, 10)\n",
    "So the first dimension picks a single logE slice -> h[i].shape = (40, 10)\n",
    "Second dim picks a dec slice -> h[:, i].shape = (50, 10)\n",
    "3rd picks a sigma slice -> h[:, :, i].shape = (50, 40)\n",
    "\n",
    "This is important: meshgrid repeats in second axis on first array xx.\n",
    "For the second array, the first axis is repeated.\n",
    "But h iterates over energy in 1st axis. So if we don't transpose, we have the whole histogram flipped! Compare to plot in mrcihmanns thesis (cos(zen))\n",
    "\n",
    "**Some notes on KDE:**\n",
    "\n",
    "Sebastian has already made a tool for adaptive and asymmetric KDE.\n",
    "1. The Kernel is the covariance matrix of the whole data set to regard different scales\n",
    "    + Note: This may only be a problem, if one dim is spread with peaks, while the other is wide spread only. Then we cannot scale the Kernel to small to fit the peaks because the smooth dimension is preventing that.\n",
    "2. Use Silvermans or Scotts rule as a first guess.\n",
    "3. Run a second pass and vary the local bandwidth according to the first guess local density.\n",
    "\n",
    "We could replace 1 and 2 by scaling the data with the inverse covariance and then using a cross validation to find the first guess bandwidth.\n",
    "Then using a second pass to vary locally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 3D histogram of BG data\n",
    "First we make a 3D histogram to better compare to mrichmann and to get an overview over the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# HANDTUNED scale parameter to \"fit\" KDE expectation to data...\n",
    "# TODO: Use Adaptive kernel width and asymmetric gaus kernels\n",
    "#       For sigma it might make sense to a take a restricted kernel [0, inf]\n",
    "fac_logE = 1.5\n",
    "fac_dec = 2.5\n",
    "fac_sigma = 2.\n",
    "\n",
    "logE = fac_logE * exp[\"logE\"]\n",
    "sigma = fac_sigma * np.rad2deg(exp[\"sigma\"])\n",
    "# np.cos(np.pi / 2. + exp[\"dec\"]); dec is for {sin(dec), dec, cos(zen)}\n",
    "dec = fac_dec * exp[\"dec\"]\n",
    "\n",
    "# Binning is rather arbitrary because we don't calc stuff with the hist\n",
    "bins = [50, 50, 50]\n",
    "# Range for sigma is picked by looking at the 1D distribution and cutting of\n",
    "# the tail. This will be covered by the KDE tail anyway. Rest is default\n",
    "r = [[np.amin(logE), np.amax(logE)],\n",
    "     [np.amin(dec), np.amax(dec)],\n",
    "     [0., fac_sigma * 5.]]\n",
    "\n",
    "sample = np.vstack((logE, dec, sigma)).T\n",
    "h, bins = np.histogramdd(sample=sample, bins=bins, range=r, normed=False)\n",
    "\n",
    "# Make bin mids for later use\n",
    "mids = []\n",
    "for b in bins:\n",
    "    mids.append(0.5 * (b[:-1] + b[1:]))\n",
    "\n",
    "# Make a nice corner plot\n",
    "fig, ax = corner_hist(h, bins=bins,\n",
    "                      label=[\"logE\", \"dec\", \"sigma deg\"],\n",
    "                      hist2D_args={\"cmap\": \"Greys\"},\n",
    "                      hist_args={\"color\":\"#353132\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Kernel Density Estimation\n",
    "\n",
    "We use scikit learn's cross validation with a gaussian kernel to get the most robust bandwidth.\n",
    "Then we integrate with the same binning as above and compare to the 3D histogram.\n",
    "\n",
    "This section relies heavily on [Jake van der Plas examples for KDE](https://jakevdp.github.io/blog/2013/12/01/kernel-density-estimation/).\n",
    "More info on how KDE cross validation works can be found in [Modern Nonparametric Methods](http://www2.stat.duke.edu/~wjang/teaching/S05-293/lecture/ch6.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# KDE CV is running on cluster and pickles the GridSearchCV\n",
    "fname = \"./kde_cv/KDE_model_selector_20_exp_IC86_I_followup_2nd_pass.pickle\"\n",
    "with open(fname, \"rb\") as f:\n",
    "    model_selector = pickle.load(f)\n",
    "\n",
    "kde = model_selector.best_estimator_\n",
    "bw = model_selector.best_params_[\"bandwidth\"]\n",
    "print(\"Best bandwidth : {:.3f}\".format(bw))\n",
    "\n",
    "# We maybe just want to stick with the slightly overfitting kernel to\n",
    "# be as close as possible to data\n",
    "OVERFIT = True\n",
    "if OVERFIT:\n",
    "    bw = 0.075\n",
    "    kde = skn.KernelDensity(bandwidth=bw, kernel=\"gaussian\", rtol=1e-8)\n",
    "\n",
    "print(\"Used bandwidth : {:.3f}\".format(bw))\n",
    "# Estimate pdf for data sample with best model\n",
    "kde.fit(sample)\n",
    "\n",
    "# Generate some BG samples to compare to the original data hist.\n",
    "# Use more statistics, histograms get normalized and we want the best estimate\n",
    "# for the pdf\n",
    "nsamples_kde = int(1e7)\n",
    "bg_samples = kde.sample(n_samples=nsamples_kde)\n",
    "\n",
    "# Make histogram with same binning as original data\n",
    "bg_h, bg_bins = np.histogramdd(sample=bg_samples, bins=bins, range=r, normed=True)\n",
    "\n",
    "fig, ax = corner_hist(bg_h, bins=bg_bins,\n",
    "                      label=[\"logE\", \"sin(dec)\", \"sigma deg\"],\n",
    "                      hist2D_args={\"cmap\": \"Greys\"},\n",
    "                      hist_args={\"color\":\"#353132\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Compare KDE to original data\n",
    "\n",
    "Make a ratio histogram of the KDE sample and the original data sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 2D marginalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create 2D hists, by leaving out one parameter\n",
    "xlabel = [\"scaled \" + s for s in [\"logE\", \"logE\", \"dec\"]]\n",
    "ylabel = [\"scaled \" + s for s in [\"dec\", \"sigma in °\", \"sigma in °\"]]\n",
    "\n",
    "for i, axes in enumerate([[0, 1], [0, 2], [1, 2]]):\n",
    "    _b = np.array(bins)\n",
    "    h_exp, b_exp = np.histogramdd(sample[:, axes],\n",
    "                                  bins=_b[axes], normed=True)\n",
    "    h_kde, b_kde = np.histogramdd(bg_samples[:, axes],\n",
    "                                  bins=_b[axes], normed=True)\n",
    "    \n",
    "    # KDE is expectation, but sampled with much more events.\n",
    "    # Weights would simply scale the total number of KDE events to match the\n",
    "    # number of original events. That would be the mean for the poisson\n",
    "    # distribution in each bin. So to get OK KDE expectation sqrt(n) errors\n",
    "    # in each bin, we divide not by the number of drawn KDE but by the number\n",
    "    # of original events.   \n",
    "    # Again shapes of meshgrid and hist are transposed\n",
    "    diffXX, _ = np.meshgrid(np.diff(_b[0]), np.diff(_b[1]))\n",
    "    norm_kde = len(exp) * diffXX.T\n",
    "    sigma_kde = np.sqrt(h_kde / norm_kde)\n",
    "\n",
    "    # Make 3 different diff/ratio hists to estimate KDE quality in\n",
    "    # 1D marginalization.\n",
    "    m = (h_exp > 0.)\n",
    "    ratio_h = np.zeros_like(h_exp)\n",
    "    ratio_h[m] = h_kde[m] / h_exp[m]\n",
    "\n",
    "    diff_h = h_kde - h_exp\n",
    "\n",
    "    m = (sigma_kde > 0.)\n",
    "    sigma_ratio_h = np.zeros_like(h_exp)\n",
    "    sigma_ratio_h[m] = (h_exp[m] - h_kde[m]) / sigma_kde[m]\n",
    "\n",
    "    # Bin mids and hist grid\n",
    "    _b = b_exp\n",
    "    m = get_binmids(_b)\n",
    "    xx, yy = map(np.ravel, np.meshgrid(m[0], m[1]))\n",
    "    \n",
    "    \n",
    "    # Big plot on the left and three right\n",
    "    fig = plt.figure(figsize=(10, 6))\n",
    "    gs = gridspec.GridSpec(3, 3)\n",
    "    axl = fig.add_subplot(gs[:, :2])\n",
    "    axrt = fig.add_subplot(gs[0, 2])\n",
    "    axrc = fig.add_subplot(gs[1, 2])\n",
    "    axrb = fig.add_subplot(gs[2, 2])\n",
    "    \n",
    "    # Steal space for colorbars\n",
    "    caxl = split_axis(axl, \"right\")\n",
    "    caxrt = split_axis(axrt, \"left\")\n",
    "    caxrc = split_axis(axrc, \"left\")\n",
    "    caxrb = split_axis(axrb, \"left\")\n",
    "\n",
    "    # Unset top and center xticklabels as they are shared with the bottom plot\n",
    "    axrt.set_xticklabels([])\n",
    "    axrc.set_xticklabels([])\n",
    "        \n",
    "    # Left: Difference over KDE sigma\n",
    "    # cbar_extr = max(np.amax(sigma_ratio_h),  # Center colormap to min/max\n",
    "    #                         abs(np.amin(sigma_ratio_h)))\n",
    "    _, _, _, imgl = axl.hist2d(xx, yy, bins=_b, weights=sigma_ratio_h.T.ravel(),\n",
    "                               cmap=\"seismic\", vmax=5, vmin=-5)\n",
    "    cbarl = plt.colorbar(cax=caxl, mappable=imgl)\n",
    "    axl.set_xlabel(xlabel[i])\n",
    "    axl.set_ylabel(ylabel[i])\n",
    "    axl.set_title(\"(exp - kde) / sigma_kde\")\n",
    "    \n",
    "    # Right top: Ratio\n",
    "    _, _, _, imgrt = axrt.hist2d(xx, yy, bins=_b, weights=ratio_h.T.ravel(),\n",
    "                                 cmap=\"seismic\", vmax=2, vmin=0);\n",
    "    cbarrt = plt.colorbar(cax=caxrt, mappable=imgrt)\n",
    "    axrt.set_title(\"kde / exp\")\n",
    "\n",
    "    # Right center: Data hist\n",
    "    _, _, _, imgrc = axrc.hist2d(xx, yy, bins=_b, weights=h_exp.T.ravel(),\n",
    "                                 cmap=\"Greys\", norm=LogNorm());\n",
    "    cbarrc = plt.colorbar(cax=caxrc, mappable=imgrc)\n",
    "    axrc.set_title(\"exp logscale\")\n",
    "\n",
    "    # Right bottom: KDE hist, same colorbar scale as on data\n",
    "    _, _, _, imgrb = axrb.hist2d(xx, yy, bins=_b, weights=h_kde.T.ravel(),\n",
    "                                 cmap=\"Greys\", norm=LogNorm());\n",
    "    # Set with same colormap as on data\n",
    "    imgrb.set_clim(cbarrc.get_clim())\n",
    "    cbarrb = plt.colorbar(cax=caxrb, mappable=imgrb)\n",
    "    axrb.set_title(\"kde logscale\")\n",
    "    \n",
    "    # Set tick and label positions\n",
    "    for ax in [caxrt, caxrc, caxrb]:\n",
    "        ax.yaxis.set_label_position(\"right\")\n",
    "        ax.yaxis.tick_left()\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 1D marginalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Pseudo smooth marginalization is done by sampling many point from KDE an\n",
    "# using a finely binned 1D histogram, so it looks smooth\n",
    "xlabel = [\"scaled \" + s for s in [\"logE\", \"dec\", \"sigma °\"]]\n",
    "\n",
    "for i, axes in enumerate([0, 1, 2]):\n",
    "    _b = np.array(bins)\n",
    "    h_exp, b_exp = np.histogram(sample[:, axes],\n",
    "                                bins=_b[axes], normed=True)\n",
    "    h_kde, b_kde = np.histogram(bg_samples[:, axes],\n",
    "                                bins=_b[axes], normed=True)\n",
    "    \n",
    "#     h_exp, b_exp = hist_marginalize(h, bins, axes=axes)\n",
    "#     h_kde, b_kde = hist_marginalize(bg_h, bg_bins, axes=axes)\n",
    "      \n",
    "    # KDE errorbars as in 2D case\n",
    "    norm_kde = len(exp) * np.diff(b_kde)\n",
    "    sigma_kde = np.sqrt(h_kde / norm_kde)\n",
    "\n",
    "    # Make 3 different diff/ratio hists to estimate KDE quality in\n",
    "    # 1D marginalization.\n",
    "    m = (h_exp > 0.)\n",
    "    ratio_h = np.zeros_like(h_exp)\n",
    "    ratio_h[m] = h_kde[m] / h_exp[m]\n",
    "\n",
    "    diff_h = h_kde - h_exp\n",
    "\n",
    "    m = (sigma_kde > 0.)\n",
    "    sigma_ratio_h = np.zeros_like(h_exp)\n",
    "    sigma_ratio_h[m] = (h_exp[m] - h_kde[m]) / sigma_kde[m]\n",
    "\n",
    "    # Bin mids\n",
    "    _b = b_exp\n",
    "    m = get_binmids([_b])[0]\n",
    "    \n",
    "    # Plot both and the ration normed. Big plot on the left and three right\n",
    "    fig = plt.figure(figsize=(10, 6))\n",
    "    gs = gridspec.GridSpec(3, 3)\n",
    "    axl = fig.add_subplot(gs[:, :2])\n",
    "    axrt = fig.add_subplot(gs[0, 2])\n",
    "    axrc = fig.add_subplot(gs[1, 2])\n",
    "    axrb = fig.add_subplot(gs[2, 2])\n",
    "\n",
    "    axrt.set_xticklabels([])\n",
    "    axrc.set_xticklabels([])\n",
    "\n",
    "    # Set ticks and labels right\n",
    "    for ax in [axrt, axrc, axrb]:\n",
    "        ax.yaxis.set_label_position(\"right\")\n",
    "        ax.yaxis.tick_right()\n",
    "\n",
    "    # Limits\n",
    "    for ax in [axl, axrt, axrc, axrb]:\n",
    "        ax.set_xlim(_b[0], _b[-1])\n",
    "        \n",
    "    # Main plot:\n",
    "    # Plot more dense to mimic a smooth curve\n",
    "    __h, __b = np.histogram(bg_samples[:, i], bins=500,\n",
    "                            range=[_b[0], _b[-1]], density=True)\n",
    "    __m = get_binmids([__b])[0]\n",
    "    axl.plot(__m, __h, lw=3, alpha=0.5)\n",
    "    \n",
    "    _ = axl.hist(m, bins=_b, weights=h_exp, label=\"exp\", histtype=\"step\",\n",
    "                 lw=2, color=\"k\")\n",
    "    _ = axl.errorbar(m, h_kde, yerr=sigma_kde, fmt=\",\", color=\"r\")\n",
    "    _ = axl.hist(m, bins=_b, weights=h_kde, label=\"kde\", histtype=\"step\",\n",
    "                 lw=2, color=\"r\")    \n",
    "    \n",
    "    axl.set_xlabel(xlabel[i])\n",
    "    axl.legend(loc=\"upper right\")\n",
    "\n",
    "    # Top right: Difference\n",
    "    _ = axrt.axhline(0, 0, 1, color=\"k\", ls=\"-\")\n",
    "    _ = axrt.hlines([-.02, -.01, .01, .02], _b[0], _b[-1],\n",
    "                    colors='#353132', linestyles='dashed')\n",
    "    _ = axrt.hist(m, bins=_b, weights=diff_h, histtype=\"step\", lw=2, color=\"r\")\n",
    "    axrt.set_ylim(-.05, +.05)\n",
    "    axrt.set_ylabel(\"kde - exp\")\n",
    "\n",
    "    # Center right: Ratio\n",
    "    _ = axrc.axhline(1, 0, 1, color=\"k\", ls=\"-\")\n",
    "    _ = axrc.hlines([0.8, 0.9, 1.1, 1.2], _b[0], _b[-1],\n",
    "                    colors='#353132', linestyles='dashed')\n",
    "    _ = axrc.hist(m, bins=_b, weights=ratio_h, histtype=\"step\", lw=2, color=\"r\")\n",
    "    axrc.set_ylim(.5, 1.5)\n",
    "    axrc.set_ylabel(\"kde / exp\")\n",
    "\n",
    "    # Bottom right: Ratio of diff to sigma of expectation\n",
    "    _ = axrb.axhline(0, 0, 1, color=\"k\", ls=\"-\")\n",
    "    _ = axrb.hlines([-2, -1, 1, 2], _b[0], _b[-1],\n",
    "                    colors='#353132', linestyles='dashed')\n",
    "    _ = axrb.hist(m, bins=_b, weights=sigma_ratio_h, histtype=\"step\", lw=2, color=\"r\")\n",
    "    axrb.set_ylim(-3, +3)\n",
    "    axrb.set_ylabel(\"(exp-kde)/sigma_kde\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Likelihoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define our Likelihoods.\n",
    "We are given a source event occurance (can be GRB, GW, HESE or anything else) at a given position in space and time.\n",
    "We want to search for a significant contribution of other events, within a predefined region in time and space around the source events.\n",
    "For this we need to derive the expected signal and background contributions in that frame.\n",
    "\n",
    "The Likelihood that describes this scenario can be derived from counting statistics.\n",
    "If we expect $n_S$ signal and $n_B$ background events in the given frame, then the probability of observing $N$ events is given by a poisson pdf:\n",
    "\n",
    "$$\n",
    "    P_\\text{Poisson}(N\\ |\\ n_S + n_B) = \\mathcal{L}(N | n_S, n_b) = \\frac{(n_S + n_B)^{-N}}{N!}\\cdot \\exp{-(n_S + n_B)}\n",
    "$$\n",
    "\n",
    "We want to fit for the number of signal events $n_S$ in the frame.\n",
    "But each event doesn_t have the same probability of contributing to either signal or background, because we don't have that information on a per event basis.\n",
    "So we include prior information on a per event basis to account for that.\n",
    "\n",
    "$$\n",
    "    \\mathcal{L}(N | n_S, n_B) = \\frac{(n_S + n_B)^{-N}}{N!}\\cdot \\exp{-(n_S + n_B)} \\cdot \\prod_{i=1}^N P_i\n",
    "$$\n",
    "\n",
    "Also the simple poisson pdf above only has one parameter, the total number of events, which can be fit for.\n",
    "So we need to resolve this degeneracy in $n_S$, $n_B$ by giving additional information.\n",
    "For that we include a weighted combination of the probability for an event to be signal, denoted by the PDF $S_i$ and for it to background, denoted by $B_i$.\n",
    "Because the simple counting probabilities are $n_S / (n_S + n_B)$ to count a signal event and likewise $n_B / (n_S + n_B)$ to count a background event we construct the per event prior $P_i$ as:\n",
    "\n",
    "$$\n",
    "    P_i = \\frac{n_S}{n_S + n_B}\\cdot S_i + \\frac{n_B}{n_S + n_B}\\cdot B_i\n",
    "        = \\frac{n_S \\cdot S_i + n_B \\cdot B_i}{n_S + n_B}\n",
    "$$\n",
    "\n",
    "Note, that for equal probabilities $S_i$ and $B_i$, we simply and up with the normal poisson counting statistic.\n",
    "\n",
    "Plugging that back into the likelihood we get:\n",
    "\n",
    "$$\n",
    "    \\mathcal{L}(N | n_S, n_B) = \\frac{(n_S + n_B)^{-N}}{N!}\\cdot \\exp{(-(n_S + n_B))} \\cdot \\prod_{i=1}^N \\frac{n_S \\cdot S_i + n_B \\cdot B_i}{n_S + n_B}\n",
    "$$\n",
    "\n",
    "Taking the natrual logarithm to get the log-likelihood we arrive at:\n",
    "\n",
    "$$\n",
    "    \\ln\\mathcal{L}(N | n_S, n_B) = -(n_S + n_B) -\\ln(N!) + \\sum_{i=1}^N \\ln((n_S + n_B) P_i)\n",
    "$$\n",
    "\n",
    "If we weight up $n_S$ then every events signal PDF is contributing a bit more than the background pdf.\n",
    "So the fitter tries to find the combination of $n_S$ and $n_B$ that maximizes the likelihood.\n",
    "\n",
    "To further simplify, we can use a measured and fixed background expectation rate $\\langle n_B\\rangle$ and fit only for the number of signal events.\n",
    "Then we only fit for the number of signal events $n_S$.\n",
    "The fixed background rate can be extracted from data by using the pdf of a larger timescale and average over that (or fit a function) to ensure that local fluctuations don't matter.\n",
    "\n",
    "Then we end up with our full Likelihood (the denominator in $P_i$ cancels with the term from the poisson PDF):\n",
    "\n",
    "$$\n",
    "    \\ln\\mathcal{L}(N | n_S) = -(n_S + \\langle n_B\\rangle) -\\ln(N!) + \\sum_{i=1}^N \\ln(n_S S_i + \\langle n_B\\rangle B_i)\n",
    "$$\n",
    "\n",
    "For the test statistic we want to test the hypothesis of having no signal $n_S=0$ vs. the alternative with a free parameter $n_S$:\n",
    "\n",
    "$$\n",
    "    \\Lambda = \\ln\\frac{\\mathcal(\\hat{n}_S)}{\\mathcal{n_S=0}}\n",
    "            = \\frac{-(\\hat{n}_S + \\langle n_B\\rangle) -\\ln(N!) + \\sum_{i=1}^N \\ln(\\hat{n}_S S_i + \\langle n_B\\rangle B_i)}{-\\langle n_B\\rangle -\\ln(N!) + \\sum_{i=1}^N \\ln(\\langle n_B\\rangle B_i)}\n",
    "            = -\\hat{n}_S + \\sum_{i=1}^N \\ln\\left( \\frac{\\hat{n}_S S_i}{\\langle n_B\\rangle B_i} + 1 \\right)\n",
    "$$\n",
    "\n",
    "The per event PDFs $S_i$ and $B_i$ can depend on arbitrary parameters.\n",
    "The common choise here is to use a time, energy proxy and spatial proxy depency which has most seperation power:\n",
    "\n",
    "$$\n",
    "    S_i(x_i, t_i, E_i) = S_T(t_i) \\cdot S_S(x_i) \\cdot S_E(E_i) \\\\ \n",
    "    B_i(x_i, t_i, E_i) = B_T(t_i) \\cdot B_S(x_i) \\cdot B_E(E_i) \n",
    "$$\n",
    "\n",
    "Because the Likelihood only contains ratios of the PDF, we only have to construct functions of the signal to background ratio for each time, spatial and energy distribution.\n",
    "\n",
    "For the energy PDFs $S_E, B_E$ we use a 2D representation in reconstructed energy and declination because this has the most seperation power (see coenders & skylab models).\n",
    "The spatial part $S_S, B_S$ is only depending on the distance from source to event, not on the absilute position on the sphere.\n",
    "The time part $S_T, B_T$ is equivalent to that, only using the distance in time between source event and event.\n",
    "\n",
    "**Note: It seems that in mrichmans analysis there has only been used a 1D energy only PDF. This lacks seperation power, when using both hemispheres, as in the southern sky the energy threshhold is much higher.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Time PDF ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "source": [
    "Background in uniformly distributed in the time window.\n",
    "Signal distribtution is falling off gaussian-like at both edges so normalization is different.\n",
    "So the ratio $S_T / B_T$ is simply the the signal pdf divided by the uniform normalization $1 / (t_1 - t_0)$ in the time frame.\n",
    "\n",
    "The signal PDFs written out explicitely, where $t_0$ is the source events time and $t$ the events time:\n",
    "\n",
    "$$\n",
    "    N \\cdot S_T(t, t_0) = \\begin{cases}\n",
    "                     \\frac{1}{\\sqrt{2\\pi}\\sigma_T}\\exp\\left(-\\frac{(t-T_0)^2}{2\\sigma_T^2}\n",
    "                     \\right)&\\quad\\mathrm{, if }\\ t \\in [a, T_0]\\\\                \n",
    "                     \\frac{1}{\\sqrt{2\\pi}\\sigma_T}&\\quad\\mathrm{, if }\\ t \\in [T_0, T_1]\\\\\n",
    "                     \\frac{1}{\\sqrt{2\\pi}\\sigma_T}\\exp\\left(-\\frac{(t-T_1)^2}\n",
    "                     {2\\sigma_T^2}\\right)&\\quad\\mathrm{, if }\\ t \\in [T_1, b]\\\\ \n",
    "                    0 &\\quad\\mathrm{, else}\n",
    "                  \\end{cases}\n",
    "$$\n",
    "\n",
    "where $a, b$ are the bounds of the total time window, $T_0, T_1$ are the part, in which the signal is assumed to be uniformly distributed in time and $\\sigma_T$ is the width of the gaussian edges.\n",
    "The gaussian width $\\sigma_T$ is as wide as the interval $T_1-T_0$ but constraint to the nearest value in $[2, 30]$ seconds if the frame gets too large or too small.\n",
    "The total normalization $N$ is given by integrating over $S_T$ in $[a, b]$, resulting in:\n",
    "\n",
    "$$\n",
    "    N = \\Phi(b) - \\Phi(a) + \\frac{T_1-T_0}{\\sqrt{2\\pi}\\sigma_T}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "    \\Phi(x) = \\int_{-\\infty}^{x}\\frac{1}{\\sqrt{2\\pi}\\sigma_T}\n",
    "      \\exp\\left(-\\frac{(t-T_0)^2}{2\\sigma_T^2}\\right)\\mathrm{d}t\n",
    "$$\n",
    "the CDF of the gaussian PDF.\n",
    "\n",
    "The background PDF respectively is simply:\n",
    "\n",
    "$$\n",
    "    B_T(t, t_0) = \\begin{cases}\n",
    "                     \\frac{1}{b-a}&\\quad\\mathrm{, if }\\ t \\in [a, b]\\\\ \n",
    "                    0 &\\quad\\mathrm{, else}\n",
    "                  \\end{cases}    \n",
    "$$\n",
    "\n",
    "To get finite support we truncate the gaussian edges at $n\\cdot\\sigma_T$.\n",
    "Though arbitrarliy introduced the concrete cutoff of the doesn't really matter (so say 4, 5, 6 sigma, etc).\n",
    "\n",
    "This is because in the LLH we get the product of $\\langle b_B \\rangle B_i$.\n",
    "A larger cutoff make the normalization of the BG pdf larger, but in the same time makes the number of expected BG event get higher in the same linear fashion.\n",
    "So as long as we choose a cutoff which ensures that $S \\approx 0$ outside, we're good to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def time_soverb(t, t0, dt, nsig):\n",
    "    \"\"\"\n",
    "    Time signal over background PDF.\n",
    "    \n",
    "    Signal and background PDFs are each normalized over seconds.\n",
    "    Signal PDF has gaussian edges to smoothly let it fall of to zero, the\n",
    "    stddev is dt when dt is in [2, 30]s, otherwise the nearest edge.\n",
    "\n",
    "    To ensure finite support, the edges are truncated after nsig * dt.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    t : array-like\n",
    "        Times given in MJD for which we want to evaluate the ratio.\n",
    "    t0 : float\n",
    "        Time of the source event.\n",
    "    dt : float\n",
    "        Time window in seconds starting from t0 in which the signal pdf is\n",
    "        assumed to be uniform. Must not be negative.\n",
    "    nsig : float\n",
    "        Clip the gaussian edges at nsig * dt\n",
    "    \"\"\"\n",
    "    if dt < 0:\n",
    "        raise ValueError(\"dt must not be negative.\")\n",
    "\n",
    "    secinday = 24. * 60. * 60.\n",
    "\n",
    "    # Normalize relative to t0 in seconds (first multiply avoids rounding?)\n",
    "    _t = t * secinday - t0 * secinday\n",
    "   \n",
    "    # Create signal PDF\n",
    "    # Constrain sig_t to [2, 30]s regardless of uniform time window\n",
    "    sig_t = np.clip(dt, 2, 30)\n",
    "    sig_t_clip = nsig * sig_t\n",
    "    gaus_norm = (np.sqrt(2 * np.pi) * sig_t)\n",
    "    \n",
    "    # Split in def regions gaus rising, uniform, gaus falling\n",
    "    gr = (_t < 0) & (_t >= -sig_t_clip)\n",
    "    gf = (_t > dt) & (_t <= dt + sig_t_clip)\n",
    "    uni = (_t >= 0) & (_t <= dt)\n",
    "    \n",
    "    pdf = np.zeros_like(t, dtype=np.float)\n",
    "    pdf[gr] = scs.norm.pdf(_t[gr], loc=0, scale=sig_t)\n",
    "    pdf[gf] = scs.norm.pdf(_t[gf], loc=dt, scale=sig_t)\n",
    "    # Connect smoothly with the gaussians\n",
    "    pdf[uni] = 1. / gaus_norm\n",
    "    \n",
    "    # Normalize signal distribtuion\n",
    "    dcdf = (scs.norm.cdf(dt + sig_t_clip, loc=dt, scale=sig_t) -\n",
    "            scs.norm.cdf(-sig_t_clip, loc=0., scale=sig_t))\n",
    "    norm = dcdf + dt / gaus_norm\n",
    "    pdf /= norm\n",
    "    \n",
    "    # Calculate the ratio\n",
    "    bg_pdf = 1. / (dt + 2 * sig_t_clip)\n",
    "    ratio = pdf / bg_pdf\n",
    "    return ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Make a plot with ratios for different time windows as in the paper\n",
    "# Arbitrary start date from data\n",
    "t0 = start_mjd[100]\n",
    "t0_sec = t0 * secinday\n",
    "\n",
    "# dt from t0 in seconds, clip at 4 sigma\n",
    "dts = [5, 50, 200]\n",
    "nsig = 4\n",
    "\n",
    "# Make t values for plotting in MJD around t0, fitting all in one plot\n",
    "max_dt = np.amax(dts)\n",
    "clip = np.clip(max_dt, 2, 30) * nsig\n",
    "plt_rng = np.array([-clip, max_dt + clip])\n",
    "t = np.linspace(t0_sec + 1.2 *plt_rng[0],\n",
    "                t0_sec + 1.2 * plt_rng[1], 1000) / secinday\n",
    "_t = t * secinday - t0 * secinday\n",
    "\n",
    "# Mark event time\n",
    "plt.axvline(0, 0, 1, c=\"#353132\", ls=\"--\", lw=2)\n",
    "\n",
    "colors = [\"C0\", \"C3\", \"C2\"]\n",
    "for i, dt in enumerate(dts):\n",
    "    # Plot ratio S/B\n",
    "    SoB = time_soverb(t, t0, dt, nsig)\n",
    "    plt.plot(_t, SoB, lw=2, c=colors[i],\n",
    "             label=r\"$T_\\mathrm{{uni}}$: {:>3d}s\".format(dt))\n",
    "    # Fill uniform part, might look nicely\n",
    "    # fbtw = (_t > 0) & (_t < dt)\n",
    "    # plt.fill_between(_t[fbtw], 0, SoB[fbtw], color=\"C7\", alpha=0.1)\n",
    "\n",
    "# Make it look like the paper plot, but with slightly extended borders, to\n",
    "# nothing breaks outside the total time frame\n",
    "plt.xlim(1.2 * plt_rng)\n",
    "plt.ylim(0, 3)\n",
    "plt.xlabel(\"t - t0 in sec\")\n",
    "plt.ylabel(\"S / B\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.grid(ls=\"--\", lw=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Energy-Space Pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO skylab style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO skylab style, but with Kent PDF"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

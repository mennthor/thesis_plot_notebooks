{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mpldates\n",
    "%matplotlib inline\n",
    "import scipy.interpolate as sci\n",
    "import scipy.optimize as sco\n",
    "import json\n",
    "from astropy.time import Time as astrotime\n",
    "import datetime\n",
    "from corner_hist import corner_hist\n",
    "import sklearn.neighbors as skn\n",
    "import sklearn.model_selection as skms  # Newer version of grid_searchss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "\n",
    "Load IC86 data from epinat, which should be the usual IC86-I (2011) PS sample, but pull corrected and OneWeights corrected by number of events generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exp = np.load(\"data/IC86_I_data.npy\")\n",
    "mc = np.load(\"data/IC86_I_mc.npy\")\n",
    "\n",
    "# Use the officially stated livetime, not the ones from below\n",
    "livetime = 332.61"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data livetime\n",
    "\n",
    "Generate from good run list as stated here:\n",
    "- http://icecube.wisc.edu/~coenders/html/build/html/ic86-bdt/muonL3.html\n",
    "- https://wiki.icecube.wisc.edu/index.php/IC86_I_Point_Source_Analysis/Data_and_Simulation\n",
    "\n",
    "It should be 332.61 days as stated by jefeintzeig and scoenders.\n",
    "We create one bin per included run, with exactly that width.\n",
    "Excluded runs are those with too high/low rate and without everything marked \"good\".\n",
    "\n",
    "Livetime ist a bit higher, because we used a newer runlist from iclive instead of the old non-json v1.4.\n",
    "See cell fruther below for a script that parses that list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Grab from json\n",
    "jsonFile = open('data/ic86-i-goodrunlist.json', 'r')\n",
    "grlist = json.load(jsonFile)\n",
    "jsonFile.close()\n",
    "\n",
    "# This is a list of dicts (one dict per run)\n",
    "runs = grlist[\"runs\"]\n",
    "# This is a dict of arrays (all run values in an array per keyword)\n",
    "run_dict = dict(zip(runs[0].keys(), zip(*[r.values() for r in runs])))\n",
    "for k in run_dict.keys():\n",
    "    run_dict[k] = np.array(run_dict[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now compile runs as stated on jfeintzeigs page\n",
    "\n",
    "# Transform livetimes to MJD floats\n",
    "start_mjd = astrotime(run_dict[\"good_tstart\"]).mjd\n",
    "stop_mjd = astrotime(run_dict[\"good_tstop\"]).mjd\n",
    "\n",
    "# Create recarry to apply mask, only keep start, stop and runID\n",
    "dtype = [(\"start_mjd\", np.float), (\"stop_mjd\", np.float), (\"runID\", np.int)]\n",
    "run_arr = np.array(list(zip(start_mjd, stop_mjd, run_dict[\"run\"])), dtype=dtype)\n",
    "\n",
    "# Note: The last 2 runs aren't included anyway, so he left them out in\n",
    "# the reported run list. This fits here, as the other 4 runs are found\n",
    "# in the list.\n",
    "exclude_rate = [120028, 120029, 120030, 120087, 120156, 120157]\n",
    "i3good = run_dict[\"good_i3\"] == True\n",
    "itgood = run_dict[\"good_it\"] == True\n",
    "ratebad = np.in1d(run_dict[\"run\"], exclude_rate)\n",
    "\n",
    "# Include if it & i3 good and rate is good\n",
    "include = i3good & itgood & ~ratebad\n",
    "inc_run_arr = run_arr[include]\n",
    "\n",
    "# Get the total and per run livetimes in mjd\n",
    "runtimes_mjd = inc_run_arr[\"stop_mjd\"] - inc_run_arr[\"start_mjd\"]\n",
    "_livetime = np.sum(runtimes_mjd)\n",
    "\n",
    "print(\"IC86-I livetime from iclive: \", _livetime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare to the v1.4 list, as used by jfeintzig.\n",
    "Oddly we have 0.2 days less livetime as he had.\n",
    "The number of runs is correct though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For comparison, also parse the v1.4 list\n",
    "# Should be: 1081 runs, with a total livetime of 332.61 days.\n",
    "with open(\"data/Prelim_IC86-I_v1.4a.txt\",'r') as f:\n",
    "    data = []\n",
    "    for line in f.readlines():\n",
    "        data.append(line.replace('\\n',''))\n",
    "        \n",
    "# Skip to beginning of run info\n",
    "data = data[73:]\n",
    "\n",
    "# Split at white space\n",
    "data = [d.split() for d in data]\n",
    "\n",
    "dtype = [(\"runID\", np.int), (\"duration\", np.float), (\"IT\", \"|S2\"),\n",
    "         (\"CONF\", \"|S7\"), (\"FLAG\", \"|S6\")]\n",
    "runlist = np.empty((len(data),), dtype=dtype)\n",
    "\n",
    "runlist[\"runID\"] = np.array([int(d[0]) for d in data])\n",
    "runlist[\"duration\"] = np.array([float(d[3]) for d in data])\n",
    "runlist[\"IT\"] = np.array([d[5] for d in data])\n",
    "runlist[\"CONF\"] = np.array([d[6] for d in data])\n",
    "runlist[\"FLAG\"] = np.array([d[7] for d in data])\n",
    "\n",
    "# Now filter: Include IT=it, CONF=full, FLAG=GOOD, exclude strange rate runs\n",
    "exclude_rate = [120028, 120029, 120030, 120087, 120156, 120157]\n",
    "itgood = runlist[\"IT\"] == b\"IT\"  # Somehow only bitwise comparison is non-empty\n",
    "confgood = runlist[\"CONF\"] == b\"full\"\n",
    "flaggood = runlist[\"FLAG\"] == b\"GOOD\"\n",
    "ratebad = np.in1d(runlist[\"runID\"], exclude_rate)\n",
    "\n",
    "include = itgood & confgood & flaggood & ~ratebad\n",
    "runlist_inc = runlist[include]\n",
    "\n",
    "# Get the livetime of the sample in days\n",
    "hoursindays = 24.\n",
    "_livetime = np.sum(runlist_inc[\"duration\"]) / hoursindays\n",
    "\n",
    "print(\"Total runs from v1.4     : \", len(runlist_inc))\n",
    "print(\"Total livetime from v1.4 : \", _livetime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bin BG according to runlist\n",
    "\n",
    "Each run is one bin in the bg rate vs time plot.\n",
    "The rate is normed to Hertz by dividing through the bin sizes in seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store events in bins with run borders\n",
    "exp_times = exp[\"timeMJD\"]\n",
    "start_mjd = inc_run_arr[\"start_mjd\"]\n",
    "stop_mjd = inc_run_arr[\"stop_mjd\"]\n",
    "\n",
    "tot = 0\n",
    "evts_in_run = {}\n",
    "for start, stop , runid in zip(start_mjd, stop_mjd, inc_run_arr[\"runID\"]):\n",
    "    mask = (exp_times >= start) & ( exp_times < stop)\n",
    "    evts_in_run[runid] = exp[mask]\n",
    "    tot += np.sum(mask)\n",
    "    \n",
    "# Crosscheck, if we got all events and counted nothing double\n",
    "print(\"Do we have all events? \", tot == len(exp))\n",
    "print(\"  Events selected : \", tot)\n",
    "print(\"  Events in exp   : \", len(exp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binmids and histogram values in each bin\n",
    "binmids = 0.5 * (start_mjd + stop_mjd)\n",
    "h = np.zeros(len(binmids), dtype=np.float)\n",
    "\n",
    "for i, evts in enumerate(evts_in_run.values()):\n",
    "    h[i] = len(evts)\n",
    "    \n",
    "# Mask those with zero rate\n",
    "m = h > 0.\n",
    "binmids = binmids[m]\n",
    "h = h[m]\n",
    "    \n",
    "# Create plot arrays\n",
    "xerr = runtimes_mjd[m] / 2.\n",
    "yerr = np.sqrt(h)\n",
    "\n",
    "# Show in Hertz, so go from MJD days to seconds in bin widths\n",
    "secsinday = 24. * 60. * 60\n",
    "norm = (stop_mjd[m] - start_mjd[m]) * secsinday\n",
    "h_norm = h / norm\n",
    "# Poisson errors just get scaled\n",
    "yerr_norm = yerr / norm\n",
    "\n",
    "# Weights only for the weighted average\n",
    "weights = np.ones_like(yerr)\n",
    "weights[yerr_norm == 0] = 0\n",
    "weights[yerr_norm != 0] = 1 / yerr[yerr_norm != 0]\n",
    "def f(x, a, b, c):\n",
    "    \"\"\"Fix baseline to wighted average\"\"\"\n",
    "    return a * np.sin(b * (normed - c)) + np.average(h_norm, weights=weights)\n",
    "normed = (binmids - binmids.min()) / (binmids.max() - binmids.min())\n",
    "\n",
    "# Scaled seed from handcrafted guess in cell below\n",
    "p0 = [-0.0005, 2 * np.pi, 0.1]\n",
    "\n",
    "# Fit a poly to the rate. No weights, because we threw out entries with 0\n",
    "# Also with weight, the period is only have despite the good seed values...\n",
    "res = sco.curve_fit(f=f, xdata=normed, ydata=h_norm, p0=p0)\n",
    "pars = res[0]\n",
    "\n",
    "print(\"Best fit pars : \", pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot like mrichman did on p. 113\n",
    "# Note: Date plots are THE MOST DIFFICULT AND LEAST FUN THING TODO...\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "# Show dates on x axis\n",
    "datetimes = astrotime(binmids, format=\"mjd\").to_datetime()\n",
    "dates = mpldates.date2num([dt.date() for dt in datetimes])\n",
    "\n",
    "# Every month, first day\n",
    "months = mpldates.MonthLocator(bymonth=np.arange(1, 13), bymonthday=1)\n",
    "monthsFmt = mpldates.DateFormatter(\"%b %Y\")\n",
    "ax.xaxis.set_major_locator(months)\n",
    "ax.xaxis.set_major_formatter(monthsFmt)\n",
    "\n",
    "ax.errorbar(dates, h_norm, fmt=\".\", xerr=xerr, yerr=yerr_norm)\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Rate in HZ\")\n",
    "ax.set_xlim(dates[0], dates[-1])\n",
    "ax.set_ylim(0., None)\n",
    "\n",
    "# Plot polyfit\n",
    "delta_days = (datetimes[-1] - datetimes[0]).days\n",
    "xdatetimes = [datetimes[0] + datetime.timedelta(days=int(x))for x in\n",
    "              np.arange(0, delta_days)]\n",
    "xtimes_mjd = astrotime(xdatetimes).mjd\n",
    "normed = (xtimes_mjd - binmids.min()) / (binmids.max() - binmids.min())\n",
    "y = f(normed, *pars)\n",
    "\n",
    "# Handcrafted seed trial & error\n",
    "# s = [-0.0005, 2 * np.pi, 0.1]\n",
    "# y = s[0] * np.sin(s[1] * (normed + s[2])) + np.average(h_norm, weights=weights)\n",
    "\n",
    "# Convert back to mpl dates\n",
    "xdates = mpldates.date2num([xd.date() for xd in xdatetimes])\n",
    "ax.plot(xdates, y, \"r-\", zorder=5)\n",
    "ax.axhline(np.average(h_norm, weights=weights), 0, 1, color=\"k\", ls=\"--\", zorder=5)\n",
    "\n",
    "# Autoprettify main xlabels\n",
    "fig.autofmt_xdate(rotation=60)\n",
    "\n",
    "# Show mjd on top\n",
    "def ax2ticker(x):\n",
    "    dates = mpldates.num2date(x)\n",
    "    mjd = astrotime(dates).mjd\n",
    "    return mjd\n",
    "ax2 = ax.twiny()\n",
    "ax2.set_xticks(ax.get_xticks())\n",
    "ax2.set_xbound(ax.get_xbound())\n",
    "ax2.set_xticklabels(ax2ticker(ax.get_xticks()),\n",
    "                    rotation=60, horizontalalignment=\"left\")\n",
    "ax2.set_xlabel(\"MJD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Let's make the BG pdf\n",
    "\n",
    "Proceeding to section 6.3.1 Randomized BG Injection, p. 113.\n",
    "Mrichmann draws events by:\n",
    "\n",
    "1. Get number of bg events to be injected from a poisson distribution with expectation values drawn from the previously build bg temporal distribution.\n",
    "   $$\n",
    "   P_{\\langle n_B\\rangle}(N_m) = \\frac{\\langle n_B\\rangle^{N_m}}{N_m\\!}\\cdot \\exp(\\langle n_B\\rangle)\n",
    "   $$\n",
    "2. These events are then drawn from a 3D pdf in energy proxy, zenith proxy and sigma proxy.\n",
    "   He does it by dividing 10x10x10 bins, first selecting energy, then zenith in that energy bin, then sigma in that zenith bin.\n",
    "   \n",
    "Here we create a smooth PDF using a kernel density estimator and obtain a sample by running a MCMC chain to create a sample a priori.\n",
    "The bandwidth is set globally and cross validated to be robust.\n",
    "\n",
    "**Some note on `numpy.histogramdd`:**\n",
    "\n",
    "The input must be an array with shape (nDim, len(data)).\n",
    "\n",
    "Shape of h is the same as the number of bins in each dim: (50, 40, 10)\n",
    "So the first dimension picks a single logE slice -> h[i].shape = (40, 10)\n",
    "Second dim picks a dec slice -> h[:, i].shape = (50, 10)\n",
    "3rd picks a sigma slice -> h[:, :, i].shape = (50, 40)\n",
    "\n",
    "This is important: meshgrid repeats in second axis on first array xx.\n",
    "For the second array, the first axis is repeated.\n",
    "But h iterates over energy in 1st axis. So if we don't transpose, we have the whole histogram flipped! Compare to plot in mrcihmanns thesis (cos(zen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D histogram\n",
    "First we make a 3D histogram to better compare to mrichmann and to get an overview over the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's make the 3D histogram. We need to tune the bin sizes, so we\n",
    "# have enough statistics, but don't loose too much resolution.\n",
    "# Check with a difference histogram in the end.\n",
    "# np.cos(np.pi / 2. + exp[\"dec\"])  # cos(zenith)\n",
    "# np.sin(exp[\"dec\"])               # sin(dec)\n",
    "sample = np.vstack((exp[\"logE\"], np.sin(exp[\"dec\"]), np.rad2deg(exp[\"sigma\"]))).T\n",
    "h, (logE_bins, dec_bins, sigma_bins) = np.histogramdd(sample=sample,\n",
    "                                                      bins=[50, 40, 10])\n",
    "\n",
    "# Do it again to lazy-get bins and create ranges in sigma easily\n",
    "r = [[logE_bins[0], logE_bins[-1]],\n",
    "     [dec_bins[0], dec_bins[-1]],\n",
    "     [0, 5]]\n",
    "\n",
    "h, bins = np.histogramdd(sample=sample, bins=[50, 40, 20], range=r, normed=True)\n",
    "(logE_bins, dec_bins, sigma_bins) = bins\n",
    "\n",
    "fig, ax = corner_hist(h, bins=bins, label=[\"logE\", \"sin(dec)\", \"sigma deg\"],\n",
    "                      hist2D_args={\"cmap\": \"Greys\"}, hist_args={\"color\":\"#353132\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Kernel Density Estimation\n",
    "\n",
    "We use scikit learns cross validation with a gaussian kernel to get the most robust bandwidth.\n",
    "Then we integrate with the same binning as above and compare to the 3D histogram.\n",
    "\n",
    "This section relies heavily on [Jake van der Plas examples for KDE](https://jakevdp.github.io/blog/2013/12/01/kernel-density-estimation/).\n",
    "More info on how KDE cross validation works can be found in [Modern Nonparametric Methods](http://www2.stat.duke.edu/~wjang/teaching/S05-293/lecture/ch6.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optimize bandwidth in a 10-fold cross validation.\n",
    "# Our data has apprximately equal scales, which is good\n",
    "kde_estimator = skn.KernelDensity(kernel=\"gaussian\", rtol=1e-8)\n",
    "model_selector = skms.GridSearchCV(estimator=kde_estimator, cv=10,\n",
    "                        param_grid={\"bandwidth\" : np.arange(0.1, 2.1, 0.1)},)\n",
    "\n",
    "# fit takes the data points as shape=(n_samples, n_features), so exactly like\n",
    "# the already used sample in the np.histogramdd method\n",
    "model_selector.fit(sample)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "\n",
    "import helper as hlp\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mpldates\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.colors import LogNorm\n",
    "%matplotlib inline\n",
    "\n",
    "import scipy.interpolate as sci\n",
    "import scipy.optimize as sco\n",
    "import scipy.stats as scs\n",
    "import scipy.integrate as scint\n",
    "\n",
    "import json\n",
    "import datetime\n",
    "import pickle\n",
    "from astropy.time import Time as astrotime\n",
    "\n",
    "import sklearn.neighbors as skn\n",
    "import sklearn.model_selection as skms  # Newer version of grid_search\n",
    "\n",
    "from corner_hist import corner_hist\n",
    "from anapymods3.plots.general import (split_axis, get_binmids,\n",
    "                                      hist_marginalize, dg)\n",
    "from anapymods3.stats.sampling import rejection_sampling\n",
    "import anapymods3.stats.KDE as KDE\n",
    "\n",
    "# Some globals\n",
    "hoursindays = 24.\n",
    "secinday = hoursindays * 60. * 60. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Load IC86 data from epinat, which should be the usual IC86-I (2011) PS sample, but pull corrected and OneWeights corrected by number of events generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "exp, mc, livetime = hlp.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Get data livetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Generate from good run list as stated here:\n",
    "- http://icecube.wisc.edu/~coenders/html/build/html/ic86-bdt/muonL3.html\n",
    "- https://wiki.icecube.wisc.edu/index.php/IC86_I_Point_Source_Analysis/Data_and_Simulation\n",
    "\n",
    "It should be 332.61 days as stated by jefeintzeig and scoenders.\n",
    "We create one bin per included run, with exactly that width.\n",
    "Excluded runs are those with too high/low rate and without everything marked \"good\".\n",
    "\n",
    "Livetime ist a bit higher, because we used a newer runlist from iclive instead of the old non-json v1.4.\n",
    "See side test for that comparison.\n",
    "\n",
    "Problem is also, that this runlist includes runs with zero events, so they are probably cut out due to the old runlist in the original selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def filter_runs(run):\n",
    "    \"\"\"\n",
    "    Filter runs as stated in jfeintzig's doc.\n",
    "    \"\"\"\n",
    "    exclude_runs = [120028, 120029, 120030, 120087, 120156, 120157]\n",
    "    if ((run[\"good_i3\"] == True) & (run[\"good_it\"] == True) &\n",
    "        (run[\"run\"] not in exclude_runs)):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "goodrun_dict, _livetime = hlp.create_goodrun_dict(\n",
    "    runlist=\"data/runlists/ic86-i-goodrunlist.json\", filter_runs=filter_runs)\n",
    "\n",
    "# We don't use this livetime, but the \"official\" one from jfeintzeig's page\n",
    "print(\"IC86-I livetime from iclive: \", _livetime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Bin BG according to runlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Each run is one bin in the bg rate vs time plot.\n",
    "The rate is normed to Hertz by dividing through the bin sizes in seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "h = hlp._create_runtime_bins(exp[\"timeMJD\"], goodrun_dict=goodrun_dict,\n",
    "                             remove_zero_runs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Plot runs\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "start, stop = h[\"start_mjd\"], h[\"stop_mjd\"]\n",
    "rate = h[\"rate\"]\n",
    "xerr = 0.5 * (stop - start)\n",
    "yerr = h[\"rate_std\"]\n",
    "binmids = 0.5 * (stop + start)\n",
    "ax.errorbar(binmids, rate, xerr=xerr, yerr=yerr, fmt=\",\")\n",
    "\n",
    "# Setup main axis\n",
    "ax.set_xlim(start[0], stop[-1])\n",
    "ax.set_ylim(0, None)\n",
    "ax.set_xlabel(\"MJD\")\n",
    "ax.set_ylabel(\"Rate in Hz\")\n",
    "# Rotate bottom labels if needed\n",
    "# def xlabels(x):\n",
    "#     return [\"{:5d}\".format(int(xi)) for xi in x]\n",
    "# ax.set_xticklabels(xlabels(ax.get_xticks()), rotation=60,\n",
    "#                    horizontalalignment=\"right\")\n",
    "\n",
    "# Second xaxis on top with month and year.\n",
    "# Convert MJD to datetimes, make dates for every month and convert to mjd\n",
    "# http://stackoverflow.com/questions/22696662/ \\\n",
    "#   python-list-of-first-day-of-month-for-given-period\n",
    "datetimes = astrotime(binmids, format=\"mjd\").to_datetime()\n",
    "dt, end = datetimes[0], datetimes[-1]\n",
    "datetimes_ticks = []\n",
    "while dt < end:\n",
    "    if not dt.month % 12:\n",
    "        dt = datetime.datetime(dt.year + 1, 1, 1)\n",
    "    else:\n",
    "        dt = datetime.datetime(dt.year, dt.month + 1, 1)\n",
    "    datetimes_ticks.append(dt)\n",
    "mjd_ticks = astrotime(datetimes_ticks, format=\"datetime\").mjd\n",
    "\n",
    "# New axis on top, make sure, we use the same range\n",
    "ax2 = ax.twiny()\n",
    "ax2.set_xlim(ax.get_xlim())\n",
    "ax2.set_xticks(mjd_ticks)\n",
    "ax2.set_xticklabels([dtt.strftime(\"%b '%y\") for dtt in datetimes_ticks],\n",
    "                    rotation=60, horizontalalignment=\"left\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Time dependent rate function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Note: I think it is unnecessary to use a time and declination dependent rate. The spatial part is injected from the data BG from KDE anyways. So we just need to have the rate to determine how much events we inject allsky.**\n",
    "\n",
    "Rate ist time dependent because of seasonal variation.\n",
    "We take this varariation into account by fitting a priodic function to the time resolved rate.\n",
    "\n",
    "The data is built by calculating the rate in each run as seen before.\n",
    "This rate is correctly normalized and smoothes local fluctuations.\n",
    "\n",
    "### Peridoc function with a weighted least squares fit\n",
    "\n",
    "See side_test for comparison to spline fits.\n",
    "The function is a simple sinus scalable by 4 parameters to fit the shape of the rates:\n",
    "\n",
    "$$\n",
    "    f(x) = a\\cdot \\sin(b\\cdot(x - c)) + d\n",
    "$$\n",
    "\n",
    "The least squares loss function is\n",
    "\n",
    "$$\n",
    "    R = \\sum_i (w_i(y_i - f(x_i)))^2\n",
    "$$\n",
    "\n",
    "Weights are standard deviations from poisson histogram error.\n",
    "\n",
    "$$\n",
    "    w_i = \\frac{1}{\\sigma_i}\n",
    "$$\n",
    "\n",
    "Seed values are estimated from plot rate vs time.\n",
    "\n",
    "- Period should be 365 days (MJD) because we have one year of data so we choose $b0 = 2\\pi/365$.\n",
    "- Amplitude is about $a_0=-0.0005$, because sinus seems to start with negative values.\n",
    "- The x-offset is choose as the first start date, to get the right order of magnitude.\n",
    "- The y-axis intersection $d$ schould be close to the weighted average, so we take this as a seed.\n",
    "\n",
    "The bounds are motivated as follows (and if we don't hit them, it's OK to use them).\n",
    "\n",
    "- Amplitude $a$ should be positive, this also resolves a degenracy between a-axis offset.\n",
    "- The period $b$ should scatter around one year, a period larger than +-1 half a year is unphysical.\n",
    "- The x-offset $c$ cannot be greater than the initial +- the period because we have a periodic function.\n",
    "- The y-axis offset $d$ is arbitrarily constrained, but as seen from the plot it should not exceed 0.1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def f(x, pars):\n",
    "    \"\"\"\n",
    "    Returns the rate at a given time in MJD.\n",
    "    \"\"\"\n",
    "    a, b, c, d = pars\n",
    "    return a * np.sin(b * (x - c)) + d\n",
    "\n",
    "def lstsq(pars, *args):\n",
    "    \"\"\"\n",
    "    Weighted leastsquares min sum((wi * (yi - fi))**2)\n",
    "    \"\"\"\n",
    "    # data x,y-values and weights are fixed\n",
    "    x, y, w = args\n",
    "    _f = f(x, pars)\n",
    "    return np.sum((w * (y - _f))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Seed values from consideration above.\n",
    "# a0 = -0.0005\n",
    "# b0 = 2. * np.pi / 365.  # We could restrict the period to one yr exact.\n",
    "# c0 = np.amin(start_mjd)\n",
    "# d0 = np.average(h, weights=yerr**2)\n",
    "\n",
    "rate = h[\"rate\"]\n",
    "rate_std = h[\"rate_std\"]\n",
    "X = exp[\"timeMJD\"]\n",
    "binmids = 0.5 * (h[\"start_mjd\"] + h[\"stop_mjd\"])\n",
    "\n",
    "a0 = 0.5 * (np.amax(rate) - np.amin(rate))\n",
    "b0 = 2. * np.pi / 365.\n",
    "c0 = np.amin(X)\n",
    "d0 = np.average(rate, weights=rate_std**2)\n",
    "\n",
    "x0 = [a0, b0, c0, d0]\n",
    "# Bounds as explained above\n",
    "bounds = [[None, None], [0.5 * b0, 1.5 * b0], [c0 - b0, c0 + b0, ], [0, 0.01]]\n",
    "# x, y values, weights\n",
    "args = (binmids, rate, 1. / rate_std)\n",
    "\n",
    "res = sco.minimize(fun=lstsq, x0=x0, args=args, bounds=bounds)\n",
    "bf_pars = res.x\n",
    "\n",
    "print(\"Amplitude   : {: 13.5f} in Hz\".format(res.x[0]))\n",
    "print(\"Period (d)  : {: 13.5f} in days\".format(2 * np.pi / res.x[1]))\n",
    "print(\"Offset (MJD): {: 13.5f} in MJD\".format(res.x[2]))\n",
    "print(\"Avg. rate   : {: 13.5f} in Hz\".format(res.x[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Define the rate function:\n",
    "def rate_fun(t):\n",
    "    \"\"\"\n",
    "    Returns the rate at a given time in MJD.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    t : array-like\n",
    "        Time in MJD.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    rate : array-like\n",
    "        The rate of background events in Hz.\n",
    "    \"\"\"\n",
    "    return f(t, *res.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Plot runs\n",
    "start, stop = h[\"start_mjd\"], h[\"stop_mjd\"]\n",
    "rate = h[\"rate\"]\n",
    "xerr = 0.5 * (stop - start)\n",
    "yerr = h[\"rate_std\"]\n",
    "binmids = 0.5 * (stop + start)\n",
    "\n",
    "plt.errorbar(binmids, rate, xerr=xerr, yerr=yerr, fmt=\",\")\n",
    "plt.ylim(0, None);\n",
    "\n",
    "# Plot fit\n",
    "t = np.linspace(start[0], stop[-1], 1000)\n",
    "y = rate_fun(t)\n",
    "plt.plot(t, y, zorder=5)\n",
    "\n",
    "# Plot y shift dashed to see baseline or years average\n",
    "plt.axhline(bf_pars[3], 0, 1, color=\"C1\", ls=\"--\", label=\"\")\n",
    "\n",
    "plt.xlim(start[0], stop[-1])\n",
    "plt.xlabel(\"MJD\")\n",
    "plt.ylabel(\"Rate in Hz\")\n",
    "\n",
    "# plt.savefig(\"./data/figs/time_rate_sinus.png\", dpi=200)\n",
    "plt.ylim(0, 0.009)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draw Number of Background Events "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fitted spline the expected background at a given time.\n",
    "Draw the actual number of events to inject per BG trial within a given time window using a poisson distribution with the mean from the spline.\n",
    "\n",
    "Classically the events drawn are then assigned a random time within the time window.\n",
    "But as we have the rate function, we can sample times from that function using a rejection sampling.\n",
    "This will only affect larger intervals, where the curvature can be seen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample number of events in frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def _prep_times(t, trange):\n",
    "    \"\"\"\n",
    "    Little wrapper to not DRY.\n",
    "    \"\"\"\n",
    "    t = np.atleast_1d(t)\n",
    "    trange = np.array(trange)\n",
    "    nsrc = len(t)\n",
    "    \n",
    "    # Make shape (nsources, 1) for the times\n",
    "    t = t.reshape(nsrc, 1)\n",
    "    \n",
    "    # If range is 1D (one for all) reshape it to (nsources, 2)\n",
    "    if len(trange.shape) == 1:\n",
    "        trange = np.repeat(trange.reshape(1, 2), repeats=nsrc, axis=0)\n",
    "        \n",
    "    # Prepare time window in MJD\n",
    "    trange = t + trange / secinday\n",
    "    \n",
    "    return t, trange\n",
    "\n",
    "def get_num_of_bg_events(t, trange, ntrials, pars):\n",
    "    \"\"\"\n",
    "    Draw number of background events per trial from a poisson distribution\n",
    "    with the mean of the fitted rate function.\n",
    "    Then draw nevents times via rejection sampling for the time dpeendent rate\n",
    "    function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    t : array-like\n",
    "        Times of the occurance of each source event in MJD.\n",
    "    trange : [float, float] or array_like, shape (len(t), 2)\n",
    "        Time window(s) in seconds relativ to the given time(s) t.\n",
    "        - If [float, float], the same window [lower, upper] is used for every\n",
    "          source.\n",
    "        - If array-like, lower [i, 0] and upper [i, 1] bounds of the time\n",
    "          window per source.\n",
    "    ntrials : int\n",
    "        Number of background trials we need the number of how many events to\n",
    "        inject for.\n",
    "    pars : array-like\n",
    "        Best fit parameters from the fit function used in its integral.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    nevents : array-like, shape (len(t), ntrials)\n",
    "        The number of events to inject for each trial for each source.\n",
    "    \"\"\"\n",
    "    # Integrate rate function analytially in desired interval\n",
    "    def rate_integral(trange, pars):\n",
    "        \"\"\"\n",
    "        Match with factor [secinday] = 24 * 60 * 60 s / MJD = 86400/(Hz*MJD)\n",
    "        in the last step.\n",
    "            [a], [d] = Hz, [b], [c], [ti] = MJD\n",
    "            [a / b] = Hz * MJD, [d * (t1 - t0)] = HZ * MJD\n",
    "        \"\"\"\n",
    "        a, b, c, d = pars\n",
    "        \n",
    "        t0 = np.atleast_2d(trange[:, 0]).reshape(len(trange), 1)\n",
    "        t1 = np.atleast_2d(trange[:, 1]).reshape(len(trange), 1)\n",
    "        \n",
    "        per = a / b * (np.cos(b * (t0 - c)) - np.cos(b * (t1 - c)))\n",
    "        lin = d * (t1 - t0)\n",
    "\n",
    "        return (per + lin) * secinday\n",
    "    \n",
    "    t, trange = _prep_times(t, trange)\n",
    "    \n",
    "    # Expectation is the integral in the time frame\n",
    "    expect = rate_integral(trange, pars)\n",
    "        \n",
    "    # Sample from poisson\n",
    "    nevts = np.random.poisson(lam=expect, size=(len(t), ntrials))\n",
    "      \n",
    "    return nevts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start, stop = h[\"start_mjd\"], h[\"stop_mjd\"]\n",
    "\n",
    "t = start[100:104]\n",
    "trange = np.array([-120, 220])\n",
    "ntrials = 10\n",
    "\n",
    "nevts = get_num_of_bg_events(t, trange, ntrials, res.x)\n",
    "nevts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now the sampling of random times in the time frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we want to see, that all BG injected events stay in the correct time frame and make a uniform distribution for small time frames.\n",
    "\n",
    "Then we make the time window really big and the events should follow the rate function PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_times_in_frame(t, trange, nsamples):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    t : float\n",
    "        Time of the occurance of the source event in MJD.\n",
    "    trange : [float, float]\n",
    "        Time window in seconds relativ to the given time t.\n",
    "    nsamples : array-like, type int, shape (len(t))\n",
    "        Number of events to inject per trial. Number of trials is given by\n",
    "        the length of nsamples.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    times : list, length len(t)\n",
    "        List of samples times in MJD of background events per source.\n",
    "        For each source i nsamples[i] times are drawn from the rate function.\n",
    "    \"\"\"\n",
    "    _pdf = rate_fun\n",
    "    \n",
    "    t, trange = _prep_times(t, trange)\n",
    "    \n",
    "    sample = []\n",
    "    nsamples = np.atleast_1d(nsamples)\n",
    "    \n",
    "    for i, ni in enumerate(nsamples):\n",
    "        sam, _ = rejection_sampling(_pdf, bounds=trange, n=ni)\n",
    "        sample.append(sam)\n",
    "        \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# First the small time frame\n",
    "# Arbitrary start date from data\n",
    "t0 = start[100]\n",
    "t0_sec = t0 * secinday\n",
    "\n",
    "# dt from t0 in seconds, clip at 4 sigma\n",
    "dt = 200\n",
    "nsig = 4.\n",
    "\n",
    "# Make t values for plotting in MJD around t0\n",
    "clip = np.clip(dt, 2, 30) * nsig\n",
    "plt_rng = [-clip, dt + clip]\n",
    "trange = plt_rng\n",
    "ntrials = 10000\n",
    "\n",
    "# Sample times\n",
    "nevts = get_num_of_bg_events(t=t0, trange=trange, ntrials=ntrials,\n",
    "                             pars=res.x)[0]\n",
    "times = get_times_in_frame(t0, trange, nevts)\n",
    "\n",
    "# Plot them in together with the PDFs\n",
    "def time_bg_pdf(t, t0, a, b):\n",
    "    # Normalize relative to t0 in seconds (first multiply avoids rounding?)\n",
    "    _t = t * secinday - t0 * secinday\n",
    "  \n",
    "    pdf = np.zeros_like(_t, dtype=np.float)\n",
    "    uni = (_t >= a) & (_t <= b)\n",
    "    pdf[uni] = 1. / (b - a)\n",
    "    return pdf\n",
    "\n",
    "def time_sig_pdf(t, t0, dt, nsig=4):\n",
    "    if dt < 0:\n",
    "        raise ValueError(\"dt must not be negative.\")\n",
    "\n",
    "    # Normalize relative to t0 in seconds (first multiply avoids rounding?)\n",
    "    _t = t * secinday - t0 * secinday\n",
    "    \n",
    "    # Constrain sig_t to [2, 30]s regardless of uniform time window\n",
    "    sig_t = np.clip(dt, 2, 30)\n",
    "    sig_t_clip = nsig * sig_t\n",
    "    gaus_norm = (np.sqrt(2 * np.pi) * sig_t)\n",
    "    \n",
    "    # Split in def regions gaus rising, uniform, gaus falling and zero\n",
    "    gr = (_t < 0) & (_t >= -sig_t_clip)\n",
    "    gf = (_t > dt) & (_t <= dt + sig_t_clip)\n",
    "    uni = (_t >= 0) & (_t <= dt)\n",
    "    \n",
    "    pdf = np.zeros_like(t, dtype=np.float)\n",
    "    pdf[gr] = scs.norm.pdf(_t[gr], loc=0, scale=sig_t)\n",
    "    pdf[gf] = scs.norm.pdf(_t[gf], loc=dt, scale=sig_t)\n",
    "    # Connect smoothly with the gaussians\n",
    "    pdf[uni] = 1. / gaus_norm\n",
    "    \n",
    "    # Normalize whole distribtuion\n",
    "    dcdf = (scs.norm.cdf(dt + sig_t_clip, loc=dt, scale=sig_t) -\n",
    "            scs.norm.cdf(-sig_t_clip, loc=0., scale=sig_t))\n",
    "    norm = dcdf + dt / gaus_norm\n",
    "    \n",
    "    return pdf / norm\n",
    "\n",
    "\n",
    "# Plot the pdfs\n",
    "t = np.linspace(t0_sec + plt_rng[0], t0_sec + plt_rng[1], 200) / secinday\n",
    "bg_pdf = time_bg_pdf(t, t0, -clip, dt + clip)\n",
    "sig_pdf = time_sig_pdf(t, t0, dt, nsig)\n",
    "\n",
    "# Plot in normalized time\n",
    "_t = t * secinday - t0 * secinday\n",
    "plt.plot(_t, bg_pdf, \"C0-\")\n",
    "plt.plot(_t, sig_pdf, \"C1-\")\n",
    "plt.axvline(dt, 0, 1, color=\"C7\", ls=\"--\")\n",
    "plt.axvline(0, 0, 1, color=\"C1\", ls=\"--\")\n",
    "\n",
    "# Plot injected events from all trials\n",
    "T = np.array([])\n",
    "for ti in times:\n",
    "    T = np.append(T, ti)  \n",
    "T = (T - t0) * secinday\n",
    "\n",
    "_ = plt.hist(T, bins=50, normed=True, color=dg, alpha=.25)\n",
    "\n",
    "plt.xlabel(\"Time relative to t0 in sec\")\n",
    "plt.ylim(0, None);\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig(\"./data/figs/bg_events_time_sampled_narrow.png\", dpi=200)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now the really large time frame, over the whole time range\n",
    "t0 = start[0]\n",
    "t0_sec = t0 * secinday\n",
    "\n",
    "# dt from t0 in seconds, clip at 4 sigma\n",
    "dt = (stop[-1] - start[0]) * secinday\n",
    "nsig = 4.\n",
    "\n",
    "# Make t values for plotting in MJD around t0\n",
    "clip = np.clip(dt, 2, 30) * nsig\n",
    "plt_rng = [-clip, dt + clip]\n",
    "trange = plt_rng\n",
    "ntrials = 1\n",
    "\n",
    "# Sample times\n",
    "nevts = get_num_of_bg_events(t=t0, trange=trange, ntrials=ntrials,\n",
    "                             pars=res.x)[0]\n",
    "times = get_times_in_frame(t0, trange, nevts)\n",
    "\n",
    "# Plot injected events from all trials\n",
    "T = np.array([])\n",
    "for ti in times:\n",
    "    T = np.append(T, ti)  \n",
    "\n",
    "h, b = np.histogram(T, bins=1081)\n",
    "m = get_binmids([b])[0]\n",
    "scale = np.diff(b) * secinday * ntrials\n",
    "yerr = np.sqrt(h) / scale\n",
    "h = h / scale\n",
    "\n",
    "plt.errorbar(m, h, yerr=yerr, fmt=\",\")\n",
    "\n",
    "# Plot normalized rate function to compare\n",
    "t = np.linspace(start[0], stop[-1], 100)\n",
    "r = rate_fun(t=t)\n",
    "plt.plot(t, r, lw=2, zorder=5)\n",
    "plt.axhline(res.x[3], 0, 1, color=\"C1\", ls=\"--\", label=\"\", zorder=5)\n",
    "\n",
    "plt.xlim(start[0], stop[-1])\n",
    "plt.ylim(0, 0.009)\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig(\"./data/figs/bg_events_time_sampled_wide.png\", dpi=200)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Create the BG PDF from data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Proceeding to section 6.3.1 Randomized BG Injection, p. 113.\n",
    "Mrichmann draws events by:\n",
    "\n",
    "1. Get number of bg events to be injected from a poisson distribution with expectation values drawn from the previously build bg temporal distribution.\n",
    "   $$\n",
    "   P_{\\langle n_B\\rangle}(N_m) = \\frac{\\langle n_B\\rangle^{N_m}}{N_m\\!}\\cdot \\exp(\\langle n_B\\rangle)\n",
    "   $$\n",
    "2. These events are then drawn from a 3D pdf in energy proxy, zenith proxy and sigma proxy.\n",
    "   He does it by dividing 10x10x10 bins, first selecting energy, then zenith in that energy bin, then sigma in that zenith bin.\n",
    "   \n",
    "Here we create a smooth PDF using a kernel density estimator and obtain a sample by running a MCMC chain to create a sample a priori.\n",
    "The bandwidth is set globally and cross validated to be robust.\n",
    "\n",
    "**Some note on `numpy.histogramdd`:**\n",
    "\n",
    "The input must be an array with shape (nDim, len(data)).\n",
    "\n",
    "Shape of h is the same as the number of bins in each dim: (50, 40, 10)\n",
    "So the first dimension picks a single logE slice -> h[i].shape = (40, 10)\n",
    "Second dim picks a dec slice -> h[:, i].shape = (50, 10)\n",
    "3rd picks a sigma slice -> h[:, :, i].shape = (50, 40)\n",
    "\n",
    "This is important: meshgrid repeats in second axis on first array xx.\n",
    "For the second array, the first axis is repeated.\n",
    "But h iterates over energy in 1st axis. So if we don't transpose, we have the whole histogram flipped! Compare to plot in mrcihmanns thesis (cos(zen))\n",
    "\n",
    "**Some notes on KDE:**\n",
    "\n",
    "Sebastian has already made a tool for adaptive and asymmetric KDE.\n",
    "1. The Kernel is the covariance matrix of the whole data set to regard different scales\n",
    "    + Note: This may only be a problem, if one dim is spread with peaks, while the other is wide spread only. Then we cannot scale the Kernel to small to fit the peaks because the smooth dimension is preventing that.\n",
    "2. Use Silvermans or Scotts rule as a first guess.\n",
    "3. Run a second pass and vary the local bandwidth according to the first guess local density.\n",
    "\n",
    "We could replace 1 and 2 by scaling the data with the inverse covariance and then using a cross validation to find the first guess bandwidth.\n",
    "Then using a second pass to vary locally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 3D histogram of BG data\n",
    "First we make a 3D histogram to better compare to mrichmann and to get an overview over the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Cut sigmas in the sample to obtain smooth tail from KDE and remove outliers\n",
    "m = exp[\"sigma\"] < np.deg2rad(10)\n",
    "_logE = exp[\"logE\"][m]\n",
    "_dec = exp[\"dec\"][m]\n",
    "_sigma = exp[\"sigma\"][m]\n",
    "# Sample must match with the one used in training here\n",
    "sample = np.vstack((_logE, _dec, _sigma)).T\n",
    "\n",
    "# Binning is rather arbitrary because we don't calc stuff with the hist\n",
    "bins = [50, 50, 50]\n",
    "\n",
    "# Plot in degrees and in sinDec\n",
    "_sam = np.vstack((_logE, np.sin(_dec), np.rad2deg(_sigma))).T\n",
    "\n",
    "h, bins = np.histogramdd(sample=_sam, bins=bins, normed=False)\n",
    "\n",
    "# Make a nice corner plot\n",
    "label = [\"logE\", \"sinDdec\", \"sigma deg\"]\n",
    "fig, ax = corner_hist(h, bins=bins,\n",
    "                      label=label,\n",
    "                      hist2D_args={\"cmap\": \"inferno\", \"norm\": LogNorm()},\n",
    "                      hist_args={\"color\":\"C1\", \"alpha\": 0.5, \"log\": True})\n",
    "\n",
    "# plt.savefig(\"./data/figs/bg_corner_scaled.png\", dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Kernel Density Estimation\n",
    "\n",
    "Use adaptive width KDE to describe BG data and be able to smoothly draw new events from it.\n",
    "We fitted one set of params to the full data and stored it to avoid lengthy (~60 mins.) refitting when testing.\n",
    "A optimal set of parameters gets determined in a cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Assign model from CV, which has already evaluated adaptive kernels\n",
    "with open(\"data/awKDE_CV/CV10_a_b_EXP_IC86I_CUT_sig.ll.90_PARS_\" +\n",
    "          \"diag_True_alpha_0.5.pickle\", \"rb\") as f:\n",
    "    model_selector = pickle.load(f)\n",
    "print(\"Model selector used alpha : \", model_selector.best_estimator_.alpha)\n",
    "\n",
    "# We could still change the alpha, but the global bandwidth must stay fixed\n",
    "kde_inj = model_selector.best_estimator_\n",
    "kde_inj.alpha = .5\n",
    "print(\"Actually used alpha       : \", kde_inj.alpha)\n",
    "\n",
    "# Sample with bounds, because KDEs spillover\n",
    "bounds = np.array([[None, None], [-np.pi / 2. , np.pi / 2.], [0, None]])\n",
    "n_samples = int(1e6)\n",
    "kde_sam = kde_inj.sample(n_samples)\n",
    "\n",
    "\n",
    "# Plot in degrees and in sinDec\n",
    "_sam_kde = np.vstack((kde_sam[:, 0],\n",
    "                      np.sin(kde_sam[:, 1]),\n",
    "                      np.rad2deg(kde_sam[:, 2]))).T\n",
    "\n",
    "h, _ = np.histogramdd(sample=_sam_kde, bins=bins, normed=False)\n",
    "fig, ax = corner_hist(h, bins=bins,\n",
    "                      label=label,\n",
    "                      hist2D_args={\"cmap\": \"inferno\", \"norm\": LogNorm()},\n",
    "                      hist_args={\"color\":\"C1\", \"alpha\": 0.5, \"log\": True})\n",
    "\n",
    "# plt.savefig(\"./data/figs/bg_corner_scaled.png\", dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Compare KDE to original data\n",
    "\n",
    "Make a ratio histogram of the KDE sample and the original data sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 2D marginalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create 2D hists, by leaving out one parameter\n",
    "xlabel = [label[0], label[0], label[1]]\n",
    "ylabel = [label[1], label[2], label[2]]\n",
    "\n",
    "for i, axes in enumerate([[0, 1], [0, 2], [1, 2]]):\n",
    "    _b = np.array(bins)\n",
    "    h_exp, b_exp = np.histogramdd(_sam[:, axes],\n",
    "                                  bins=_b[axes], normed=True)\n",
    "    h_kde, b_kde = np.histogramdd(_sam_kde[:, axes],\n",
    "                                  bins=_b[axes], normed=True)\n",
    "    \n",
    "    # KDE is expectation, but sampled with much more events.\n",
    "    # Weights would simply scale the total number of KDE events to match the\n",
    "    # number of original events. That would be the mean for the poisson\n",
    "    # distribution in each bin. So to get OK KDE expectation sqrt(n) errors\n",
    "    # in each bin, we divide not by the number of drawn KDE but by the number\n",
    "    # of original events.   \n",
    "    # Again shapes of meshgrid and hist are transposed\n",
    "    diffXX, _ = np.meshgrid(np.diff(_b[0]), np.diff(_b[1]))\n",
    "    norm_kde = len(exp) * diffXX.T\n",
    "    sigma_kde = np.sqrt(h_kde / norm_kde)\n",
    "\n",
    "    # Make 3 different diff/ratio hists to estimate KDE quality in\n",
    "    # 1D marginalization.\n",
    "    m = (h_exp > 0.)\n",
    "    ratio_h = np.zeros_like(h_exp)\n",
    "    ratio_h[m] = h_kde[m] / h_exp[m]\n",
    "\n",
    "    diff_h = h_kde - h_exp\n",
    "\n",
    "    m = (sigma_kde > 0.)\n",
    "    sigma_ratio_h = np.zeros_like(h_exp)\n",
    "    sigma_ratio_h[m] = (h_exp[m] - h_kde[m]) / sigma_kde[m]\n",
    "\n",
    "    # Bin mids and hist grid\n",
    "    _b = b_exp\n",
    "    m = get_binmids(_b)\n",
    "    xx, yy = map(np.ravel, np.meshgrid(m[0], m[1]))\n",
    "    \n",
    "    \n",
    "    # Big plot on the left and three right\n",
    "    fig = plt.figure(figsize=(10, 6))\n",
    "    gs = gridspec.GridSpec(3, 3)\n",
    "    axl = fig.add_subplot(gs[:, :2])\n",
    "    axrt = fig.add_subplot(gs[0, 2])\n",
    "    axrc = fig.add_subplot(gs[1, 2])\n",
    "    axrb = fig.add_subplot(gs[2, 2])\n",
    "    \n",
    "    # Steal space for colorbars\n",
    "    caxl = split_axis(axl, \"right\")\n",
    "    caxrt = split_axis(axrt, \"left\")\n",
    "    caxrc = split_axis(axrc, \"left\")\n",
    "    caxrb = split_axis(axrb, \"left\")\n",
    "\n",
    "    # Unset top and center xticklabels as they are shared with the bottom plot\n",
    "    axrt.set_xticklabels([])\n",
    "    axrc.set_xticklabels([])\n",
    "        \n",
    "    # Left: Difference over KDE sigma\n",
    "    # cbar_extr = max(np.amax(sigma_ratio_h),  # Center colormap to min/max\n",
    "    #                         abs(np.amin(sigma_ratio_h)))\n",
    "    _, _, _, imgl = axl.hist2d(xx, yy, bins=_b, weights=sigma_ratio_h.T.ravel(),\n",
    "                               cmap=\"seismic\", vmax=5, vmin=-5)\n",
    "    cbarl = plt.colorbar(cax=caxl, mappable=imgl)\n",
    "    axl.set_xlabel(xlabel[i])\n",
    "    axl.set_ylabel(ylabel[i])\n",
    "    axl.set_title(\"(exp - kde) / sigma_kde\")\n",
    "    \n",
    "    # Right top: Ratio\n",
    "    _, _, _, imgrt = axrt.hist2d(xx, yy, bins=_b, weights=ratio_h.T.ravel(),\n",
    "                                 cmap=\"seismic\", vmax=2, vmin=0);\n",
    "    cbarrt = plt.colorbar(cax=caxrt, mappable=imgrt)\n",
    "    axrt.set_title(\"kde / exp\")\n",
    "\n",
    "    # Right center: Data hist\n",
    "    _, _, _, imgrc = axrc.hist2d(xx, yy, bins=_b, weights=h_exp.T.ravel(),\n",
    "                                 cmap=\"inferno\", norm=LogNorm());\n",
    "    cbarrc = plt.colorbar(cax=caxrc, mappable=imgrc)\n",
    "    axrc.set_title(\"exp logscale\")\n",
    "\n",
    "    # Right bottom: KDE hist, same colorbar scale as on data\n",
    "    _, _, _, imgrb = axrb.hist2d(xx, yy, bins=_b, weights=h_kde.T.ravel(),\n",
    "                                 cmap=\"inferno\", norm=LogNorm());\n",
    "    # Set with same colormap as on data\n",
    "    imgrb.set_clim(cbarrc.get_clim())\n",
    "    cbarrb = plt.colorbar(cax=caxrb, mappable=imgrb)\n",
    "    axrb.set_title(\"kde logscale\")\n",
    "    \n",
    "    # Set tick and label positions\n",
    "    for ax in [caxrt, caxrc, caxrb]:\n",
    "        ax.yaxis.set_label_position(\"right\")\n",
    "        ax.yaxis.tick_left()\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    plt.savefig(\"./data/figs/kde_data_2d_{}_{}.png\".format(\n",
    "                    xlabel[i], ylabel[i]),\n",
    "                dpi=200)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 1D marginalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Pseudo smooth marginalization is done by sampling many point from KDE an\n",
    "# using a finely binned 1D histogram, so it looks smooth\n",
    "xlabel = label\n",
    "\n",
    "for i, axes in enumerate([0, 1, 2]):\n",
    "    _b = np.array(bins)\n",
    "    h_exp, b_exp = np.histogram(_sam[:, axes],\n",
    "                                bins=_b[axes], normed=True)\n",
    "    h_kde, b_kde = np.histogram(_sam_kde[:, axes],\n",
    "                                bins=_b[axes], normed=True)\n",
    "    \n",
    "#     h_exp, b_exp = hist_marginalize(h, bins, axes=axes)\n",
    "#     h_kde, b_kde = hist_marginalize(bg_h, bg_bins, axes=axes)\n",
    "      \n",
    "    # KDE errorbars as in 2D case\n",
    "    norm_kde = len(exp) * np.diff(b_kde)\n",
    "    sigma_kde = np.sqrt(h_kde / norm_kde)\n",
    "\n",
    "    # Make 3 different diff/ratio hists to estimate KDE quality in\n",
    "    # 1D marginalization.\n",
    "    m = (h_exp > 0.)\n",
    "    ratio_h = np.zeros_like(h_exp)\n",
    "    ratio_h[m] = h_kde[m] / h_exp[m]\n",
    "\n",
    "    diff_h = h_kde - h_exp\n",
    "\n",
    "    m = (sigma_kde > 0.)\n",
    "    sigma_ratio_h = np.zeros_like(h_exp)\n",
    "    sigma_ratio_h[m] = (h_exp[m] - h_kde[m]) / sigma_kde[m]\n",
    "\n",
    "    # Bin mids\n",
    "    _b = b_exp\n",
    "    m = get_binmids([_b])[0]\n",
    "    \n",
    "    # Plot both and the ration normed. Big plot on the left and three right\n",
    "    fig = plt.figure(figsize=(10, 6))\n",
    "    gs = gridspec.GridSpec(3, 3)\n",
    "    axl = fig.add_subplot(gs[:, :2])\n",
    "    axrt = fig.add_subplot(gs[0, 2])\n",
    "    axrc = fig.add_subplot(gs[1, 2])\n",
    "    axrb = fig.add_subplot(gs[2, 2])\n",
    "\n",
    "    axrt.set_xticklabels([])\n",
    "    axrc.set_xticklabels([])\n",
    "\n",
    "    # Set ticks and labels right\n",
    "    for ax in [axrt, axrc, axrb]:\n",
    "        ax.yaxis.set_label_position(\"right\")\n",
    "        ax.yaxis.tick_right()\n",
    "\n",
    "    # Limits\n",
    "    for ax in [axl, axrt, axrc, axrb]:\n",
    "        ax.set_xlim(_b[0], _b[-1])\n",
    "        \n",
    "    # Main plot:\n",
    "    # Plot more dense to mimic a smooth curve\n",
    "    __h, __b = np.histogram(_sam_kde[:, i], bins=500,\n",
    "                            range=[_b[0], _b[-1]], density=True)\n",
    "    __m = get_binmids([__b])[0]\n",
    "    axl.plot(__m, __h, lw=3, alpha=0.5)\n",
    "    \n",
    "    _ = axl.hist(m, bins=_b, weights=h_exp, label=\"exp\", histtype=\"step\",\n",
    "                 lw=2, color=\"k\")\n",
    "    _ = axl.errorbar(m, h_kde, yerr=sigma_kde, fmt=\",\", color=\"r\")\n",
    "    _ = axl.hist(m, bins=_b, weights=h_kde, label=\"kde\", histtype=\"step\",\n",
    "                 lw=2, color=\"r\")    \n",
    "    \n",
    "    axl.set_xlabel(xlabel[i])\n",
    "    axl.legend(loc=\"upper right\")\n",
    "\n",
    "    # Top right: Difference\n",
    "    _ = axrt.axhline(0, 0, 1, color=\"k\", ls=\"-\")\n",
    "    _ = axrt.hlines([-.02, -.01, .01, .02], _b[0], _b[-1],\n",
    "                    colors='#353132', linestyles='dashed')\n",
    "    _ = axrt.hist(m, bins=_b, weights=diff_h, histtype=\"step\", lw=2, color=\"r\")\n",
    "    axrt.set_ylim(-.05, +.05)\n",
    "    axrt.set_ylabel(\"kde - exp\")\n",
    "\n",
    "    # Center right: Ratio\n",
    "    _ = axrc.axhline(1, 0, 1, color=\"k\", ls=\"-\")\n",
    "    _ = axrc.hlines([0.8, 0.9, 1.1, 1.2], _b[0], _b[-1],\n",
    "                    colors='#353132', linestyles='dashed')\n",
    "    _ = axrc.hist(m, bins=_b, weights=ratio_h, histtype=\"step\", lw=2, color=\"r\")\n",
    "    axrc.set_ylim(.5, 1.5)\n",
    "    axrc.set_ylabel(\"kde / exp\")\n",
    "\n",
    "    # Bottom right: Ratio of diff to sigma of expectation\n",
    "    _ = axrb.axhline(0, 0, 1, color=\"k\", ls=\"-\")\n",
    "    _ = axrb.hlines([-2, -1, 1, 2], _b[0], _b[-1],\n",
    "                    colors='#353132', linestyles='dashed')\n",
    "    _ = axrb.hist(m, bins=_b, weights=sigma_ratio_h, histtype=\"step\", lw=2, color=\"r\")\n",
    "    axrb.set_ylim(-3, +3)\n",
    "    axrb.set_ylabel(\"(exp-kde)/sigma_kde\")\n",
    "    \n",
    "    plt.savefig(\"./data/figs/kde_data_1d_{}.png\".format(\n",
    "            xlabel[i]), dpi=200)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Define the Likelihoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here we define our Likelihoods.\n",
    "We are given a source event occurance (can be GRB, GW, HESE or anything else) at a given position in space and time.\n",
    "We want to search for a significant contribution of other events, within a predefined region in time and space around the source events.\n",
    "For this we need to derive the expected signal and background contributions in that frame.\n",
    "\n",
    "The Likelihood that describes this scenario can be derived from counting statistics.\n",
    "If we expect $n_S$ signal and $n_B$ background events in the given frame, then the probability of observing $N$ events is given by a poisson pdf:\n",
    "\n",
    "$$\n",
    "    P_\\text{Poisson}(N\\ |\\Â n_S + n_B) = \\mathcal{L}(N | n_S, n_b) = \\frac{(n_S + n_B)^{-N}}{N!}\\cdot \\exp{-(n_S + n_B)}\n",
    "$$\n",
    "\n",
    "We want to fit for the number of signal events $n_S$ in the frame.\n",
    "But each event doesn_t have the same probability of contributing to either signal or background, because we don't have that information on a per event basis.\n",
    "So we include prior information on a per event basis to account for that.\n",
    "\n",
    "$$\n",
    "    \\mathcal{L}(N | n_S, n_B) = \\frac{(n_S + n_B)^{-N}}{N!}\\cdot \\exp{-(n_S + n_B)} \\cdot \\prod_{i=1}^N P_i\n",
    "$$\n",
    "\n",
    "Also the simple poisson pdf above only has one parameter, the total number of events, which can be fit for.\n",
    "So we need to resolve this degeneracy in $n_S$, $n_B$ by giving additional information.\n",
    "For that we include a weighted combination of the probability for an event to be signal, denoted by the PDF $S_i$ and for it to background, denoted by $B_i$.\n",
    "Because the simple counting probabilities are $n_S / (n_S + n_B)$ to count a signal event and likewise $n_B / (n_S + n_B)$ to count a background event we construct the per event prior $P_i$ as:\n",
    "\n",
    "$$\n",
    "    P_i = \\frac{n_S}{n_S + n_B}\\cdot S_i + \\frac{n_B}{n_S + n_B}\\cdot B_i\n",
    "        = \\frac{n_S \\cdot S_i + n_B \\cdot B_i}{n_S + n_B}\n",
    "$$\n",
    "\n",
    "Note, that for equal probabilities $S_i$ and $B_i$, we simply and up with the normal poisson counting statistic.\n",
    "\n",
    "Plugging that back into the likelihood we get:\n",
    "\n",
    "$$\n",
    "    \\mathcal{L}(N | n_S, n_B) = \\frac{(n_S + n_B)^{-N}}{N!}\\cdot \\exp{(-(n_S + n_B))} \\cdot \\prod_{i=1}^N \\frac{n_S \\cdot S_i + n_B \\cdot B_i}{n_S + n_B}\n",
    "$$\n",
    "\n",
    "Taking the natrual logarithm to get the log-likelihood we arrive at:\n",
    "\n",
    "$$\n",
    "    \\ln\\mathcal{L}(N | n_S, n_B) = -(n_S + n_B) -\\ln(N!) + \\sum_{i=1}^N \\ln((n_S + n_B) P_i)\n",
    "$$\n",
    "\n",
    "If we weight up $n_S$ then every events signal PDF is contributing a bit more than the background pdf.\n",
    "So the fitter tries to find the combination of $n_S$ and $n_B$ that maximizes the likelihood.\n",
    "\n",
    "To further simplify, we can use a measured and fixed background expectation rate $\\langle n_B\\rangle$ and fit only for the number of signal events.\n",
    "Then we only fit for the number of signal events $n_S$.\n",
    "The fixed background rate can be extracted from data by using the pdf of a larger timescale and average over that (or fit a function) to ensure that local fluctuations don't matter.\n",
    "\n",
    "Then we end up with our full Likelihood (the denominator in $P_i$ cancels with the term from the poisson PDF):\n",
    "\n",
    "$$\n",
    "    \\ln\\mathcal{L}(N | n_S) = -(n_S + \\langle n_B\\rangle) -\\ln(N!) + \\sum_{i=1}^N \\ln(n_S S_i + \\langle n_B\\rangle B_i)\n",
    "$$\n",
    "\n",
    "For the test statistic we want to test the hypothesis of having no signal $n_S=0$ vs. the alternative with a free parameter $n_S$:\n",
    "\n",
    "$$\n",
    "    \\Lambda = \\ln\\frac{\\mathcal(\\hat{n}_S)}{\\mathcal{n_S=0}}\n",
    "            = \\frac{-(\\hat{n}_S + \\langle n_B\\rangle) -\\ln(N!) + \\sum_{i=1}^N \\ln(\\hat{n}_S S_i + \\langle n_B\\rangle B_i)}{-\\langle n_B\\rangle -\\ln(N!) + \\sum_{i=1}^N \\ln(\\langle n_B\\rangle B_i)}\n",
    "            = -\\hat{n}_S + \\sum_{i=1}^N \\ln\\left( \\frac{\\hat{n}_S S_i}{\\langle n_B\\rangle B_i} + 1 \\right)\n",
    "$$\n",
    "\n",
    "The per event PDFs $S_i$ and $B_i$ can depend on arbitrary parameters.\n",
    "The common choise here is to use a time, energy proxy and spatial proxy depency which has most seperation power:\n",
    "\n",
    "$$\n",
    "    S_i(x_i, t_i, E_i) = S_T(t_i) \\cdot S_S(x_i) \\cdot S_E(E_i) \\\\ \n",
    "    B_i(x_i, t_i, E_i) = B_T(t_i) \\cdot B_S(x_i) \\cdot B_E(E_i) \n",
    "$$\n",
    "\n",
    "Because the Likelihood only contains ratios of the PDF, we only have to construct functions of the signal to background ratio for each time, spatial and energy distribution.\n",
    "\n",
    "For the energy PDFs $S_E, B_E$ we use a 2D representation in reconstructed energy and declination because this has the most seperation power (see coenders & skylab models).\n",
    "The spatial part $S_S, B_S$ is only depending on the distance from source to event, not on the absilute position on the sphere.\n",
    "The time part $S_T, B_T$ is equivalent to that, only using the distance in time between source event and event.\n",
    "\n",
    "**Note: It seems that in mrichmans analysis there has only been used a 1D energy only PDF. This lacks seperation power, when using both hemispheres, as in the southern sky the energy threshhold is much higher.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Time PDF ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "source": [
    "Background in uniformly distributed in the time window.\n",
    "Signal distribtution is falling off gaussian-like at both edges so normalization is different.\n",
    "So the ratio $S_T / B_T$ is simply the the signal pdf divided by the uniform normalization $1 / (t_1 - t_0)$ in the time frame.\n",
    "\n",
    "The signal PDFs written out explicitely, where $t_0$ is the source events time and $t$ the events time:\n",
    "\n",
    "$$\n",
    "    N \\cdot S_T(t, t_0) = \\begin{cases}\n",
    "                     \\frac{1}{\\sqrt{2\\pi}\\sigma_T}\\exp\\left(-\\frac{(t-T_0)^2}{2\\sigma_T^2}\n",
    "                     \\right)&\\quad\\mathrm{, if }\\ t \\in [a, T_0]\\\\                \n",
    "                     \\frac{1}{\\sqrt{2\\pi}\\sigma_T}&\\quad\\mathrm{, if }\\ t \\in [T_0, T_1]\\\\\n",
    "                     \\frac{1}{\\sqrt{2\\pi}\\sigma_T}\\exp\\left(-\\frac{(t-T_1)^2}\n",
    "                     {2\\sigma_T^2}\\right)&\\quad\\mathrm{, if }\\ t \\in [T_1, b]\\\\ \n",
    "                    0 &\\quad\\mathrm{, else}\n",
    "                  \\end{cases}\n",
    "$$\n",
    "\n",
    "where $a, b$ are the bounds of the total time window, $T_0, T_1$ are the part, in which the signal is assumed to be uniformly distributed in time and $\\sigma_T$ is the width of the gaussian edges.\n",
    "The gaussian width $\\sigma_T$ is as wide as the interval $T_1-T_0$ but constraint to the nearest value in $[2, 30]$ seconds if the frame gets too large or too small.\n",
    "The total normalization $N$ is given by integrating over $S_T$ in $[a, b]$, resulting in:\n",
    "\n",
    "$$\n",
    "    N = \\Phi(b) - \\Phi(a) + \\frac{T_1-T_0}{\\sqrt{2\\pi}\\sigma_T}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "    \\Phi(x) = \\int_{-\\infty}^{x}\\frac{1}{\\sqrt{2\\pi}\\sigma_T}\n",
    "      \\exp\\left(-\\frac{(t-T_0)^2}{2\\sigma_T^2}\\right)\\mathrm{d}t\n",
    "$$\n",
    "the CDF of the gaussian PDF.\n",
    "\n",
    "The background PDF respectively is simply:\n",
    "\n",
    "$$\n",
    "    B_T(t, t_0) = \\begin{cases}\n",
    "                     \\frac{1}{b-a}&\\quad\\mathrm{, if }\\ t \\in [a, b]\\\\ \n",
    "                    0 &\\quad\\mathrm{, else}\n",
    "                  \\end{cases}    \n",
    "$$\n",
    "\n",
    "To get finite support we truncate the gaussian edges at $n\\cdot\\sigma_T$.\n",
    "Though arbitrarliy introduced the concrete cutoff of the doesn't really matter (so say 4, 5, 6 sigma, etc).\n",
    "\n",
    "This is because in the LLH we get the product of $\\langle b_B \\rangle B_i$.\n",
    "A larger cutoff make the normalization of the BG pdf larger, but in the same time makes the number of expected BG event get higher in the same linear fashion.\n",
    "So as long as we choose a cutoff which ensures that $S \\approx 0$ outside, we're good to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def time_soverb(t, t0, dt, nsig):\n",
    "    \"\"\"\n",
    "    Time signal over background PDF.\n",
    "    \n",
    "    Signal and background PDFs are each normalized over seconds.\n",
    "    Signal PDF has gaussian edges to smoothly let it fall of to zero, the\n",
    "    stddev is dt when dt is in [2, 30]s, otherwise the nearest edge.\n",
    "\n",
    "    To ensure finite support, the edges are truncated after nsig * dt.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    t : array-like\n",
    "        Times given in MJD for which we want to evaluate the ratio.\n",
    "    t0 : float\n",
    "        Time of the source event.\n",
    "    dt : float\n",
    "        Time window in seconds starting from t0 in which the signal pdf is\n",
    "        assumed to be uniform. Must not be negative.\n",
    "    nsig : float\n",
    "        Clip the gaussian edges at nsig * dt\n",
    "    \"\"\"\n",
    "    if dt < 0:\n",
    "        raise ValueError(\"dt must not be negative.\")\n",
    "\n",
    "    secinday = 24. * 60. * 60.\n",
    "\n",
    "    # Normalize relative to t0 in seconds (first multiply avoids rounding?)\n",
    "    _t = t * secinday - t0 * secinday\n",
    "   \n",
    "    # Create signal PDF\n",
    "    # Constrain sig_t to [2, 30]s regardless of uniform time window\n",
    "    sig_t = np.clip(dt, 2, 30)\n",
    "    sig_t_clip = nsig * sig_t\n",
    "    gaus_norm = (np.sqrt(2 * np.pi) * sig_t)\n",
    "    \n",
    "    # Split in def regions gaus rising, uniform, gaus falling\n",
    "    gr = (_t < 0) & (_t >= -sig_t_clip)\n",
    "    gf = (_t > dt) & (_t <= dt + sig_t_clip)\n",
    "    uni = (_t >= 0) & (_t <= dt)\n",
    "    \n",
    "    pdf = np.zeros_like(t, dtype=np.float)\n",
    "    pdf[gr] = scs.norm.pdf(_t[gr], loc=0, scale=sig_t)\n",
    "    pdf[gf] = scs.norm.pdf(_t[gf], loc=dt, scale=sig_t)\n",
    "    # Connect smoothly with the gaussians\n",
    "    pdf[uni] = 1. / gaus_norm\n",
    "    \n",
    "    # Normalize signal distribtuion\n",
    "    dcdf = (scs.norm.cdf(dt + sig_t_clip, loc=dt, scale=sig_t) -\n",
    "            scs.norm.cdf(-sig_t_clip, loc=0., scale=sig_t))\n",
    "    norm = dcdf + dt / gaus_norm\n",
    "    pdf /= norm\n",
    "    \n",
    "    # Calculate the ratio\n",
    "    bg_pdf = 1. / (dt + 2 * sig_t_clip)\n",
    "    ratio = pdf / bg_pdf\n",
    "    return ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Make a plot with ratios for different time windows as in the paper\n",
    "# Arbitrary start date from data\n",
    "t0 = start_mjd[100]\n",
    "t0_sec = t0 * secinday\n",
    "\n",
    "# dt from t0 in seconds, clip at 4 sigma\n",
    "dts = [5, 50, 200]\n",
    "nsig = 4\n",
    "\n",
    "# Make t values for plotting in MJD around t0, fitting all in one plot\n",
    "max_dt = np.amax(dts)\n",
    "clip = np.clip(max_dt, 2, 30) * nsig\n",
    "plt_rng = np.array([-clip, max_dt + clip])\n",
    "t = np.linspace(t0_sec + 1.2 *plt_rng[0],\n",
    "                t0_sec + 1.2 * plt_rng[1], 1000) / secinday\n",
    "_t = t * secinday - t0 * secinday\n",
    "\n",
    "# Mark event time\n",
    "plt.axvline(0, 0, 1, c=\"#353132\", ls=\"--\", lw=2)\n",
    "\n",
    "colors = [\"C0\", \"C3\", \"C2\"]\n",
    "for i, dt in enumerate(dts):\n",
    "    # Plot ratio S/B\n",
    "    SoB = time_soverb(t, t0, dt, nsig)\n",
    "    plt.plot(_t, SoB, lw=2, c=colors[i],\n",
    "             label=r\"$T_\\mathrm{{uni}}$: {:>3d}s\".format(dt))\n",
    "    # Fill uniform part, might look nicely\n",
    "    # fbtw = (_t > 0) & (_t < dt)\n",
    "    # plt.fill_between(_t[fbtw], 0, SoB[fbtw], color=\"C7\", alpha=0.1)\n",
    "\n",
    "# Make it look like the paper plot, but with slightly extended borders, to\n",
    "# nothing breaks outside the total time frame\n",
    "plt.xlim(1.2 * plt_rng)\n",
    "plt.ylim(0, 3)\n",
    "plt.xlabel(\"t - t0 in sec\")\n",
    "plt.ylabel(\"S / B\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.grid(ls=\"--\", lw=1)\n",
    "\n",
    "plt.savefig(\"./data/figs/time_pdf_ratio.png\", dpi=200)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Spatial Pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The spatial pdf is holding information on how close the event was to the source position.\n",
    "Close events are more likely to originate from the source.\n",
    "\n",
    "To model this behavioure we use a Kent distribution (gaussian correctly normalized on a sphere).\n",
    "\n",
    "$$\n",
    "    S_S(x_\\mathrm{evt}; x_S, \\kappa) = \\frac{\\kappa}{4\\pi \\sinh{\\kappa}}\\cdot \\exp(\\kappa\\cos(\\psi))\n",
    "$$\n",
    "\n",
    "where $x_\\mathrm{evt}$ is the directional vector of the event, $x_S$ is the directional vector of the source an $\\kappa$ resembles to the uncertainty in the event reconstruction and is connected with the more familiar $\\sigma$ error by.\n",
    "\n",
    "The connections between $\\kappa$ and $\\sigma$ is valid up to a $\\sigma\\approx 40^\\circ$ and is given by $\\kappa = 1 /\\sigma^2$.\n",
    "\n",
    "Classicaly the background pdf is constructed from data\n",
    "It is assumed to be uniform in right-ascension and the declination dependence is modeled with a spline fitted to a histogram in sinDec.\n",
    "Then the PDF is given by:\n",
    "\n",
    "$$\n",
    "    B_S(x_\\mathrm{evt}) = \\frac{1}{2\\pi}\\cdot p(\\sin\\delta)\n",
    "$$\n",
    "\n",
    "But we already made the work of creating a smooth KDE of our data in logE, declination and sigma.\n",
    "So we can use that KDE to get the values of our declination distribution.\n",
    "Because integrating out the KDE is slow, we just use our previous sample from the KDE, bin it finely (quasi continously) and interpolate it with a spline to get also values from in between.\n",
    "This way we are not dependent on a binning on the data itself, but can use the available validated KDE PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def spatial_signal(src_ra, src_dec, ev_ra, ev_dec, ev_sig, kent=True):\n",
    "        \"\"\"\n",
    "        Spatial distance PDF between source position(s) and event positions.\n",
    "\n",
    "        Signal is assumed to cluster around source position(s).\n",
    "        The PDF is a convolution of a delta function for the localized sources\n",
    "        and a Kent (gaussian on a sphere) distribution with the events\n",
    "        positional reconstruction error as width.\n",
    "        \n",
    "        Multiplie source positions can be given, to use it in a stacked\n",
    "        search.\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        src_ra : array-like\n",
    "            Src positions in equatorial RA in radian: [0, 2pi].\n",
    "        src_dec : array-like\n",
    "            Src positions in equatorial DEC in radian: [-pi/2, pi/2].\n",
    "        ev_ra : array-like\n",
    "            Event positions in equatorial RA in radian: [0, 2pi].\n",
    "        ev_dec : array-like\n",
    "            Event positions in equatorial DEC in radian: [-pi/2, pi/2].\n",
    "        ev_sig : array-like\n",
    "            Event positional reconstruction error in radian (eg. Paraboloid).\n",
    "        \n",
    "        Returns\n",
    "        --------\n",
    "        S : array-like, shape(n_sources, n_events)\n",
    "            Spatial signal probability for each event and each source.\n",
    "\n",
    "        \"\"\"\n",
    "        # Shape (n_sources, 1), suitable for 1 src or multiple srcs\n",
    "        src_ra = np.atleast_1d(src_ra)[:, np.newaxis]\n",
    "        src_dec = np.atleast_1d(src_dec)[:, np.newaxis]\n",
    "\n",
    "        # Dot product in polar coordinates\n",
    "        cosDist = (np.cos(src_ra - ev_ra) *\n",
    "                   np.cos(src_dec) * np.cos(ev_dec) +\n",
    "                   np.sin(src_dec) * np.sin(ev_dec))\n",
    "    \n",
    "        # Handle possible floating precision errors\n",
    "        cosDist = np.clip(cosDist, -1, 1)\n",
    "        \n",
    "        if kent:\n",
    "            # Stabilized version for possibly large kappas\n",
    "            kappa = 1. / ev_sig**2\n",
    "            S = (kappa / (2. * np.pi * (1. - np.exp(-2. * kappa))) *\n",
    "                 np.exp(kappa * (cosDist - 1. )))\n",
    "        else:\n",
    "            # Otherwise use standard symmetric 2D gaussian\n",
    "            dist = np.arccos(cosDist)\n",
    "            ev_sig_2 = 2 * ev_sig**2\n",
    "            S = np.exp(-dist**2 / (ev_sig_2)) / (np.pi * ev_sig_2)\n",
    "        \n",
    "        return S\n",
    "    \n",
    "def create_spatial_bg_spline(sin_dec, bins=100, range=None, k=3):\n",
    "    \"\"\"\n",
    "    Fit an interpolsating spline to the a histogram of sin(dec).\n",
    "    \n",
    "    The spline is fitted to the logarithm of the histogram, to avoid ringing.\n",
    "    Normalization is done by normalizing the hist.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sin_dec : array-like\n",
    "        Sinus declination coorcinates of each event, [-1, 1].\n",
    "    bins : int or array-like\n",
    "        Binning passed to `np.histogram`. (default: 100)\n",
    "    range : array-like\n",
    "        Lower and upper boundary for the histogram. (default: None)\n",
    "    k : int\n",
    "        Order of the spline. (default: 3)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    spl : scipy.interpolate.InterpolatingSpline\n",
    "        Spline object interpolating the histogram. Must be evaluated with\n",
    "        sin(dec) and exponentiated to give the correct values.\n",
    "        Spline is interpolating outside it's definition range.\n",
    "    \"\"\"\n",
    "    hist, bins = np.histogram(sin_dec, bins=bins, \n",
    "                              range=range, density=True)\n",
    "    \n",
    "    if np.any(hist <= 0.):\n",
    "        estr = (\"Declination hist bins empty, this must not happen. Empty \" +\n",
    "                \"bins: {0}\".format(np.arange(len(bins) - 1)[hist <= 0.]))\n",
    "        raise ValueError(estr)\n",
    "    elif np.any((sin_dec < bins[0]) | (sin_dec > bins[-1])):\n",
    "        raise ValueError(\"Data outside of declination bins!\")\n",
    "\n",
    "    mids = 0.5 * (bins[:-1] + bins[1:])\n",
    "    return sci.InterpolatedUnivariateSpline(mids, np.log(hist), k=k, ext=0)\n",
    "\n",
    "def spatial_background(ev_sin_dec, sindec_log_bg_spline):\n",
    "    \"\"\"\n",
    "    Calculate the value of the backgournd PDF for each event from a previously\n",
    "    created spline, interpolating the declination distribution of the data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ev_sin_dec : array-like\n",
    "        Sinus Declination coordinates of each event, [-1, 1].\n",
    "    sindec_log_bg_spline : scipy.interpolate.InterpolatingSpline\n",
    "        Spline returning the logarithm of the bg PDF at given sin_dec values.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    B : array-like\n",
    "        The value of the background PDF for each event.\n",
    "    \"\"\"\n",
    "    return 1. / 2. / np.pi * np.exp(sindec_log_bg_spline(ev_sin_dec))\n",
    "\n",
    "\n",
    "def spatial_SoB(src_ra, src_dec, ev_ra, ev_dec, ev_sig,\n",
    "                sindec_log_bg_spline, kent=True):\n",
    "    S = spatial_signal(src_ra, src_dec, ev_ra, ev_dec, ev_sig, kent)\n",
    "    B = spatial_background(ev_sin_dec, sindec_log_bg_spline)\n",
    "    \n",
    "    SoB = np.zeros_like(S)\n",
    "    B = np.repeat(B[np.newaxis, :], repeats=S.shape[0], axis=0)\n",
    "    m = B > 0\n",
    "    SoB[m] = S[m] / B[m]\n",
    "\n",
    "    return SoB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Signal PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_dec_vs_signal(S, ev_dec, src_ra, src_dec, weights, ax=None):\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots(1, 1)\n",
    "    # Plot signal per source for each event\n",
    "    for i, (sra, sdec) in enumerate(zip(src_ra, src_dec)):\n",
    "        ax.plot(np.rad2deg(ev_dec), S[i], ls=\"-\")\n",
    "        ax.plot(np.rad2deg(sdec), -10, \"k|\")\n",
    "\n",
    "    # Simulate a simple stacking, one weight per source\n",
    "    ax.plot(np.rad2deg(ev_dec), np.sum(weights * S, axis=0) / np.sum(weights),\n",
    "             ls=\"--\", c=dg, label=\"stacked\")\n",
    "\n",
    "    ax.set_xlim([-1 + smin, smax + 1])\n",
    "    ax.set_xlabel(\"DEC in Â°\")\n",
    "    ax.set_ylabel(\"Signal pdf\")\n",
    "    ax.legend(loc=\"upper right\")\n",
    "    return ax\n",
    "\n",
    "# Simulate a simple case: 5 src and the events are in the same range, but with\n",
    "# tighter spacing\n",
    "smax = 5\n",
    "smin = -5\n",
    "step = 2\n",
    "\n",
    "src_ra = np.deg2rad(np.arange(smin, smax + step, step))\n",
    "src_dec = np.deg2rad(np.arange(smin, smax + step, step))\n",
    "\n",
    "ev_ra = np.deg2rad(np.linspace(smin, smax, 1000))\n",
    "ev_dec = np.deg2rad(np.linspace(smin, smax, 1000))\n",
    "ev_sig = np.deg2rad(np.ones_like(ev_ra))\n",
    "\n",
    "S = spatial_signal(src_ra, src_dec, ev_ra, ev_dec, ev_sig, kent=True)  \n",
    "\n",
    "weights = np.arange(1, len(src_dec) + 1)[:, np.newaxis]\n",
    "_ = plot_dec_vs_signal(S, ev_dec, src_ra, src_dec, weights)\n",
    "plt.show()\n",
    "\n",
    "# Now with the real data. Sort first in dec to show with nice lines\n",
    "idx = np.argsort(exp[\"dec\"])\n",
    "ev_ra = exp[\"ra\"][idx]\n",
    "ev_dec = exp[\"dec\"][idx]\n",
    "# ev_sig = np.deg2rad(np.ones_like(ev_ra))\n",
    "ev_sig = exp[\"sigma\"][idx]\n",
    "\n",
    "S = spatial_signal(src_ra, src_dec, ev_ra, ev_dec, ev_sig, kent=True)\n",
    "\n",
    "weights = np.ones_like(weights)\n",
    "ax = plot_dec_vs_signal(S, ev_dec, src_ra, src_dec, weights)\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_ylim(1, 1e4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Background PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# KDE CV is running on cluster and pickles the GridSearchCV\n",
    "fname = \"./data/kde_cv/KDE_model_selector_20_exp_IC86_I_followup_2nd_pass.pickle\"\n",
    "with open(fname, \"rb\") as f:\n",
    "    model_selector = pickle.load(f)\n",
    "\n",
    "kde = model_selector.best_estimator_\n",
    "bw = model_selector.best_params_[\"bandwidth\"]\n",
    "print(\"Best bandwidth : {:.3f}\".format(bw))\n",
    "\n",
    "# We maybe just want to stick with the slightly overfitting kernel to\n",
    "# be as close as possible to data\n",
    "OVERFIT = True\n",
    "if OVERFIT:\n",
    "    bw = 0.075\n",
    "    kde = skn.KernelDensity(bandwidth=bw, kernel=\"gaussian\", rtol=1e-8)\n",
    "print(\"Used bandwidth : {:.3f}\".format(bw))\n",
    "\n",
    "# KDE sample must be cut in sigma before fitting, similar to range in hist\n",
    "_exp = exp[exp[\"sigma\"] <= np.deg2rad(5)]\n",
    "\n",
    "fac_logE = 1.5\n",
    "fac_dec = 2.5\n",
    "fac_sigma = 2.\n",
    "\n",
    "_logE = fac_logE * _exp[\"logE\"]\n",
    "_sigma = fac_sigma * np.rad2deg(_exp[\"sigma\"])\n",
    "_dec = fac_dec * _exp[\"dec\"]\n",
    "\n",
    "kde_sample = np.vstack((_logE, _dec, _sigma)).T\n",
    "\n",
    "# Fit KDE best model to sample\n",
    "kde.fit(kde_sample)\n",
    "\n",
    "# Generate some BG samples to compare to the original data hist.\n",
    "# Use more statistics, histograms get normalized and we want the best estimate\n",
    "# for the pdf\n",
    "nsamples_kde = int(1e7)\n",
    "bg_samples = kde.sample(n_samples=nsamples_kde)\n",
    "\n",
    "# Restore the orignal scaling and cut away spillovers from the finite width\n",
    "bg_dec = bg_samples[:, 1] / fac_dec\n",
    "m = (bg_dec > -np.pi / 2.) & (bg_dec < np.pi / 2.)\n",
    "bg_dec = bg_dec[m]\n",
    "bg_sin_dec = np.sin(bg_dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, (axl, axr) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# First finely binned KDE. Show the data in the same binning to see the diff\n",
    "bins = 100\n",
    "h, b, _ = axl.hist(bg_sin_dec, bins=bins, normed=True, alpha=0.5)\n",
    "h, b = np.histogram(bg_sin_dec, bins=bins, density=True)\n",
    "kde_spl = create_spatial_bg_spline(bg_sin_dec, bins=bins)\n",
    "\n",
    "_sin_dec = np.linspace(-1, 1, 1000)\n",
    "pdf = np.exp(kde_spl(_sin_dec))\n",
    "axl.plot(_sin_dec, pdf, lw=2)\n",
    "\n",
    "# Now classic with coarse binned data\n",
    "bins = 20\n",
    "sin_dec = np.sin(exp[\"dec\"])\n",
    "h, b, _ = axr.hist(bg_sin_dec, bins=bins, normed=True, alpha=0.5)\n",
    "spl = create_spatial_bg_spline(sin_dec, bins=bins)\n",
    "\n",
    "pdf = np.exp(spl(_sin_dec))\n",
    "axr.plot(_sin_dec, pdf)\n",
    "\n",
    "# Quickly integrate BG pdf to check norm is OK (increased subdvivision lim)\n",
    "I = scint.quad(spatial_background, -1, 1, args=(kde_spl), limit=100)[0]\n",
    "print(\"Area under all sky BG PDF is : \", 2. * np.pi * I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Signal over Background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Make srcs across the dec range. SoB should follow the sinDec BG\n",
    "# distribtuion. With a single source we couldn't see that, because it drops\n",
    "# to zero far from the src position\n",
    "smin, smax, step = -90, +90, 10\n",
    "\n",
    "src_ra = np.deg2rad(np.arange(smin, smax + step, step))\n",
    "src_dec = np.deg2rad(np.arange(smin, smax + step, step))\n",
    "\n",
    "ev_ra = np.deg2rad(np.linspace(smin, smax, 1000))\n",
    "ev_dec = np.deg2rad(np.linspace(smin, smax, 1000))\n",
    "ev_sin_dec = np.sin(ev_dec)\n",
    "ev_sig = np.deg2rad(np.ones_like(ev_ra))\n",
    "\n",
    "weights = np.arange(1, len(src_dec) + 1)[:, np.newaxis]\n",
    "\n",
    "fig, ((axtl, axtr), (axbl, axbr)) = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Signal only\n",
    "S = spatial_signal(src_ra, src_dec, ev_ra, ev_dec, ev_sig, kent=True)  \n",
    "_ = plot_dec_vs_signal(S, ev_dec, src_ra, src_dec, weights, ax=axtl)\n",
    "axtl.set_xlim(-90, 90)\n",
    "\n",
    "# Background only\n",
    "bins = 100\n",
    "h, b, _ = axl.hist(bg_sin_dec, bins=bins, normed=True, alpha=0.5)\n",
    "h, b = np.histogram(bg_sin_dec, bins=bins, density=True)\n",
    "_sin_dec = np.linspace(-1, 1, 1000)\n",
    "pdf = np.exp(kde_spl(_sin_dec))\n",
    "axbl.plot(np.rad2deg(np.arcsin(_sin_dec)), pdf, lw=2, label=\"pdf\")\n",
    "axbl.set_ylim(0, 1)\n",
    "# 1 / BG PDF on second axis\n",
    "axbl2 = axbl.twinx()\n",
    "axbl2.plot(np.rad2deg(np.arcsin(_sin_dec)), 1. / pdf, c=\"C1\",\n",
    "           lw=2, label=\"1/pdf\")\n",
    "axbl2.set_ylim(0, 6)\n",
    "axbl.set_xlabel(\"DEC in Â°\")\n",
    "axbl.set_xlim(-90, 90)\n",
    "axbl.legend(loc=\"upper left\")\n",
    "axbl2.legend(loc=\"upper center\")\n",
    "\n",
    "# SoB on example + BG PDF\n",
    "SoB = spatial_SoB(src_ra, src_dec, ev_ra, ev_dec, ev_sig, kde_spl, kent=True)  \n",
    "weights = np.arange(1, len(src_dec) + 1)[:, np.newaxis]\n",
    "_ = plot_dec_vs_signal(SoB, ev_dec, src_ra, src_dec, weights, ax=axtr)\n",
    "axtr.plot(np.rad2deg(np.arcsin(_sin_dec)), pdf, lw=3, label=\"BG pdf\", c=dg)\n",
    "axtr.set_xlim(-90, 90)\n",
    "axtr.set_yscale(\"log\")\n",
    "axtr.set_ylim(0.1, 1e5)\n",
    "axtr.legend(loc=\"upper left\")\n",
    "\n",
    "# Now with the real data. Sort first in dec to show with nice lines + BG PDF\n",
    "idx = np.argsort(exp[\"dec\"])\n",
    "ev_ra = exp[\"ra\"][idx]\n",
    "ev_dec = exp[\"dec\"][idx]\n",
    "ev_sin_dec = np.sin(ev_dec)\n",
    "ev_sig = exp[\"sigma\"][idx]\n",
    "# ev_sig = np.deg2rad(np.ones_like(ev_ra))  # To match the simple example\n",
    "\n",
    "SoB = spatial_SoB(src_ra, src_dec, ev_ra, ev_dec, ev_sig, kde_spl, kent=True)\n",
    "\n",
    "_ = plot_dec_vs_signal(SoB, ev_dec, src_ra, src_dec, weights, ax=axbr)\n",
    "axbr.plot(np.rad2deg(np.arcsin(_sin_dec)), pdf, lw=3, label=\"BG pdf\", c=\"C0\")\n",
    "axbr.set_yscale(\"log\")\n",
    "axbr.set_ylim(0.1, 1e5)\n",
    "axbr.legend(loc=\"upper left\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Energy-Space Pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This will be the same in skylab and the first time we need a MC set.\n",
    "Make equally binned 2D histograms in logE and sinDec, then take the ratio.\n",
    "Because of the equal binning, the normalization is automatically correct.\n",
    "Then fit a 2D spline to it which gives the signal to background ratio directly.\n",
    "\n",
    "Here we use again a KDE fitted both to data.\n",
    "This way we can sample more events in the sparsely populated areas and obtain a broader ratio distribution.\n",
    "Because we can't use the sklearn KDE for weighted samples we use a normal histogram for the MC, which has more event anyway so the problem is not so urgent.\n",
    "\n",
    "Where data is missing either use background MC or conservatively use the highest ratio where data is available also at positions, where no data is present.\n",
    "This is only relevant for signal injection, because on data we have the ratio defined everywhere, where data is by definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_bg_sample_from_kde(nsamples_kde = int(1e7)):\n",
    "    print(\"Sampling from BG KDE\")\n",
    "    # KDE CV is running on cluster and pickles the GridSearchCV\n",
    "    fname = \"./data/kde_cv/KDE_model_selector_20_exp_IC86_I_followup_2nd_pass.pickle\"\n",
    "    with open(fname, \"rb\") as f:\n",
    "        model_selector = pickle.load(f)\n",
    "\n",
    "    kde = model_selector.best_estimator_\n",
    "    bw = model_selector.best_params_[\"bandwidth\"]\n",
    "    print(\"Best bandwidth : {:.3f}\".format(bw))\n",
    "\n",
    "    # We maybe just want to stick with the slightly overfitting kernel to\n",
    "    # be as close as possible to data\n",
    "    OVERFIT = True\n",
    "    if OVERFIT:\n",
    "        bw = 0.075\n",
    "        kde = skn.KernelDensity(bandwidth=bw, kernel=\"gaussian\", rtol=1e-8)\n",
    "    print(\"Used bandwidth : {:.3f}\".format(bw))\n",
    "\n",
    "    # KDE sample must be cut in sigma before fitting, similar to range in hist\n",
    "    _exp = exp[exp[\"sigma\"] <= np.deg2rad(5)]\n",
    "\n",
    "    fac_logE = 1.5\n",
    "    fac_dec = 2.5\n",
    "    fac_sigma = 2.\n",
    "\n",
    "    _logE = fac_logE * _exp[\"logE\"]\n",
    "    _sigma = fac_sigma * np.rad2deg(_exp[\"sigma\"])\n",
    "    _dec = fac_dec * _exp[\"dec\"]\n",
    "\n",
    "    # Fit KDE best model to background sample\n",
    "    kde_sample = np.vstack((_logE, _dec, _sigma)).T\n",
    "    kde.fit(kde_sample)\n",
    "\n",
    "    # Generate some BG samples to compare to the original data hist.\n",
    "    # Use more statistics, histograms get normalized and we want the best estimate\n",
    "    # for the pdf\n",
    "    bg_samples = kde.sample(n_samples=nsamples_kde)\n",
    "\n",
    "    # Restore the orignal scaling and cut away spillovers from the finite width\n",
    "    bg_logE = bg_samples[:, 0] / fac_logE\n",
    "    bg_dec = bg_samples[:, 1] / fac_dec\n",
    "    bg_sigma = bg_samples[:, 2] / fac_sigma\n",
    "\n",
    "    m = (bg_dec > -np.pi / 2.) & (bg_dec < np.pi / 2.)\n",
    "    m = m & (bg_sigma > 0 )\n",
    "\n",
    "    bg_logE = bg_logE[m]\n",
    "    bg_dec = bg_dec[m]\n",
    "    bg_sindec = np.sin(bg_dec)\n",
    "    bg_sigma = np.deg2rad(bg_sigma[m])\n",
    "    \n",
    "    return bg_sindec, bg_logE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Prepare the MC data, signal weighted to astro unbroken power law\n",
    "gamma = 2.\n",
    "# No flux norm, because we normalize anyway\n",
    "mc_w = mc[\"ow\"] * mc[\"trueE\"]**(-gamma)\n",
    "\n",
    "# Make 2D hist from data KDE and from MC, use the MC binning\n",
    "mc_sindec = np.sin(mc[\"dec\"])\n",
    "mc_logE = mc[\"logE\"]\n",
    "bins = [50, 40]\n",
    "range = [[-1, 1], [1, 10]]\n",
    "mc_h, bx, by = np.histogram2d(mc_sindec, mc_logE, bins=bins, range=range,\n",
    "                              weights=mc_w, normed=True)\n",
    "\n",
    "b = [bx, by]\n",
    "\n",
    "MODE = \"DATA\"  # \"DATA\", \"KDE_SAM\", \"KDE_INT\"\n",
    "if MODE == \"DATA\":\n",
    "    bg_logE = exp[\"logE\"]\n",
    "    bg_sindec = np.sin(exp[\"dec\"])\n",
    "    bg_h, _, _ = np.histogram2d(bg_sindec, bg_logE, bins=b,\n",
    "                                range=range, normed=True)\n",
    "elif MODE == \"KDE_SAM\":\n",
    "    bg_sindec, bg_logE = get_bg_sample_from_kde(int(2e7))\n",
    "    bg_h, _, _ = np.histogram2d(bg_sindec, bg_logE, bins=b,\n",
    "                                range=range, normed=True)\n",
    "elif MODE == \"KDE_INT\":\n",
    "    _bins = np.load(\"data/1d_integrate_kde/logE_sinDec_bins_50x50.npy\")\n",
    "    vals = np.load(\"data/1d_integrate_kde/logE_sinDec_int_50x50.npy\")\n",
    "    mids = get_binmids(_bins)\n",
    "    xx, yy  = map(np.ravel, np.meshgrid(mids[0], mids[1]))\n",
    "    bg_h, _, _ = np.histogram2d(xx, yy, bins=_bins, weights=vals,\n",
    "                                normed=True, range=range)\n",
    "    # Turn around to have sinDec vs logE like in the other examples\n",
    "    bg_h = bg_h.T\n",
    "    # KDE_INT is not so good, because it falls too quickly. Need to clip it\n",
    "    bg_h = np.clip(bg_h, 1e-10, 1)\n",
    "    \n",
    "# 3 cases:\n",
    "#   - Data & MC: Calculate the ratio\n",
    "#   - No data or no MC: Assign nearest value in energy bin\n",
    "#   - No data and no MC: Assign any value (eg 1), these are never accessed\n",
    "# Get logE value per bin in entrie histogram\n",
    "m = get_binmids(b)\n",
    "\n",
    "# Fill value: 1) min/max for low/hig edge or 2) nearest in column\n",
    "FILLVAL = \"MINMAX\"  # \"COL\" |Â \"MINMAX\"\n",
    "\n",
    "# This assumes at least one valid point in one sinDec slice\n",
    "m1 = (bg_h > 0) & (mc_h > 0)\n",
    "SoB = np.ones_like(bg_h) * -1  # Init with unphysical value\n",
    "SoB[m1] = mc_h[m1] / bg_h[m1]\n",
    "SOBmin, SoBmax = np.amin(SoB[m1]), np.amax(SoB[m1])\n",
    "\n",
    "# In each energy bin assign nearest value to bins with no data or no MC\n",
    "for i in np.arange(bins[0]):\n",
    "    bghi = bg_h[i]  # Get sinDec slice\n",
    "    mchi = mc_h[i]\n",
    "    _m = (bghi <= 0) | (mchi <= 0)  # All invalid points\n",
    "    # Only fill missing logE border values and then proceed to interpolation\n",
    "\n",
    "    # First lower edge (argmax stops at first True, argmin at first False)\n",
    "    low_first_invalid_id = np.argmax(_m)\n",
    "    if low_first_invalid_id == 0:\n",
    "        # Set lower edge with first valid point from bottom\n",
    "        low_first_valid_id = np.argmin(_m)\n",
    "        if FILLVAL == \"COL\":\n",
    "            SoB[i, 0] = SoB[i, low_first_valid_id]\n",
    "        elif FILLVAL == \"MINMAX\":\n",
    "            SoB[i, 0] = np.amin(SoB[m1])\n",
    "\n",
    "    # Repeat with turned around array for upper edge\n",
    "    hig_first_invalid_id = np.argmax(_m[::-1])\n",
    "    if hig_first_invalid_id == 0:\n",
    "        # Set lower edge with first valid point from bottom\n",
    "        hig_first_valid_id = len(_m) - 1 - np.argmin(_m[::-1])\n",
    "        if FILLVAL == \"COL\":\n",
    "            SoB[i, -1] = SoB[i, hig_first_valid_id]\n",
    "        elif FILLVAL == \"MINMAX\":\n",
    "            SoB[i, -1] = np.amax(SoB[m1])\n",
    "        \n",
    "    # Interpolate in each slice over missing entries\n",
    "    _m = SoB[i] > 0\n",
    "    x = m[1][_m]\n",
    "    y = SoB[i, _m]\n",
    "    fi = sci.interp1d(x, y, kind=\"linear\")\n",
    "    SoB[i] = fi(m[1])\n",
    "\n",
    "# These do never occur, so set them to 1to be identified quickly in the plot\n",
    "m4 = (bg_h <= 0) & (mc_h <= 0)\n",
    "SoB[m4] = 1.\n",
    "\n",
    "# Now fit a spline to the ratio\n",
    "SoB_spl = sci.RegularGridInterpolator(m, np.log(SoB), method=\"linear\",\n",
    "                                      bounds_error=False, fill_value=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Coenders style sindec vs logE\n",
    "m = get_binmids(b)\n",
    "xx, yy = map(np.ravel, np.meshgrid(*m))\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "(axtl, axtr), (axbl, axbr) = ax\n",
    "\n",
    "# Data\n",
    "_, _, _, img = axtl.hist2d(xx, yy, bins=b, weights=bg_h.T.flatten(),\n",
    "                         norm=LogNorm())\n",
    "axtl.set_title(\"Exp events : {}\".format(len(exp)))\n",
    "caxtl = split_axis(axtl, cbar=True)\n",
    "plt.colorbar(cax=caxtl, mappable=img)\n",
    "\n",
    "# MC\n",
    "_, _, _, img = axtr.hist2d(xx, yy, bins=b, weights=mc_h.T.flatten(),\n",
    "                         norm=LogNorm())\n",
    "axtr.set_title(\"Signal. gamma = {:.1f}\".format(gamma))\n",
    "caxtr = split_axis(axtr, cbar=True)\n",
    "plt.colorbar(cax=caxtr, mappable=img)\n",
    "\n",
    "# Ratio hist\n",
    "cnorm = max(np.amin(SoB), np.amax(SoB))  # coenders: 1e-3, 1e3\n",
    "_, _, _, img = axbl.hist2d(xx, yy, bins=b, weights=SoB.T.flatten(),\n",
    "                         norm=LogNorm(), cmap=\"coolwarm\",\n",
    "                         vmin=1. / cnorm, vmax=cnorm)\n",
    "axbl.set_title(\"Signal over background\".format(gamma))\n",
    "caxbl = split_axis(axbl, cbar=True)\n",
    "plt.colorbar(cax=caxbl, mappable=img)\n",
    "\n",
    "# Ratio spline\n",
    "x = np.linspace(*range[0], num=500 + 1)\n",
    "y = np.linspace(*range[1], num=500 + 1)\n",
    "XX, YY = np.meshgrid(x, y)\n",
    "xx, yy = map(np.ravel, [XX, YY])\n",
    "gpts = np.vstack((xx, yy)).T\n",
    "zz = np.exp(SoB_spl(gpts))\n",
    "ZZ = zz.reshape(XX.shape)\n",
    "# Plotting with hist creates strange effects... Use pcolormesh instead\n",
    "img = axbr.pcolormesh(XX, YY, ZZ, norm=LogNorm(), cmap=\"coolwarm\",\n",
    "                    vmin=1. / cnorm, vmax=cnorm)\n",
    "axbr.set_title(\"Spline interpolation\".format(gamma))\n",
    "caxbr = split_axis(axbr, cbar=True)\n",
    "plt.colorbar(cax=caxbr, mappable=img)\n",
    "\n",
    "# plt.savefig(\"./data/figs/energy_ratio_spline_minmaxfill.png\", dpi=200)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import helper as hlp\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mpldates\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.colors import LogNorm\n",
    "%matplotlib inline\n",
    "\n",
    "import scipy.interpolate as sci\n",
    "import scipy.optimize as sco\n",
    "import scipy.stats as scs\n",
    "\n",
    "import json\n",
    "import datetime\n",
    "import pickle\n",
    "from astropy.time import Time as astrotime\n",
    "\n",
    "import sklearn.neighbors as skn\n",
    "import sklearn.model_selection as skms  # Newer version of grid_search\n",
    "\n",
    "from corner_hist import corner_hist\n",
    "from anapymods3.plots.general import split_axis, get_binmids, hist_marginalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "\n",
    "Load IC86 data from epinat, which should be the usual IC86-I (2011) PS sample, but pull corrected and OneWeights corrected by number of events generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exp, mc, livetime = hlp.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data livetime\n",
    "\n",
    "Generate from good run list as stated here:\n",
    "- http://icecube.wisc.edu/~coenders/html/build/html/ic86-bdt/muonL3.html\n",
    "- https://wiki.icecube.wisc.edu/index.php/IC86_I_Point_Source_Analysis/Data_and_Simulation\n",
    "\n",
    "It should be 332.61 days as stated by jefeintzeig and scoenders.\n",
    "We create one bin per included run, with exactly that width.\n",
    "Excluded runs are those with too high/low rate and without everything marked \"good\".\n",
    "\n",
    "Livetime ist a bit higher, because we used a newer runlist from iclive instead of the old non-json v1.4.\n",
    "See side test for that comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run_list = hlp.get_run_list()\n",
    "run_dict = hlp.get_run_dict(run_list)\n",
    "inc_run_arr, _livetime = hlp.get_good_runs(run_dict)\n",
    "\n",
    "print(\"IC86-I livetime from iclive: \", _livetime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bin BG according to runlist\n",
    "\n",
    "Each run is one bin in the bg rate vs time plot.\n",
    "The rate is normed to Hertz by dividing through the bin sizes in seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Store events in bins with run borders\n",
    "exp_times = exp[\"timeMJD\"]\n",
    "start_mjd = inc_run_arr[\"start_mjd\"]\n",
    "stop_mjd = inc_run_arr[\"stop_mjd\"]\n",
    "\n",
    "tot = 0\n",
    "evts_in_run = {}\n",
    "for start, stop , runid in zip(start_mjd, stop_mjd, inc_run_arr[\"runID\"]):\n",
    "    mask = (exp_times >= start) & ( exp_times < stop)\n",
    "    evts_in_run[runid] = exp[mask]\n",
    "    tot += np.sum(mask)\n",
    "    \n",
    "# Crosscheck, if we got all events and counted nothing double\n",
    "print(\"Do we have all events? \", tot == len(exp))\n",
    "print(\"  Events selected : \", tot)\n",
    "print(\"  Events in exp   : \", len(exp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create binmids and histogram values in each bin\n",
    "binmids = 0.5 * (start_mjd + stop_mjd)\n",
    "h = np.zeros(len(binmids), dtype=np.float)\n",
    "\n",
    "for i, evts in enumerate(evts_in_run.values()):\n",
    "    h[i] = len(evts)\n",
    "    \n",
    "# Mask those with zero rate\n",
    "m = h > 0.\n",
    "binmids = binmids[m]\n",
    "h = h[m]\n",
    "    \n",
    "# Create plot arrays\n",
    "runtimes_mjd = inc_run_arr[\"stop_mjd\"] - inc_run_arr[\"start_mjd\"]\n",
    "xerr = runtimes_mjd[m] / 2.\n",
    "yerr = np.sqrt(h)\n",
    "\n",
    "# Show in Hertz, so go from MJD days to seconds in bin widths\n",
    "secsinday = 24. * 60. * 60\n",
    "norm = (stop_mjd[m] - start_mjd[m]) * secsinday\n",
    "h_norm = h / norm\n",
    "# Poisson errors just get scaled\n",
    "yerr_norm = yerr / norm\n",
    "\n",
    "# Weights only for the weighted average\n",
    "weights = np.ones_like(yerr)\n",
    "weights[yerr_norm == 0] = 0\n",
    "weights[yerr_norm != 0] = 1 / yerr[yerr_norm != 0]\n",
    "def f(x, a, b, c):\n",
    "    \"\"\"Fix baseline to weighted average\"\"\"\n",
    "    return a * np.sin(b * (normed - c)) + np.average(h_norm, weights=weights)\n",
    "normed = (binmids - binmids.min()) / (binmids.max() - binmids.min())\n",
    "\n",
    "# Scaled seed from handcrafted guess in cell below\n",
    "p0 = [-0.0005, 2 * np.pi, 0.1]\n",
    "\n",
    "# Fit a poly to the rate. No weights, because we threw out entries with 0\n",
    "# Also with weight, the period is only have despite the good seed values...\n",
    "res = sco.curve_fit(f=f, xdata=normed, ydata=h_norm, p0=p0)\n",
    "pars = res[0]\n",
    "\n",
    "print(\"Best fit pars : \", pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot like mrichman did on p. 113\n",
    "Note: Date plots are THE MOST DIFFICULT AND LEAST FUN THING TODO...\n",
    "\"\"\"\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "# Show dates on x axis\n",
    "datetimes = astrotime(binmids, format=\"mjd\").to_datetime()\n",
    "dates = mpldates.date2num([dt.date() for dt in datetimes])\n",
    "\n",
    "# Every month, first day\n",
    "months = mpldates.MonthLocator(bymonth=np.arange(1, 13), bymonthday=1)\n",
    "monthsFmt = mpldates.DateFormatter(\"%b %Y\")\n",
    "ax.xaxis.set_major_locator(months)\n",
    "ax.xaxis.set_major_formatter(monthsFmt)\n",
    "\n",
    "ax.errorbar(dates, h_norm, fmt=\".\", xerr=xerr, yerr=yerr_norm)\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Rate in HZ\")\n",
    "ax.set_xlim(dates[0], dates[-1])\n",
    "ax.set_ylim(0., None)\n",
    "\n",
    "# Plot polyfit\n",
    "delta_days = (datetimes[-1] - datetimes[0]).days\n",
    "xdatetimes = [datetimes[0] + datetime.timedelta(days=int(x))for x in\n",
    "              np.arange(0, delta_days)]\n",
    "xtimes_mjd = astrotime(xdatetimes).mjd\n",
    "normed = (xtimes_mjd - binmids.min()) / (binmids.max() - binmids.min())\n",
    "y = f(normed, *pars)\n",
    "\n",
    "# Handcrafted seed trial & error\n",
    "# s = [-0.0005, 2 * np.pi, 0.1]\n",
    "# y = s[0] * np.sin(s[1] * (normed + s[2]))\n",
    "#     + np.average(h_norm, weights=weights)\n",
    "\n",
    "# Convert back to mpl dates\n",
    "xdates = mpldates.date2num([xd.date() for xd in xdatetimes])\n",
    "ax.plot(xdates, y, \"r-\", zorder=5)\n",
    "ax.axhline(np.average(h_norm, weights=weights), 0, 1, color=\"k\",\n",
    "                      ls=\"--\", zorder=5)\n",
    "\n",
    "# Autoprettify main xlabels\n",
    "fig.autofmt_xdate(rotation=60)\n",
    "\n",
    "# Show mjd on top\n",
    "def ax2ticker(x):\n",
    "    dates = mpldates.num2date(x)\n",
    "    mjd = astrotime(dates).mjd\n",
    "    return mjd\n",
    "ax2 = ax.twiny()\n",
    "ax2.set_xticks(ax.get_xticks())\n",
    "ax2.set_xbound(ax.get_xbound())\n",
    "ax2.set_xticklabels(ax2ticker(ax.get_xticks()),\n",
    "                    rotation=60, horizontalalignment=\"left\")\n",
    "ax2.set_xlabel(\"MJD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's make the BG pdf\n",
    "\n",
    "Proceeding to section 6.3.1 Randomized BG Injection, p. 113.\n",
    "Mrichmann draws events by:\n",
    "\n",
    "1. Get number of bg events to be injected from a poisson distribution with expectation values drawn from the previously build bg temporal distribution.\n",
    "   $$\n",
    "   P_{\\langle n_B\\rangle}(N_m) = \\frac{\\langle n_B\\rangle^{N_m}}{N_m\\!}\\cdot \\exp(\\langle n_B\\rangle)\n",
    "   $$\n",
    "2. These events are then drawn from a 3D pdf in energy proxy, zenith proxy and sigma proxy.\n",
    "   He does it by dividing 10x10x10 bins, first selecting energy, then zenith in that energy bin, then sigma in that zenith bin.\n",
    "   \n",
    "Here we create a smooth PDF using a kernel density estimator and obtain a sample by running a MCMC chain to create a sample a priori.\n",
    "The bandwidth is set globally and cross validated to be robust.\n",
    "\n",
    "**Some note on `numpy.histogramdd`:**\n",
    "\n",
    "The input must be an array with shape (nDim, len(data)).\n",
    "\n",
    "Shape of h is the same as the number of bins in each dim: (50, 40, 10)\n",
    "So the first dimension picks a single logE slice -> h[i].shape = (40, 10)\n",
    "Second dim picks a dec slice -> h[:, i].shape = (50, 10)\n",
    "3rd picks a sigma slice -> h[:, :, i].shape = (50, 40)\n",
    "\n",
    "This is important: meshgrid repeats in second axis on first array xx.\n",
    "For the second array, the first axis is repeated.\n",
    "But h iterates over energy in 1st axis. So if we don't transpose, we have the whole histogram flipped! Compare to plot in mrcihmanns thesis (cos(zen))\n",
    "\n",
    "**Some notes on KDE:**\n",
    "\n",
    "Sebastian has already made a tool for adaptive and asymmetric KDE.\n",
    "1. The Kernel is the covariance matrix of the whole data set to regard different scales\n",
    "    + Note: This may only be a problem, if one dim is spread with peaks, while the other is wide spread only. Then we cannot scale the Kernel to small to fit the peaks because the smooth dimension is preventing that.\n",
    "2. Use Silvermans or Scotts rule as a first guess.\n",
    "3. Run a second pass and vary the local bandwidth according to the first guess local density.\n",
    "\n",
    "We could replace 1 and 2 by scaling the data with the inverse covariance and then using a cross validation to find the first guess bandwidth.\n",
    "Then using a second pass to vary locally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D histogram\n",
    "First we make a 3D histogram to better compare to mrichmann and to get an overview over the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# HANDTUNED scale parameter to \"fit\" KDE expectation to data...\n",
    "# TODO: Use Adaptive kernel width and asymmetric gaus kernels\n",
    "#       For sigma it might make sense to a take a restricted kernel [0, inf]\n",
    "fac_logE = 1.5\n",
    "fac_dec = 2.5\n",
    "fac_sigma = 2.\n",
    "\n",
    "logE = fac_logE * exp[\"logE\"]\n",
    "sigma = fac_sigma * np.rad2deg(exp[\"sigma\"])\n",
    "# np.cos(np.pi / 2. + exp[\"dec\"]); dec is for {sin(dec), dec, cos(zen)}\n",
    "dec = fac_dec * exp[\"dec\"]\n",
    "\n",
    "# Binning is rather arbitrary because we don't calc stuff with the hist\n",
    "bins = [50, 50, 50]\n",
    "# Range for sigma is picked by looking at the 1D distribution and cutting of\n",
    "# the tail. This will be covered by the KDE tail anyway. Rest is default\n",
    "r = [[np.amin(logE), np.amax(logE)],\n",
    "     [np.amin(dec), np.amax(dec)],\n",
    "     [0., fac_sigma * 5.]]\n",
    "\n",
    "sample = np.vstack((logE, dec, sigma)).T\n",
    "h, bins = np.histogramdd(sample=sample, bins=bins, range=r, normed=False)\n",
    "\n",
    "# Make bin mids for later use\n",
    "mids = []\n",
    "for b in bins:\n",
    "    mids.append(0.5 * (b[:-1] + b[1:]))\n",
    "\n",
    "# Make a nice corner plot\n",
    "fig, ax = corner_hist(h, bins=bins,\n",
    "                      label=[\"logE\", \"dec\", \"sigma deg\"],\n",
    "                      hist2D_args={\"cmap\": \"Greys\"},\n",
    "                      hist_args={\"color\":\"#353132\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Density Estimation\n",
    "\n",
    "We use scikit learn's cross validation with a gaussian kernel to get the most robust bandwidth.\n",
    "Then we integrate with the same binning as above and compare to the 3D histogram.\n",
    "\n",
    "This section relies heavily on [Jake van der Plas examples for KDE](https://jakevdp.github.io/blog/2013/12/01/kernel-density-estimation/).\n",
    "More info on how KDE cross validation works can be found in [Modern Nonparametric Methods](http://www2.stat.duke.edu/~wjang/teaching/S05-293/lecture/ch6.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "````\n",
    "# KDE CV is running on cluster and pickles the GridSearchCV\n",
    "fname = \"data/KDE_model_selector_CV20_exp_IC86_I.pickle\"\n",
    "with open(fname, \"rb\") as f:\n",
    "    model_selector = pickle.load(f)\n",
    "\n",
    "kde = model_selector.best_estimator_\n",
    "bw = model_selector.best_params_[\"bandwidth\"]\n",
    "print(\"Best bandwidth : {:.3f}\".format(bw))\n",
    "\n",
    "# Estimate pdf for data sample with best model\n",
    "kde.fit(sample)\n",
    "\n",
    "# Generate some BG samples to compare to the original data hist\n",
    "bg_samples = kde.sample(n_samples=2 * len(exp))\n",
    "\n",
    "# Make histogram with same binning as original data\n",
    "bg_h, bg_bins = np.histogramdd(sample=sample, bins=bins, range=r)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kde = skn.KernelDensity(rtol=1e-8, kernel=\"gaussian\", bandwidth=0.1)\n",
    "\n",
    "# Estimate pdf for data sample with best model\n",
    "kde.fit(sample)\n",
    "\n",
    "# Generate some BG samples to compare to the original data hist.\n",
    "# Use more statistics, histograms get normalized and we want the best estimate\n",
    "# for the pdf\n",
    "nsamples_kde = int(1e8)\n",
    "bg_samples = kde.sample(n_samples=nsamples_kde)\n",
    "\n",
    "# Make histogram with same binning as original data\n",
    "bg_h, bg_bins = np.histogramdd(sample=bg_samples, bins=bins, range=r, normed=True)\n",
    "\n",
    "fig, ax = corner_hist(bg_h, bins=bg_bins,\n",
    "                      label=[\"logE\", \"sin(dec)\", \"sigma deg\"],\n",
    "                      hist2D_args={\"cmap\": \"Greys\"},\n",
    "                      hist_args={\"color\":\"#353132\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare KDE to original data\n",
    "\n",
    "Make a ratio histogram of the KDE sample and the original data sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def marginalize_kde(sample, rnge, nvals, axis):\n",
    "    \"\"\"\n",
    "    Integrate out the KDE to 1D by using a large sample and do a MC\n",
    "    integration by simply counting all point in that range.\n",
    "    \"\"\"\n",
    "    # Make bins\n",
    "    bins = np.linspace(rnge[0], rnge[1], nvals)\n",
    "    y, bins = np.histogram(sample.T[int(axis)], bins=bins, density=True)\n",
    "    x = 0.5 * (bins[:-1] + bins[1:])  \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Try to integrate one dimension out with a real integration over one dim.\n",
    "# Pretend, that the integral point in the middle is a good approximation of\n",
    "# the bin.\n",
    "# TODO: Multiply the value from the integration with dx, dy binwidth to\n",
    "# approximate the integration over the binwidth.\n",
    "# Simply sampling is so much easier\n",
    "\n",
    "import scipy.integrate as scint\n",
    "\n",
    "xx, yy = map(np.ravel, np.meshgrid(m[0], m[1]))\n",
    "\n",
    "grids = np.vstack((xx, yy)).T\n",
    "\n",
    "def pdf(x, *args):\n",
    "    # axes = which axes are fixed. x is integrated over.\n",
    "    # a0, a1 are the coords of the fixed gridpoints, as stated in axes\n",
    "    a0, a1, axes = args\n",
    "    \n",
    "    point = np.array([x, x, x])\n",
    "    point[axes] = [a0, a1]\n",
    "    \n",
    "    return np.exp(kde.score_samples([point,]))\n",
    "    \n",
    "margin = []\n",
    "for gp in grids:\n",
    "    integral = scint.quad(pdf, bins[2][0], bins[2][-1],\n",
    "                          args=(gp[0], gp[1], [0, 1]))\n",
    "    margin.append(integral)\n",
    "\n",
    "vals = np.array(margin)[:, 0]\n",
    "m = get_binmids(bins)\n",
    "xx, yy = map(np.ravel, np.meshgrid(m[0], m[1]))\n",
    "\n",
    "plt.hist2d(xx, yy, bins=[bins[0], bins[1]], weights=vals);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D marginalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xlabel = [\"scaled \" + s for s in [\"logE\", \"logE\", \"dec\"]]\n",
    "ylabel = [\"scaled \" + s for s in [\"dec\", \"sigma in °\", \"sigma in °\"]]\n",
    "\n",
    "for i, axes in enumerate([[0, 1], [0, 2], [1, 2]]):\n",
    "    _b = np.array(bins)\n",
    "    h_exp, b_exp = np.histogramdd(sample[:, axes],\n",
    "                                  bins=_b[axes], normed=True)\n",
    "    h_kde, b_kde = np.histogramdd(bg_samples[:, axes],\n",
    "                                  bins=_b[axes], normed=True)\n",
    "    \n",
    "    # KDE is expectation, but sampled with much more events.\n",
    "    # Weights would simply scale the total number of KDE events to match the\n",
    "    # number of original events. That would be the mean for the poisson\n",
    "    # distribution in each bin. So to get OK KDE expectation sqrt(n) errors\n",
    "    # in each bin, we divide not by the number of drawn KDE but by the number\n",
    "    # of original events.   \n",
    "    # Again shapes of meshgrid and hist are transposed\n",
    "    diffXX, _ = np.meshgrid(np.diff(_b[0]), np.diff(_b[1]))\n",
    "    norm_kde = len(exp) * diffXX.T\n",
    "    sigma_kde = np.sqrt(h_kde / norm_kde)\n",
    "\n",
    "    # Make 3 different diff/ratio hists to estimate KDE quality in\n",
    "    # 1D marginalization.\n",
    "    m = (h_exp > 0.)\n",
    "    ratio_h = np.zeros_like(h_exp)\n",
    "    ratio_h[m] = h_kde[m] / h_exp[m]\n",
    "\n",
    "    diff_h = h_kde - h_exp\n",
    "\n",
    "    m = (sigma_kde > 0.)\n",
    "    sigma_ratio_h = np.zeros_like(h_exp)\n",
    "    sigma_ratio_h[m] = (h_exp[m] - h_kde[m]) / sigma_kde[m]\n",
    "\n",
    "    # Bin mids and hist grid\n",
    "    _b = b_exp\n",
    "    m = get_binmids(_b)\n",
    "    xx, yy = map(np.ravel, np.meshgrid(m[0], m[1]))\n",
    "    \n",
    "    \n",
    "    # Big plot on the left and three right\n",
    "    fig = plt.figure(figsize=(10, 6))\n",
    "    gs = gridspec.GridSpec(3, 3)\n",
    "    axl = fig.add_subplot(gs[:, :2])\n",
    "    axrt = fig.add_subplot(gs[0, 2])\n",
    "    axrc = fig.add_subplot(gs[1, 2])\n",
    "    axrb = fig.add_subplot(gs[2, 2])\n",
    "    \n",
    "    # Steal space for colorbars\n",
    "    caxl = split_axis(axl, \"right\")\n",
    "    caxrt = split_axis(axrt, \"left\")\n",
    "    caxrc = split_axis(axrc, \"left\")\n",
    "    caxrb = split_axis(axrb, \"left\")\n",
    "\n",
    "    # Unset top and center xticklabels as they are shared with the bottom plot\n",
    "    axrt.set_xticklabels([])\n",
    "    axrc.set_xticklabels([])\n",
    "        \n",
    "    # Left: Difference over KDE sigma\n",
    "    # cbar_extr = max(np.amax(sigma_ratio_h),  # Center colormap to min/max\n",
    "    #                         abs(np.amin(sigma_ratio_h)))\n",
    "    _, _, _, imgl = axl.hist2d(xx, yy, bins=_b, weights=sigma_ratio_h.T.ravel(),\n",
    "                               cmap=\"seismic\", vmax=5, vmin=-5)\n",
    "    cbarl = plt.colorbar(cax=caxl, mappable=imgl)\n",
    "    axl.set_xlabel(xlabel[i])\n",
    "    axl.set_ylabel(ylabel[i])\n",
    "    axl.set_title(\"(exp - kde) / sigma_kde\")\n",
    "    \n",
    "    # Right top: Ratio\n",
    "    _, _, _, imgrt = axrt.hist2d(xx, yy, bins=_b, weights=ratio_h.T.ravel(),\n",
    "                                 cmap=\"seismic\", vmax=2, vmin=0);\n",
    "    cbarrt = plt.colorbar(cax=caxrt, mappable=imgrt)\n",
    "    axrt.set_title(\"kde / exp\")\n",
    "\n",
    "    # Right center: Data hist\n",
    "    _, _, _, imgrc = axrc.hist2d(xx, yy, bins=_b, weights=h_exp.T.ravel(),\n",
    "                                 cmap=\"Greys\", norm=LogNorm());\n",
    "    cbarrc = plt.colorbar(cax=caxrc, mappable=imgrc)\n",
    "    axrc.set_title(\"exp logscale\")\n",
    "\n",
    "    # Right bottom: KDE hist, same colorbar scale as on data\n",
    "    _, _, _, imgrb = axrb.hist2d(xx, yy, bins=_b, weights=h_kde.T.ravel(),\n",
    "                                 cmap=\"Greys\", norm=LogNorm());\n",
    "    # Set with same colormap as on data\n",
    "    imgrb.set_clim(cbarrc.get_clim())\n",
    "    cbarrb = plt.colorbar(cax=caxrb, mappable=imgrb)\n",
    "    axrb.set_title(\"kde logscale\")\n",
    "    \n",
    "    # Set tick and label positions\n",
    "    for ax in [caxrt, caxrc, caxrb]:\n",
    "        ax.yaxis.set_label_position(\"right\")\n",
    "        ax.yaxis.tick_left()\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D marginalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_b = np.array(bins)\n",
    "h_exp, b_exp = np.histogramdd(sample[:, [0,]], bins=_b[[0,]], normed=True)\n",
    "\n",
    "m = get_binmids(b_exp)\n",
    "\n",
    "plt.hist(m[0], bins=b_exp[0], weights=h_exp);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xlabel = [\"scaled \" + s for s in [\"logE\", \"dec\", \"sigma °\"]]\n",
    "\n",
    "for i, axes in enumerate([(1, 2), (0, 2), (0, 1)]):\n",
    "    h_exp, b_exp = hist_marginalize(h, bins, axes=axes)\n",
    "    h_kde, b_kde = hist_marginalize(bg_h, bg_bins, axes=axes)\n",
    "      \n",
    "    # KDE is expectation, but sampled with much more events.\n",
    "    # Weights would simply scale the total number of KDE events to match the\n",
    "    # number of original events. That would be the mean for the poisson\n",
    "    # distribution in each bin. So to get OK KDE expectation sqrt(n) errors\n",
    "    # in each bin, we divide not by the number of drawn KDE but by the number\n",
    "    # of original events.\n",
    "    norm_kde = len(exp) * np.diff(b_kde)\n",
    "    sigma_kde = np.sqrt(h_kde / norm_kde)\n",
    "\n",
    "    # Make 3 different diff/ratio hists to estimate KDE quality in\n",
    "    # 1D marginalization.\n",
    "    m = (h_exp > 0.)\n",
    "    ratio_h = np.zeros_like(h_exp)\n",
    "    ratio_h[m] = h_kde[m] / h_exp[m]\n",
    "\n",
    "    diff_h = h_kde - h_exp\n",
    "\n",
    "    m = (sigma_kde > 0.)\n",
    "    sigma_ratio_h = np.zeros_like(h_exp)\n",
    "    sigma_ratio_h[m] = (h_exp[m] - h_kde[m]) / sigma_kde[m]\n",
    "\n",
    "    # Bin mids\n",
    "    _b = b_exp\n",
    "    m = get_binmids([_b])[0]\n",
    "    \n",
    "    # Plot both and the ration normed. Big plot on the left and three right\n",
    "    fig = plt.figure(figsize=(10, 6))\n",
    "    gs = gridspec.GridSpec(3, 3)\n",
    "    axl = fig.add_subplot(gs[:, :2])\n",
    "    axrt = fig.add_subplot(gs[0, 2])\n",
    "    axrc = fig.add_subplot(gs[1, 2])\n",
    "    axrb = fig.add_subplot(gs[2, 2])\n",
    "\n",
    "    axrt.set_xticklabels([])\n",
    "    axrc.set_xticklabels([])\n",
    "\n",
    "    # Set ticks and labels right\n",
    "    for ax in [axrt, axrc, axrb]:\n",
    "        ax.yaxis.set_label_position(\"right\")\n",
    "        ax.yaxis.tick_right()\n",
    "\n",
    "    # Limits\n",
    "    for ax in [axl, axrt, axrc, axrb]:\n",
    "        ax.set_xlim(_b[0], _b[-1])\n",
    "        \n",
    "    # Main plot:\n",
    "    # Plot more dense to mimic a smooth curve\n",
    "    __h, __b = np.histogram(bg_samples[:, i], bins=500,\n",
    "                            range=[_b[0], _b[-1]], density=True)\n",
    "    __m = get_binmids([__b])[0]\n",
    "    axl.plot(__m, __h, lw=3, alpha=0.5)\n",
    "    \n",
    "    _ = axl.hist(m, bins=_b, weights=h_exp, label=\"exp\", histtype=\"step\",\n",
    "                 lw=2, color=\"k\")\n",
    "    _ = axl.errorbar(m, h_kde, yerr=sigma_kde, fmt=\",\", color=\"r\")\n",
    "    _ = axl.hist(m, bins=_b, weights=h_kde, label=\"kde\", histtype=\"step\",\n",
    "                 lw=2, color=\"r\")    \n",
    "    \n",
    "    axl.set_xlabel(xlabel[i])\n",
    "    axl.legend(loc=\"upper right\")\n",
    "\n",
    "    # Top right: Difference\n",
    "    _ = axrt.axhline(0, 0, 1, color=\"k\", ls=\"-\")\n",
    "    _ = axrt.hlines([-.02, -.01, .01, .02], _b[0], _b[-1],\n",
    "                    colors='#353132', linestyles='dashed')\n",
    "    _ = axrt.hist(m, bins=_b, weights=diff_h, histtype=\"step\", lw=2, color=\"r\")\n",
    "    axrt.set_ylim(-.05, +.05)\n",
    "    axrt.set_ylabel(\"kde - exp\")\n",
    "\n",
    "    # Center right: Ratio\n",
    "    _ = axrc.axhline(1, 0, 1, color=\"k\", ls=\"-\")\n",
    "    _ = axrc.hlines([0.8, 0.9, 1.1, 1.2], _b[0], _b[-1],\n",
    "                    colors='#353132', linestyles='dashed')\n",
    "    _ = axrc.hist(m, bins=_b, weights=ratio_h, histtype=\"step\", lw=2, color=\"r\")\n",
    "    axrc.set_ylim(.5, 1.5)\n",
    "    axrc.set_ylabel(\"kde / exp\")\n",
    "\n",
    "    # Bottom right: Ratio of diff to sigma of expectation\n",
    "    _ = axrb.axhline(0, 0, 1, color=\"k\", ls=\"-\")\n",
    "    _ = axrb.hlines([-2, -1, 1, 2], _b[0], _b[-1],\n",
    "                    colors='#353132', linestyles='dashed')\n",
    "    _ = axrb.hist(m, bins=_b, weights=sigma_ratio_h, histtype=\"step\", lw=2, color=\"r\")\n",
    "    axrb.set_ylim(-3, +3)\n",
    "    axrb.set_ylabel(\"(exp-kde)/sigma_kde\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Likelihoods\n",
    "\n",
    "Here we define our Likelihoods.\n",
    "We are given a source event occurance (can be GRB, GW, HESE or anything else) at a given position in space and time.\n",
    "We want to search for a significant contribution of other events, within a predefined region in time and space around the source events.\n",
    "For this we need to derive the expected signal and background contributions in that frame.\n",
    "\n",
    "The Likelihood that describes this scenario can be derived from counting statistics.\n",
    "If we expect $n_S$ signal and $n_B$ background events in the given frame, then the probability of observing $N$ events is given by a poisson pdf:\n",
    "\n",
    "$$\n",
    "    P_\\text{Poisson}(N\\ |\\ n_S + n_B) = \\mathcal{L}(N | n_S, n_b) = \\frac{(n_S + n_B)^{-N}}{N!}\\cdot \\exp{-(n_S + n_B)}\n",
    "$$\n",
    "\n",
    "We want to fit for the number of signal events $n_S$ in the frame.\n",
    "But each event doesn_t have the same probability of contributing to either signal or background, because we don't have that information on a per event basis.\n",
    "So we include prior information on a per event basis to account for that.\n",
    "\n",
    "$$\n",
    "    \\mathcal{L}(N | n_S, n_B) = \\frac{(n_S + n_B)^{-N}}{N!}\\cdot \\exp{-(n_S + n_B)} \\cdot \\prod_{i=1}^N P_i\n",
    "$$\n",
    "\n",
    "Also the simple poisson pdf above only has one parameter, the total number of events, which can be fit for.\n",
    "So we need to resolve this degeneracy in $n_S$, $n_B$ by giving additional information.\n",
    "For that we include a weighted combination of the probability for an event to be signal, denoted by the PDF $S_i$ and for it to background, denoted by $B_i$.\n",
    "Because the simple counting probabilities are $n_S / (n_S + n_B)$ to count a signal event and likewise $n_B / (n_S + n_B)$ to count a background event we construct the per event prior $P_i$ as:\n",
    "\n",
    "$$\n",
    "    P_i = \\frac{n_S}{n_S + n_B}\\cdot S_i + \\frac{n_B}{n_S + n_B}\\cdot B_i\n",
    "        = \\frac{n_S \\cdot S_i + n_B \\cdot B_i}{n_S + n_B}\n",
    "$$\n",
    "\n",
    "Note, that for equal probabilities $S_i$ and $B_i$, we simply and up with the normal poisson counting statistic.\n",
    "\n",
    "Plugging that back into the likelihood we get:\n",
    "\n",
    "$$\n",
    "    \\mathcal{L}(N | n_S, n_B) = \\frac{(n_S + n_B)^{-N}}{N!}\\cdot \\exp{(-(n_S + n_B))} \\cdot \\prod_{i=1}^N \\frac{n_S \\cdot S_i + n_B \\cdot B_i}{n_S + n_B}\n",
    "$$\n",
    "\n",
    "Taking the natrual logarithm to get the log-likelihood we arrive at:\n",
    "\n",
    "$$\n",
    "    \\ln\\mathcal{L}(N | n_S, n_B) = -(n_S + n_B) -\\ln(N!) + \\sum_{i=1}^N \\ln((n_S + n_B) P_i)\n",
    "$$\n",
    "\n",
    "If we weight up $n_S$ then every events signal PDF is contributing a bit more than the background pdf.\n",
    "So the fitter tries to find the combination of $n_S$ and $n_B$ that maximizes the likelihood.\n",
    "\n",
    "To further simplify, we can use a measured and fixed background expectation rate $\\langle n_B\\rangle$ and fit only for the number of signal events.\n",
    "Then we only fit for the number of signal events $n_S$.\n",
    "The fixed background rate can be extracted from data by using the pdf of a larger timescale and average over that (or fit a function) to ensure that local fluctuations don't matter.\n",
    "\n",
    "Then we end up with our full Likelihood (the denominator in $P_i$ cancels with the term from the poisson PDF):\n",
    "\n",
    "$$\n",
    "    \\ln\\mathcal{L}(N | n_S) = -(n_S + \\langle n_B\\rangle) -\\ln(N!) + \\sum_{i=1}^N \\ln(n_S S_i + \\langle n_B\\rangle B_i)\n",
    "$$\n",
    "\n",
    "For the test statistic we want to test the hypothesis of having no signal $n_S=0$ vs. the alternative with a free parameter $n_S$:\n",
    "\n",
    "$$\n",
    "    \\Lambda = \\ln\\frac{\\mathcal(\\hat{n}_S)}{\\mathcal{n_S=0}}\n",
    "            = \\frac{-(\\hat{n}_S + \\langle n_B\\rangle) -\\ln(N!) + \\sum_{i=1}^N \\ln(\\hat{n}_S S_i + \\langle n_B\\rangle B_i)}{-\\langle n_B\\rangle -\\ln(N!) + \\sum_{i=1}^N \\ln(\\langle n_B\\rangle B_i)}\n",
    "            = -\\hat{n}_S + \\sum_{i=1}^N \\ln\\left( \\frac{\\hat{n}_S S_i}{\\langle n_B\\rangle B_i} + 1 \\right)\n",
    "$$\n",
    "\n",
    "The per event PDFs $S_i$ and $B_i$ can depend on arbitrary parameters.\n",
    "The common choise here is to use a time, energy proxy and spatial proxy depency which has most seperation power:\n",
    "\n",
    "$$\n",
    "    S_i(x_i, t_i, E_i) = S_T(t_i) \\cdot S_S(x_i) \\cdot S_E(E_i) \\\\ \n",
    "    B_i(x_i, t_i, E_i) = B_T(t_i) \\cdot B_S(x_i) \\cdot B_E(E_i) \n",
    "$$\n",
    "\n",
    "Because the Likelihood only contains ratios of the PDF, we only have to construct 1D PDFs of the signal to background ratio for each time, spatial and energy distribution.\n",
    "\n",
    "**Why not using a combined spatial/energy PDF? Like the one used to draw background events. See coenders talk 4 for exactly that**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Time PDF\n",
    "\n",
    "The time pdf is starting at the source events time with a given time window.\n",
    "The edges are falling off like gaussian with a relativ length.\n",
    "Edge gaussians are truncated to 0 after 4 sigma to avoid more calculation with no effect, as the function is almost zero after 5 sigma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def time_pdf(t, window_size, trunc=4):\n",
    "    \"\"\"\n",
    "    t in seconds\n",
    "    \n",
    "    window_size in seconds\n",
    "    \n",
    "    trunc in units of sigma\n",
    "    \"\"\"\n",
    "    # Set edge width to window size\n",
    "    gaus_sigma = window_size\n",
    "    sigma_llim, sigma_ulim = 2, 30\n",
    "    # Limit size of edges in both directions\n",
    "    if gaus_sigma < sigma_llim:\n",
    "        gaus_sigma = sigma_llim\n",
    "    elif gaus_sigma > sigma_ulim:\n",
    "        gaus_sigma = sigma_ulim\n",
    "        \n",
    "    # Get the pdfs\n",
    "    # Set support (truncation) for gaussian kernel\n",
    "    gaus = scs.norm(a=-trunc * gaus_sigma, b=+trunc * gaus_sigma)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

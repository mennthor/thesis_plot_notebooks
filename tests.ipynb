{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mpldates\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.colors import LogNorm\n",
    "%matplotlib inline\n",
    "\n",
    "import scipy.interpolate as sci\n",
    "import scipy.optimize as sco\n",
    "\n",
    "import json\n",
    "import datetime\n",
    "import pickle\n",
    "from astropy.time import Time as astrotime\n",
    "\n",
    "import sklearn.neighbors as skn\n",
    "import sklearn.model_selection as skms  # Newer version of grid_search\n",
    "\n",
    "from corner_hist import corner_hist\n",
    "from anapymods3.plots.general import split_axis, get_binmids, hist_marginalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "\n",
    "Load IC86 data from epinat, which should be the usual IC86-I (2011) PS sample, but pull corrected and OneWeights corrected by number of events generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exp = np.load(\"data/IC86_I_data.npy\")\n",
    "mc = np.load(\"data/IC86_I_mc.npy\")\n",
    "\n",
    "# Use the officially stated livetime, not the ones from below\n",
    "livetime = 332.61"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Get data livetime\n",
    "\n",
    "Generate from good run list as stated here:\n",
    "- http://icecube.wisc.edu/~coenders/html/build/html/ic86-bdt/muonL3.html\n",
    "- https://wiki.icecube.wisc.edu/index.php/IC86_I_Point_Source_Analysis/Data_and_Simulation\n",
    "\n",
    "It should be 332.61 days as stated by jefeintzeig and scoenders.\n",
    "We create one bin per included run, with exactly that width.\n",
    "Excluded runs are those with too high/low rate and without everything marked \"good\".\n",
    "\n",
    "Livetime ist a bit higher, because we used a newer runlist from iclive instead of the old non-json v1.4.\n",
    "See side test for that comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Grab from json\n",
    "jsonFile = open('data/ic86-i-goodrunlist.json', 'r')\n",
    "grlist = json.load(jsonFile)\n",
    "jsonFile.close()\n",
    "\n",
    "# This is a list of dicts (one dict per run)\n",
    "runs = grlist[\"runs\"]\n",
    "# This is a dict of arrays (all run values in an array per keyword)\n",
    "run_dict = dict(zip(runs[0].keys(), zip(*[r.values() for r in runs])))\n",
    "for k in run_dict.keys():\n",
    "    run_dict[k] = np.array(run_dict[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Now compile runs as stated on jfeintzeigs page\n",
    "\n",
    "# Transform livetimes to MJD floats\n",
    "start_mjd = astrotime(run_dict[\"good_tstart\"]).mjd\n",
    "stop_mjd = astrotime(run_dict[\"good_tstop\"]).mjd\n",
    "\n",
    "# Create recarry to apply mask, only keep start, stop and runID\n",
    "dtype = [(\"start_mjd\", np.float), (\"stop_mjd\", np.float), (\"runID\", np.int)]\n",
    "run_arr = np.array(list(zip(start_mjd, stop_mjd, run_dict[\"run\"])), dtype=dtype)\n",
    "\n",
    "# Note: The last 2 runs aren't included anyway, so he left them out in\n",
    "# the reported run list. This fits here, as the other 4 runs are found\n",
    "# in the list.\n",
    "exclude_rate = [120028, 120029, 120030, 120087, 120156, 120157]\n",
    "i3good = run_dict[\"good_i3\"] == True\n",
    "itgood = run_dict[\"good_it\"] == True\n",
    "ratebad = np.in1d(run_dict[\"run\"], exclude_rate)\n",
    "\n",
    "# Include if it & i3 good and rate is good\n",
    "include = i3good & itgood & ~ratebad\n",
    "inc_run_arr = run_arr[include]\n",
    "\n",
    "# Get the total and per run livetimes in mjd\n",
    "runtimes_mjd = inc_run_arr[\"stop_mjd\"] - inc_run_arr[\"start_mjd\"]\n",
    "_livetime = np.sum(runtimes_mjd)\n",
    "\n",
    "print(\"IC86-I livetime from iclive: \", _livetime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Bin BG according to runlist\n",
    "\n",
    "Each run is one bin in the bg rate vs time plot.\n",
    "The rate is normed to Hertz by dividing through the bin sizes in seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Store events in bins with run borders\n",
    "exp_times = exp[\"timeMJD\"]\n",
    "start_mjd = inc_run_arr[\"start_mjd\"]\n",
    "stop_mjd = inc_run_arr[\"stop_mjd\"]\n",
    "\n",
    "tot = 0\n",
    "evts_in_run = {}\n",
    "for start, stop , runid in zip(start_mjd, stop_mjd, inc_run_arr[\"runID\"]):\n",
    "    mask = (exp_times >= start) & ( exp_times < stop)\n",
    "    evts_in_run[runid] = exp[mask]\n",
    "    tot += np.sum(mask)\n",
    "    \n",
    "# Crosscheck, if we got all events and counted nothing double\n",
    "print(\"Do we have all events? \", tot == len(exp))\n",
    "print(\"  Events selected : \", tot)\n",
    "print(\"  Events in exp   : \", len(exp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create binmids and histogram values in each bin\n",
    "binmids = 0.5 * (start_mjd + stop_mjd)\n",
    "h = np.zeros(len(binmids), dtype=np.float)\n",
    "\n",
    "for i, evts in enumerate(evts_in_run.values()):\n",
    "    h[i] = len(evts)\n",
    "    \n",
    "# Mask those with zero rate\n",
    "m = h > 0.\n",
    "binmids = binmids[m]\n",
    "h = h[m]\n",
    "    \n",
    "# Create plot arrays\n",
    "xerr = runtimes_mjd[m] / 2.\n",
    "yerr = np.sqrt(h)\n",
    "\n",
    "# Show in Hertz, so go from MJD days to seconds in bin widths\n",
    "secsinday = 24. * 60. * 60\n",
    "norm = (stop_mjd[m] - start_mjd[m]) * secsinday\n",
    "h_norm = h / norm\n",
    "# Poisson errors just get scaled\n",
    "yerr_norm = yerr / norm\n",
    "\n",
    "# Weights only for the weighted average\n",
    "weights = np.ones_like(yerr)\n",
    "weights[yerr_norm == 0] = 0\n",
    "weights[yerr_norm != 0] = 1 / yerr[yerr_norm != 0]\n",
    "def f(x, a, b, c):\n",
    "    \"\"\"Fix baseline to wighted average\"\"\"\n",
    "    return a * np.sin(b * (normed - c)) + np.average(h_norm, weights=weights)\n",
    "normed = (binmids - binmids.min()) / (binmids.max() - binmids.min())\n",
    "\n",
    "# Scaled seed from handcrafted guess in cell below\n",
    "p0 = [-0.0005, 2 * np.pi, 0.1]\n",
    "\n",
    "# Fit a poly to the rate. No weights, because we threw out entries with 0\n",
    "# Also with weight, the period is only have despite the good seed values...\n",
    "res = sco.curve_fit(f=f, xdata=normed, ydata=h_norm, p0=p0)\n",
    "pars = res[0]\n",
    "\n",
    "print(\"Best fit pars : \", pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot like mrichman did on p. 113\n",
    "# Note: Date plots are THE MOST DIFFICULT AND LEAST FUN THING TODO...\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "# Show dates on x axis\n",
    "datetimes = astrotime(binmids, format=\"mjd\").to_datetime()\n",
    "dates = mpldates.date2num([dt.date() for dt in datetimes])\n",
    "\n",
    "# Every month, first day\n",
    "months = mpldates.MonthLocator(bymonth=np.arange(1, 13), bymonthday=1)\n",
    "monthsFmt = mpldates.DateFormatter(\"%b %Y\")\n",
    "ax.xaxis.set_major_locator(months)\n",
    "ax.xaxis.set_major_formatter(monthsFmt)\n",
    "\n",
    "ax.errorbar(dates, h_norm, fmt=\".\", xerr=xerr, yerr=yerr_norm)\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Rate in HZ\")\n",
    "ax.set_xlim(dates[0], dates[-1])\n",
    "ax.set_ylim(0., None)\n",
    "\n",
    "# Plot polyfit\n",
    "delta_days = (datetimes[-1] - datetimes[0]).days\n",
    "xdatetimes = [datetimes[0] + datetime.timedelta(days=int(x))for x in\n",
    "              np.arange(0, delta_days)]\n",
    "xtimes_mjd = astrotime(xdatetimes).mjd\n",
    "normed = (xtimes_mjd - binmids.min()) / (binmids.max() - binmids.min())\n",
    "y = f(normed, *pars)\n",
    "\n",
    "# Handcrafted seed trial & error\n",
    "# s = [-0.0005, 2 * np.pi, 0.1]\n",
    "# y = s[0] * np.sin(s[1] * (normed + s[2])) + np.average(h_norm, weights=weights)\n",
    "\n",
    "# Convert back to mpl dates\n",
    "xdates = mpldates.date2num([xd.date() for xd in xdatetimes])\n",
    "ax.plot(xdates, y, \"r-\", zorder=5)\n",
    "ax.axhline(np.average(h_norm, weights=weights), 0, 1, color=\"k\", ls=\"--\", zorder=5)\n",
    "\n",
    "# Autoprettify main xlabels\n",
    "fig.autofmt_xdate(rotation=60)\n",
    "\n",
    "# Show mjd on top\n",
    "def ax2ticker(x):\n",
    "    dates = mpldates.num2date(x)\n",
    "    mjd = astrotime(dates).mjd\n",
    "    return mjd\n",
    "ax2 = ax.twiny()\n",
    "ax2.set_xticks(ax.get_xticks())\n",
    "ax2.set_xbound(ax.get_xbound())\n",
    "ax2.set_xticklabels(ax2ticker(ax.get_xticks()),\n",
    "                    rotation=60, horizontalalignment=\"left\")\n",
    "ax2.set_xlabel(\"MJD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's make the BG pdf\n",
    "\n",
    "Proceeding to section 6.3.1 Randomized BG Injection, p. 113.\n",
    "Mrichmann draws events by:\n",
    "\n",
    "1. Get number of bg events to be injected from a poisson distribution with expectation values drawn from the previously build bg temporal distribution.\n",
    "   $$\n",
    "   P_{\\langle n_B\\rangle}(N_m) = \\frac{\\langle n_B\\rangle^{N_m}}{N_m\\!}\\cdot \\exp(\\langle n_B\\rangle)\n",
    "   $$\n",
    "2. These events are then drawn from a 3D pdf in energy proxy, zenith proxy and sigma proxy.\n",
    "   He does it by dividing 10x10x10 bins, first selecting energy, then zenith in that energy bin, then sigma in that zenith bin.\n",
    "   \n",
    "Here we create a smooth PDF using a kernel density estimator and obtain a sample by running a MCMC chain to create a sample a priori.\n",
    "The bandwidth is set globally and cross validated to be robust.\n",
    "\n",
    "**Some note on `numpy.histogramdd`:**\n",
    "\n",
    "The input must be an array with shape (nDim, len(data)).\n",
    "\n",
    "Shape of h is the same as the number of bins in each dim: (50, 40, 10)\n",
    "So the first dimension picks a single logE slice -> h[i].shape = (40, 10)\n",
    "Second dim picks a dec slice -> h[:, i].shape = (50, 10)\n",
    "3rd picks a sigma slice -> h[:, :, i].shape = (50, 40)\n",
    "\n",
    "This is important: meshgrid repeats in second axis on first array xx.\n",
    "For the second array, the first axis is repeated.\n",
    "But h iterates over energy in 1st axis. So if we don't transpose, we have the whole histogram flipped! Compare to plot in mrcihmanns thesis (cos(zen))\n",
    "\n",
    "**Some notes on KDE:**\n",
    "\n",
    "Sebastian has already made a tool for adaptive and asymmetric KDE.\n",
    "1. The Kernel is the covariance matrix of the whole data set to regard different scales\n",
    "    + Note: This may only be a problem, if one dim is spread with peaks, while the other is wide spread only. Then we cannot scale the Kernel to small to fit the peaks because the smooth dimension is preventing that.\n",
    "2. Use Silvermans or Scotts rule as a first guess.\n",
    "3. Run a second pass and vary the local bandwidth according to the first guess local density.\n",
    "\n",
    "We could replace 1 and 2 by scaling the data with the inverse covariance and then using a cross validation to find the first guess bandwidth.\n",
    "Then using a second pass to vary locally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D histogram\n",
    "First we make a 3D histogram to better compare to mrichmann and to get an overview over the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# HANDTUNED scale parameter to \"fit\" KDE expectation to data...\n",
    "# TODO: Use Adaptive kernel width and asymmetric gaus kernels\n",
    "#       For sigma it might make sense to a take a restricted kernel [0, inf]\n",
    "fac_logE = 1.5\n",
    "fac_dec = 2.5\n",
    "fac_sigma = 2.\n",
    "\n",
    "logE = fac_logE * exp[\"logE\"]\n",
    "sigma = fac_sigma * np.rad2deg(exp[\"sigma\"])\n",
    "# np.cos(np.pi / 2. + exp[\"dec\"]); dec is for {sin(dec), dec, cos(zen)}\n",
    "dec = fac_dec * exp[\"dec\"]\n",
    "\n",
    "# Binning is rather arbitrary because we don't calc stuff with the hist\n",
    "bins = [50, 50, 50]\n",
    "# Range for sigma is picked by looking at the 1D distribution, rest is default\n",
    "r = [[np.amin(logE), np.amax(logE)],\n",
    "     [np.amin(dec), np.amax(dec)],\n",
    "     [0., fac_sigma * 5.]]\n",
    "\n",
    "sample = np.vstack((logE, dec, sigma)).T\n",
    "h, bins = np.histogramdd(sample=sample, bins=bins, range=r, normed=True)\n",
    "\n",
    "# Make bin mids for later use\n",
    "mids = []\n",
    "for b in bins:\n",
    "    mids.append(0.5 * (b[:-1] + b[1:]))\n",
    "\n",
    "# Make a nice corner plot\n",
    "fig, ax = corner_hist(h, bins=bins,\n",
    "                      label=[\"logE\", \"sin(dec)\", \"sigma deg\"],\n",
    "                      hist2D_args={\"cmap\": \"Greys\"},\n",
    "                      hist_args={\"color\":\"#353132\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Density Estimation\n",
    "\n",
    "We use scikit learn's cross validation with a gaussian kernel to get the most robust bandwidth.\n",
    "Then we integrate with the same binning as above and compare to the 3D histogram.\n",
    "\n",
    "This section relies heavily on [Jake van der Plas examples for KDE](https://jakevdp.github.io/blog/2013/12/01/kernel-density-estimation/).\n",
    "More info on how KDE cross validation works can be found in [Modern Nonparametric Methods](http://www2.stat.duke.edu/~wjang/teaching/S05-293/lecture/ch6.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "````\n",
    "# KDE CV is running on cluster and pickles the GridSearchCV\n",
    "fname = \"data/KDE_model_selector_CV20_exp_IC86_I.pickle\"\n",
    "with open(fname, \"rb\") as f:\n",
    "    model_selector = pickle.load(f)\n",
    "\n",
    "kde = model_selector.best_estimator_\n",
    "bw = model_selector.best_params_[\"bandwidth\"]\n",
    "print(\"Best bandwidth : {:.3f}\".format(bw))\n",
    "\n",
    "# Estimate pdf for data sample with best model\n",
    "kde.fit(sample)\n",
    "\n",
    "# Generate some BG samples to compare to the original data hist\n",
    "bg_samples = kde.sample(n_samples=2 * len(exp))\n",
    "\n",
    "# Make histogram with same binning as original data\n",
    "bg_h, bg_bins = np.histogramdd(sample=sample, bins=bins, range=r)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kde = skn.KernelDensity(rtol=1e-8, kernel=\"gaussian\", bandwidth=0.05)\n",
    "\n",
    "# Estimate pdf for data sample with best model\n",
    "kde.fit(sample)\n",
    "\n",
    "# Generate some BG samples to compare to the original data hist.\n",
    "# Use more statistics, histograms get normalized and we want the best estimate\n",
    "# for the pdf\n",
    "nsamples_kde = int(1e7)\n",
    "bg_samples = kde.sample(n_samples=nsamples_kde)\n",
    "\n",
    "# Make histogram with same binning as original data\n",
    "bg_h, bg_bins = np.histogramdd(sample=bg_samples, bins=bins, range=r, normed=True)\n",
    "\n",
    "fig, ax = corner_hist(bg_h, bins=bg_bins,\n",
    "                      label=[\"logE\", \"sin(dec)\", \"sigma deg\"],\n",
    "                      hist2D_args={\"cmap\": \"Greys\"},\n",
    "                      hist_args={\"color\":\"#353132\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare KDE to original data\n",
    "\n",
    "Make a ratio histogram of the KDE sample and the original data sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def marginalize_kde(sample, rnge, nvals, axis):\n",
    "    \"\"\"\n",
    "    Integrate out the KDE to 1D by using a large sample and do a MC\n",
    "    integration by simply counting all point in that range.\n",
    "    \"\"\"\n",
    "    # Make bins\n",
    "    bins = np.linspace(rnge[0], rnge[1], nvals)\n",
    "    y, bins = np.histogram(sample.T[int(axis)], bins=bins, density=True)\n",
    "    x = 0.5 * (bins[:-1] + bins[1:])  \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D marginalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xlabel = [\"scaled \" + s for s in [\"logE\", \"dec\", \"sigma °\"]]\n",
    "\n",
    "for i, axes in enumerate([(2), (1), (0)]):\n",
    "    h_exp, b_exp = hist_marginalize(h, bins, axes=axes)\n",
    "    h_kde, b_kde = hist_marginalize(bg_h, bg_bins, axes=axes)\n",
    "    \n",
    "    # KDE is expectation, but sampled with much more events.\n",
    "    # Weights would simply scale the total number of KDE events to match the\n",
    "    # number of original events. That would be the mean for the poisson\n",
    "    # distribution in each bin. So to get OK KDE expectation sqrt(n) errors\n",
    "    # in each bin, we divide not by the number of drawn KDE but by the number\n",
    "    # of original events.\n",
    "    _b = b_kde\n",
    "    diffXX, _ = np.meshgrid(np.diff(_b[0]), np.diff(_b[1]))\n",
    "    # Again shapes of meshgrid and hist are transposed\n",
    "    norm_kde = len(exp) * diffXX.T\n",
    "    sigma_kde = np.sqrt(h_kde / norm_kde)\n",
    "\n",
    "    # Make 3 different diff/ratio hists to estimate KDE quality in\n",
    "    # 1D marginalization.\n",
    "    m = (h_exp > 0.)\n",
    "    ratio_h = np.zeros_like(h_exp)\n",
    "    ratio_h[m] = h_kde[m] / h_exp[m]\n",
    "\n",
    "    diff_h = h_kde - h_exp\n",
    "\n",
    "    m = (sigma_kde > 0.)\n",
    "    sigma_ratio_h = np.zeros_like(h_exp)\n",
    "    sigma_ratio_h[m] = (h_exp[m] - h_kde[m]) / sigma_kde[m]\n",
    "\n",
    "    # Bin mids and hist grid\n",
    "    _b = b_exp\n",
    "    m = get_binmids(_b)\n",
    "    xx, yy = map(np.ravel, np.meshgrid(m[0], m[1]))\n",
    "    \n",
    "    \n",
    "    # Big plot on the left and three right\n",
    "    fig = plt.figure(figsize=(10, 6))\n",
    "    gs = gridspec.GridSpec(3, 3)\n",
    "    axl = fig.add_subplot(gs[:, :2])\n",
    "    axrt = fig.add_subplot(gs[0, 2])\n",
    "    axrc = fig.add_subplot(gs[1, 2])\n",
    "    axrb = fig.add_subplot(gs[2, 2])\n",
    "    \n",
    "    # Steal space for colorbars\n",
    "    caxl = split_axis(axl, \"right\")\n",
    "    caxrt = split_axis(axrt, \"right\")\n",
    "\n",
    "    # Unset top and center xticklabels as they are shared with the bottom plot\n",
    "    axrt.set_xticklabels([])\n",
    "    axrc.set_xticklabels([])\n",
    "    # Unset top yticklabels in favour of a colorbar\n",
    "    axrt.set_yticklabels([])\n",
    "\n",
    "    # Set ticks and labels right\n",
    "    for ax in [axrt, axrc, axrb]:\n",
    "        ax.yaxis.set_label_position(\"right\")\n",
    "        ax.yaxis.tick_right()\n",
    "        \n",
    "    # Left: Difference over KDE sigma\n",
    "    # Center colormap\n",
    "    cbar_extr = max(np.amax(sigma_ratio_h), abs(np.amin(sigma_ratio_h)))\n",
    "    _, _, _, imgl = axl.hist2d(xx, yy, bins=_b, weights=sigma_ratio_h.T.ravel(),\n",
    "               cmap=\"seismic\", vmax=5, vmin=-5)\n",
    "    cbarl = plt.colorbar(cax=caxl, mappable=imgl)\n",
    "    axl.set_xlabel(xlabel[i])\n",
    "    axl.set_title(\"(exp - kde) / sigma_kde\")\n",
    "    \n",
    "    # Right top: Ratio\n",
    "    _, _, _, imgrt = axrt.hist2d(xx, yy, bins=_b, weights=ratio_h.T.ravel(),\n",
    "               cmap=\"seismic\", vmax=2, vmin=0);\n",
    "    cbarl = plt.colorbar(cax=caxrt, mappable=imgrt)\n",
    "    axl.set_xlabel(xlabel[i])\n",
    "    axrt.set_title(\"kde / exp\")\n",
    "\n",
    "    # Right center: Data hist\n",
    "    axrc.hist2d(xx, yy, bins=_b, weights=h_exp.T.ravel(),\n",
    "               cmap=\"Greys\");\n",
    "\n",
    "    # Right bottom: KDE hist, same colorbar scale as on data\n",
    "    axrb.hist2d(xx, yy, bins=_b, weights=h_kde.T.ravel(),\n",
    "               cmap=\"Greys\");\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D marginalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xlabel = [\"scaled \" + s for s in [\"logE\", \"dec\", \"sigma °\"]]\n",
    "\n",
    "for i, axes in enumerate([(1, 2), (0, 2), (0, 1)]):\n",
    "    h_exp, b_exp = hist_marginalize(h, bins, axes=axes)\n",
    "    h_kde, b_kde = hist_marginalize(bg_h, bg_bins, axes=axes)\n",
    "      \n",
    "    # KDE is expectation, but sampled with much more events.\n",
    "    # Weights would simply scale the total number of KDE events to match the\n",
    "    # number of original events. That would be the mean for the poisson\n",
    "    # distribution in each bin. So to get OK KDE expectation sqrt(n) errors\n",
    "    # in each bin, we divide not by the number of drawn KDE but by the number\n",
    "    # of original events.\n",
    "    norm_kde = len(exp) * np.diff(b_kde)\n",
    "    sigma_kde = np.sqrt(h_kde / norm_kde)\n",
    "\n",
    "    # Make 3 different diff/ratio hists to estimate KDE quality in\n",
    "    # 1D marginalization.\n",
    "    m = (h_exp > 0.)\n",
    "    ratio_h = np.zeros_like(h_exp)\n",
    "    ratio_h[m] = h_kde[m] / h_exp[m]\n",
    "\n",
    "    diff_h = h_kde - h_exp\n",
    "\n",
    "    m = (sigma_kde > 0.)\n",
    "    sigma_ratio_h = np.zeros_like(h_exp)\n",
    "    sigma_ratio_h[m] = (h_exp[m] - h_kde[m]) / sigma_kde[m]\n",
    "\n",
    "    # Bin mids\n",
    "    _b = b_exp\n",
    "    m = get_binmids([_b])[0]\n",
    "    \n",
    "    # Plot both and the ration normed. Big plot on the left and three right\n",
    "    fig = plt.figure(figsize=(10, 6))\n",
    "    gs = gridspec.GridSpec(3, 3)\n",
    "    axl = fig.add_subplot(gs[:, :2])\n",
    "    axrt = fig.add_subplot(gs[0, 2])\n",
    "    axrc = fig.add_subplot(gs[1, 2])\n",
    "    axrb = fig.add_subplot(gs[2, 2])\n",
    "\n",
    "    axrt.set_xticklabels([])\n",
    "    axrc.set_xticklabels([])\n",
    "\n",
    "    # Set ticks and labels right\n",
    "    for ax in [axrt, axrc, axrb]:\n",
    "        ax.yaxis.set_label_position(\"right\")\n",
    "        ax.yaxis.tick_right()\n",
    "\n",
    "    # Limits\n",
    "    for ax in [axl, axrt, axrc, axrb]:\n",
    "        ax.set_xlim(_b[0], _b[-1])\n",
    "        \n",
    "    # Main plot:\n",
    "    # Plot more dense to mimic a smooth curve\n",
    "    __h, __b = np.histogram(bg_samples[:, i], bins=500,\n",
    "                            range=[_b[0], _b[-1]], density=True)\n",
    "    __m = get_binmids([__b])[0]\n",
    "    axl.plot(__m, __h, lw=3, alpha=0.5)\n",
    "    \n",
    "    _ = axl.hist(m, bins=_b, weights=h_exp, label=\"exp\", histtype=\"step\",\n",
    "                 lw=2, color=\"k\")\n",
    "    _ = axl.errorbar(m, h_kde, yerr=sigma_kde, fmt=\",\", color=\"r\")\n",
    "    _ = axl.hist(m, bins=_b, weights=h_kde, label=\"kde\", histtype=\"step\",\n",
    "                 lw=2, color=\"r\")    \n",
    "    \n",
    "    axl.set_xlabel(xlabel[i])\n",
    "    axl.legend(loc=\"upper right\")\n",
    "\n",
    "    # Top right: Difference\n",
    "    _ = axrt.axhline(0, 0, 1, color=\"k\", ls=\"-\")\n",
    "    _ = axrt.hlines([-.02, -.01, .01, .02], _b[0], _b[-1],\n",
    "                    colors='#353132', linestyles='dashed')\n",
    "    _ = axrt.hist(m, bins=_b, weights=diff_h, histtype=\"step\", lw=2, color=\"r\")\n",
    "    axrt.set_ylim(-.05, +.05)\n",
    "    axrt.set_ylabel(\"kde - exp\")\n",
    "\n",
    "    # Center right: Ratio\n",
    "    _ = axrc.axhline(1, 0, 1, color=\"k\", ls=\"-\")\n",
    "    _ = axrc.hlines([0.8, 0.9, 1.1, 1.2], _b[0], _b[-1],\n",
    "                    colors='#353132', linestyles='dashed')\n",
    "    _ = axrc.hist(m, bins=_b, weights=ratio_h, histtype=\"step\", lw=2, color=\"r\")\n",
    "    axrc.set_ylim(.5, 1.5)\n",
    "    axrc.set_ylabel(\"kde / exp\")\n",
    "\n",
    "    # Bottom right: Ratio of diff to sigma of expectation\n",
    "    _ = axrb.axhline(0, 0, 1, color=\"k\", ls=\"-\")\n",
    "    _ = axrb.hlines([-2, -1, 1, 2], _b[0], _b[-1],\n",
    "                    colors='#353132', linestyles='dashed')\n",
    "    _ = axrb.hist(m, bins=_b, weights=sigma_ratio_h, histtype=\"step\", lw=2, color=\"r\")\n",
    "    axrb.set_ylim(-3, +3)\n",
    "    axrb.set_ylabel(\"(exp-kde)/sigma_kde\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

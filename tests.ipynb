{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import helper as hlp\n",
    "\n",
    "import numpy as np\n",
    "from numpy.lib.recfunctions import drop_fields\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mpldates\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.colors import LogNorm\n",
    "import matplotlib.animation as animation\n",
    "%matplotlib inline\n",
    "\n",
    "import scipy.interpolate as sci\n",
    "import scipy.optimize as sco\n",
    "import scipy.stats as scs\n",
    "import scipy.integrate as scint\n",
    "\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import pickle\n",
    "from astropy.time import Time as astrotime\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import sklearn.neighbors as skn\n",
    "import sklearn.model_selection as skms  # Newer version of grid_search\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "from anapymods3.healpy import wrap_theta_phi_range\n",
    "from anapymods3.plots.astro import skymap, EquCoordsToMapCoords\n",
    "from anapymods3.plots import split_axis, get_binmids, hist_marginalize, dg\n",
    "from anapymods3.stats.sampling import rejection_sampling\n",
    "from anapymods3.stats import KDE, json2kde\n",
    "\n",
    "# Some globals\n",
    "hoursindays = 24.\n",
    "secinday = hoursindays * 60. * 60. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Load IC86 data from epinat, which should be the usual IC86-I (2011) PS sample, but pull corrected and OneWeights corrected by number of events generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp = np.load(\"data/IC86_I_data.npy\")\n",
    "mc = np.load(\"data/IC86_I_mc.npy\")\n",
    "# Use the officially stated livetime, not the ones from below\n",
    "livetime = 332.61"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Get data livetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Generate from good run list as stated here:\n",
    "- http://icecube.wisc.edu/~coenders/html/build/html/ic86-bdt/muonL3.html\n",
    "- https://wiki.icecube.wisc.edu/index.php/IC86_I_Point_Source_Analysis/Data_and_Simulation\n",
    "\n",
    "It should be 332.61 days as stated by jefeintzeig and scoenders.\n",
    "We create one bin per included run, with exactly that width.\n",
    "Excluded runs are those with too high/low rate and without everything marked \"good\".\n",
    "\n",
    "Livetime ist a bit higher, because we used a newer runlist from iclive instead of the old non-json v1.4.\n",
    "See side test for that comparison.\n",
    "\n",
    "Problem is also, that this runlist includes runs with zero events, so they are probably cut out due to the old runlist in the original selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def filter_runs(run):\n",
    "    \"\"\"\n",
    "    Filter runs as stated in jfeintzig's doc.\n",
    "    \"\"\"\n",
    "    exclude_runs = [120028, 120029, 120030, 120087, 120156, 120157]\n",
    "    if ((run[\"good_i3\"] == True) & (run[\"good_it\"] == True) &\n",
    "        (run[\"run\"] not in exclude_runs)):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "goodrun_dict, _livetime = hlp.create_goodrun_dict(\n",
    "    runlist=\"data/runlists/ic86-i-goodrunlist.json\", filter_runs=filter_runs)\n",
    "\n",
    "# We don't use this livetime, but the \"official\" one from jfeintzeig's page\n",
    "print(\"IC86-I livetime from iclive: \", _livetime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Bin BG according to runlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Each run is one bin in the bg rate vs time plot.\n",
    "The rate is normed to Hertz by dividing through the bin sizes in seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "h = hlp._create_runtime_bins(exp[\"timeMJD\"], goodrun_dict=goodrun_dict,\n",
    "                             remove_zero_runs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot runs\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "start, stop = h[\"start_mjd\"], h[\"stop_mjd\"]\n",
    "rate = h[\"rate\"]\n",
    "xerr = 0.5 * (stop - start)\n",
    "yerr = h[\"rate_std\"]\n",
    "binmids = 0.5 * (stop + start)\n",
    "ax.errorbar(binmids, rate, xerr=xerr, yerr=yerr, fmt=\",\")\n",
    "\n",
    "# Setup main axis\n",
    "ax.set_xlim(start[0], stop[-1])\n",
    "ax.set_ylim(0, None)\n",
    "ax.set_xlabel(\"MJD\")\n",
    "ax.set_ylabel(\"Rate in Hz\")\n",
    "# Rotate bottom labels if needed\n",
    "# def xlabels(x):\n",
    "#     return [\"{:5d}\".format(int(xi)) for xi in x]\n",
    "# ax.set_xticklabels(xlabels(ax.get_xticks()), rotation=60,\n",
    "#                    horizontalalignment=\"right\")\n",
    "\n",
    "# Second xaxis on top with month and year.\n",
    "# Convert MJD to datetimes, make dates for every month and convert to mjd\n",
    "# http://stackoverflow.com/questions/22696662/ \\\n",
    "#   python-list-of-first-day-of-month-for-given-period\n",
    "datetimes = astrotime(binmids, format=\"mjd\").to_datetime()\n",
    "dt, end = datetimes[0], datetimes[-1]\n",
    "datetimes_ticks = []\n",
    "while dt < end:\n",
    "    if not dt.month % 12:\n",
    "        dt = datetime.datetime(dt.year + 1, 1, 1)\n",
    "    else:\n",
    "        dt = datetime.datetime(dt.year, dt.month + 1, 1)\n",
    "    datetimes_ticks.append(dt)\n",
    "mjd_ticks = astrotime(datetimes_ticks, format=\"datetime\").mjd\n",
    "\n",
    "# New axis on top, make sure, we use the same range\n",
    "ax2 = ax.twiny()\n",
    "ax2.set_xlim(ax.get_xlim())\n",
    "ax2.set_xticks(mjd_ticks)\n",
    "ax2.set_xticklabels([dtt.strftime(\"%b '%y\") for dtt in datetimes_ticks],\n",
    "                    rotation=60, horizontalalignment=\"left\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Time dependent rate function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Rate ist time dependent because of seasonal variation.\n",
    "We take this varariation into account by fitting a priodic function to the time resolved rate.\n",
    "\n",
    "The data is built by calculating the rate in each run as seen before.\n",
    "This rate is correctly normalized and smoothes local fluctuations.\n",
    "\n",
    "### Peridoc function with a weighted least squares fit\n",
    "\n",
    "See side_test for comparison to spline fits.\n",
    "The function is a simple sinus scalable by 4 parameters to fit the shape of the rates:\n",
    "\n",
    "$$\n",
    "    f(x) = a\\cdot \\sin(b\\cdot(x - c)) + d\n",
    "$$\n",
    "\n",
    "The least squares loss function is\n",
    "\n",
    "$$\n",
    "    R = \\sum_i (w_i(y_i - f(x_i)))^2\n",
    "$$\n",
    "\n",
    "Weights usually are standard deviations from each data point.\n",
    "But here we have a counting experiment for each run and we want to give higher statistics runs a higher weight so we shouldn't use Poisson errors directly as they scale with the number of counts.\n",
    "The relative error instead drops with higher count rate so we use the inverse of the relative error to weight each point.\n",
    "\n",
    "$$\n",
    "    w_i = \\frac{N}{\\sqrt{N}} = \\sqrt{N}\n",
    "$$\n",
    "\n",
    "Also this has the nice property of excluding zero entry bins.\n",
    "\n",
    "Seed values are estimated from plot rate vs time.\n",
    "\n",
    "- Period should be 365 days (MJD) because we have one year of data so we choose $b0 = 2\\pi/365$.\n",
    "- Amplitude is about $a_0=-0.0005$, because sinus seems to start with negative values.\n",
    "- The x-offset is choose as the first start date, to get the right order of magnitude.\n",
    "- The y-axis intersection $d$ schould be close to the weighted average, so we take this as a seed.\n",
    "\n",
    "The bounds are motivated as follows (and if we don't hit them, it's OK to use them).\n",
    "\n",
    "- Amplitude $a$ should be positive, this also resolves a degenracy between a-axis offset.\n",
    "- The period $b$ should scatter around one year, a period larger than +-1 half a year is unphysical.\n",
    "- The x-offset $c$ cannot be greater than the initial +- the period because we have a periodic function.\n",
    "- The y-axis offset $d$ is arbitrarily constrained, but as seen from the plot it should not exceed 0.1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def f(x, pars):\n",
    "    \"\"\"\n",
    "    Returns the rate at a given time in MJD.\n",
    "    \"\"\"\n",
    "    a, b, c, d = pars\n",
    "    return a * np.sin(b * (x - c)) + d\n",
    "\n",
    "def lstsq(pars, *args):\n",
    "    \"\"\"\n",
    "    Weighted leastsquares min sum((wi * (yi - fi))**2)\n",
    "    \"\"\"\n",
    "    # data x,y-values and weights are fixed\n",
    "    x, y, w = args\n",
    "    _f = f(x, pars)\n",
    "    return np.sum((w * (y - _f))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Seed values from consideration above.\n",
    "# a0 = -0.0005\n",
    "# b0 = 2. * np.pi / 365.  # We could restrict the period to one yr exact.\n",
    "# c0 = np.amin(start_mjd)\n",
    "# d0 = np.average(h, weights=yerr**2)\n",
    "\n",
    "rate = h[\"rate\"]\n",
    "rate_std = h[\"rate_std\"]\n",
    "X = exp[\"timeMJD\"]\n",
    "binmids = 0.5 * (h[\"start_mjd\"] + h[\"stop_mjd\"])\n",
    "\n",
    "a0 = 0.5 * (np.amax(rate) - np.amin(rate))\n",
    "b0 = 2. * np.pi / 365.\n",
    "c0 = np.amin(X)\n",
    "d0 = np.average(rate, weights=rate_std**2)\n",
    "\n",
    "x0 = [a0, b0, c0, d0]\n",
    "# Bounds as explained above\n",
    "bounds = [[None, None], [0.5 * b0, 1.5 * b0], [c0 - b0, c0 + b0, ], [0, 0.01]]\n",
    "# x, y values, weights\n",
    "args = (binmids, rate, np.sqrt(rate_std))\n",
    "\n",
    "res = sco.minimize(fun=lstsq, x0=x0, args=args, bounds=bounds)\n",
    "bf_pars = res.x\n",
    "\n",
    "print(\"Amplitude   : {: 13.5f} in Hz\".format(res.x[0]))\n",
    "print(\"Period (d)  : {: 13.5f} in days\".format(2 * np.pi / res.x[1]))\n",
    "print(\"Offset (MJD): {: 13.5f} in MJD\".format(res.x[2]))\n",
    "print(\"Avg. rate   : {: 13.5f} in Hz\".format(res.x[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the rate function:\n",
    "def rate_fun(t):\n",
    "    \"\"\"\n",
    "    Returns the rate at a given time in MJD.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    t : array-like\n",
    "        Time in MJD.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    rate : array-like\n",
    "        The rate of background events in Hz.\n",
    "    \"\"\"\n",
    "    return f(t, res.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot runs\n",
    "start, stop = h[\"start_mjd\"], h[\"stop_mjd\"]\n",
    "rate = h[\"rate\"]\n",
    "xerr = 0.5 * (stop - start)\n",
    "yerr = h[\"rate_std\"]\n",
    "binmids = 0.5 * (stop + start)\n",
    "\n",
    "plt.errorbar(binmids, rate, xerr=xerr, yerr=yerr, fmt=\",\")\n",
    "plt.ylim(0, None);\n",
    "\n",
    "# Plot fit\n",
    "t = np.linspace(start[0], stop[-1], 1000)\n",
    "y = rate_fun(t)\n",
    "plt.plot(t, y, zorder=5)\n",
    "\n",
    "# Plot y shift dashed to see baseline or years average\n",
    "plt.axhline(bf_pars[3], 0, 1, color=\"C1\", ls=\"--\", label=\"\")\n",
    "\n",
    "plt.xlim(start[0], stop[-1])\n",
    "plt.xlabel(\"MJD\")\n",
    "plt.ylabel(\"Rate in Hz\")\n",
    "\n",
    "# plt.savefig(\"./data/figs/time_rate_sinus.png\", dpi=200)\n",
    "plt.ylim(0, 0.009)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Draw Number of Background Events "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The fitted spline models the expected background at a given time.\n",
    "Draw the actual number of events to inject per BG trial within a given time window using a poisson distribution with the mean from the spline.\n",
    "\n",
    "Classically the events drawn are then assigned a random time within the time window.\n",
    "But as we have the rate function, we can sample times from that function using a rejection sampling.\n",
    "This will only affect larger intervals, where the curvature can be seen and should only be used in these cases, as it is more costly to sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Sample number of events in frame\n",
    "\n",
    "Sample for single source but many trials at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Execute \"Time dependent rate function\" cells for best fit result\n",
    "best_pars = res.x\n",
    "\n",
    "def _transform_trange_mjd(t, trange):\n",
    "    \"\"\"\n",
    "    Returns\n",
    "    -------\n",
    "    trange : array-like, shape(2, len(t))\n",
    "        Reshaped time window `[[t0], [t1]]` in MJD for each time t.\n",
    "    \"\"\"\n",
    "    t = np.atleast_1d(t)\n",
    "    return t + np.array(trange).reshape(2, 1) / secinday\n",
    "\n",
    "def rate_fun_integral(t, trange):\n",
    "    \"\"\"\n",
    "    Analytic integral of the sinusodial rate function in interval `trange`.\n",
    "\n",
    "    t, trange: float, [float, float]\n",
    "        Time in MJD and tine frame around t in seconds.\n",
    "    \"\"\"\n",
    "    a, b, c, d = res.x\n",
    "\n",
    "    # Transform time window to MJD\n",
    "    t0, t1 = _transform_trange_mjd(t, trange)\n",
    "    # Split analytic expression for readability only\n",
    "    per = a / b * (np.cos(b * (t0 - c)) - np.cos(b * (t1 - c)))\n",
    "    lin = d * (t1 - t0)\n",
    "    # Match units with secinday = 24 * 60 * 60 s/MJD = 86400 / (Hz*MJD)\n",
    "    #     [a], [d] = Hz, [b], [c], [ti] = MJD\n",
    "    #     [a / b] = Hz * MJD, [d * (t1 - t0)] = HZ * MJD\n",
    "    return (per + lin) * secinday\n",
    "    \n",
    "# Just choose a single time and a large time frame to see the sinus shape\n",
    "t = h[\"start_mjd\"][100]\n",
    "trange = [0, 365 * secinday]  # 1 year window -> 1 period\n",
    "\n",
    "# Expectation is the integrals over the time frame\n",
    "expect = rate_fun_integral(t, trange)\n",
    "\n",
    "# Compare expectation with rectangular integration rule dt * avg\n",
    "print(\"Expectation : \", expect)\n",
    "print(\"Estimated   : \", np.diff(trange) * res.x[3])\n",
    "\n",
    "# Sample single number of events from poisson distribution\n",
    "ntrials = 100\n",
    "nevents = np.random.poisson(lam=expect, size=ntrials)\n",
    "print(\"Sampled events        : \", sum(nevents))\n",
    "print(\"Sample mean rate in Hz: \", sum(nevents) / ntrials / np.diff(trange)[0])\n",
    "print(\"Fitted mean rate in Hz: \", bf_pars[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Now the sampling of random times in the time frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "From the number of events, make injected times sampled form the rate function per trial.\n",
    "Sample all events at once and split up in trials later.\n",
    "This is way faster than looping over every trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def sample_times(t, trange, n_samples=1):\n",
    "    \"\"\"\n",
    "    %(RateFunction.sample.summary)s\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    %(RateFunction.sample.parameters)s\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    %(RateFunction.sample.returns)s\n",
    "    \"\"\"\n",
    "    # rejection_sampling needs bounds in shape (1, 2)\n",
    "    trange = _transform_trange_mjd(t, trange).T\n",
    "    times = rejection_sampling(rate_fun, bounds=trange, n=n_samples)[0]\n",
    "    return times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Sample all events for a time frame trange at once and split later\n",
    "tot_nevts = np.sum(nevents)\n",
    "print(\"Sampling total events : \", tot_nevts)\n",
    "\n",
    "times = sample_times(t, trange, tot_nevts)\n",
    "print(times)\n",
    "\n",
    "# Build index slices\n",
    "start = np.cumsum(np.append(0, nevents[:-1]))\n",
    "end = np.cumsum(nevents)\n",
    "# Split them up in list of trials, so each trial has the correct num of times\n",
    "trials = [times[i:k] for i, k in zip(start, end)]\n",
    "\n",
    "# See, if each trial has the corret amount of events sampled\n",
    "print(\"\\nTrials have correct length: \", np.all(np.equal(np.array(\n",
    "                list(map(len, trials))), nevents)))\n",
    "\n",
    "print(\"nevents per trial:\\n\", nevents[:10].reshape(10, 1))\n",
    "print(\"Examples of sampled times per trial:\\n\", trials[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Plot samples trials and see that it follows a sinus function\n",
    "h_, b_ = np.histogram(times, bins=50, density=False)\n",
    "err = np.sqrt(h_)\n",
    "\n",
    "# Norm to rate\n",
    "norm = np.diff(b_) * secinday * ntrials\n",
    "h_ = h_ / norm\n",
    "err = err / norm\n",
    "\n",
    "plt.errorbar(get_binmids(b_), h_, xerr=0.5 * np.diff(b_), yerr=err,\n",
    "             fmt=\"none\", markersize=3)\n",
    "\n",
    "x_ = t + np.linspace(trange[0] / secinday, trange[1] / secinday, 500)\n",
    "plt.plot(x_, rate_fun(t=x_))\n",
    "\n",
    "plt.xlabel(\"MJD\")\n",
    "plt.ylabel(\"Rate in Hz\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Create the BG PDF from data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Proceeding to section 6.3.1 Randomized BG Injection, p. 113.\n",
    "Mrichmann draws events by:\n",
    "\n",
    "1. Get number of bg events to be injected from a poisson distribution with expectation values drawn from the previously build bg temporal distribution.\n",
    "\n",
    "   $$\n",
    "   P_{\\langle n_B\\rangle}(N_m) = \\frac{\\langle n_B\\rangle^{N_m}}{N_m\\!}\\cdot \\exp(\\langle n_B\\rangle)\n",
    "   $$\n",
    "   \n",
    "2. These events are then drawn from a 3D pdf in energy proxy, zenith proxy and sigma proxy.\n",
    "   He does it by dividing 10x10x10 bins, first selecting energy, then zenith in that energy bin, then sigma in that zenith bin.\n",
    "   \n",
    "Here we create a smooth PDF using a kernel density estimator and obtain a sample by running a MCMC chain to create a sample a priori.\n",
    "The bandwidth is set globally and cross validated to be robust.\n",
    "\n",
    "**Some note on `numpy.histogramdd`:**\n",
    "\n",
    "The input must be an array with shape (nDim, len(data)).\n",
    "\n",
    "Shape of h is the same as the number of bins in each dim: (50, 40, 10)\n",
    "So the first dimension picks a single logE slice -> h[i].shape = (40, 10)\n",
    "Second dim picks a dec slice -> h[:, i].shape = (50, 10)\n",
    "3rd picks a sigma slice -> h[:, :, i].shape = (50, 40)\n",
    "\n",
    "This is important: meshgrid repeats in second axis on first array xx.\n",
    "For the second array, the first axis is repeated.\n",
    "But h iterates over energy in 1st axis. So if we don't transpose, we have the whole histogram flipped! Compare to plot in mrcihmanns thesis (cos(zen))\n",
    "\n",
    "**Some notes on KDE:**\n",
    "\n",
    "Sebastian has already made a tool for adaptive and asymmetric KDE.\n",
    "1. The Kernel is the covariance matrix of the whole data set to regard different scales\n",
    "    + Note: This may only be a problem, if one dim is spread with peaks, while the other is wide spread only. Then we cannot scale the Kernel to small to fit the peaks because the smooth dimension is preventing that.\n",
    "2. Use Silvermans or Scotts rule as a first guess.\n",
    "3. Run a second pass and vary the local bandwidth according to the first guess local density.\n",
    "\n",
    "We could replace 1 and 2 by scaling the data with the inverse covariance and then using a cross validation to find the first guess bandwidth.\n",
    "Then using a second pass to vary locally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 3D histogram of BG data\n",
    "First we make a 3D histogram to better compare to mrichmann and to get an overview over the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Cut sigmas in the sample to obtain smooth tail from KDE and remove outliers\n",
    "m = exp[\"sigma\"] < np.deg2rad(10)\n",
    "_logE = exp[\"logE\"][m]\n",
    "_dec = exp[\"dec\"][m]\n",
    "_sigma = exp[\"sigma\"][m]\n",
    "# Sample must match with the one used in training here\n",
    "sample = np.vstack((_logE, _dec, _sigma)).T\n",
    "\n",
    "# Binning is rather arbitrary because we don't calc stuff with the hist\n",
    "bins = [50, 50, 50]\n",
    "\n",
    "# Plot in degrees and in sinDec\n",
    "_sam = np.vstack((_logE, np.sin(_dec), np.rad2deg(_sigma))).T\n",
    "\n",
    "h, bins = np.histogramdd(sample=_sam, bins=bins, normed=False)\n",
    "\n",
    "# Make a nice corner plot\n",
    "label = [\"logE\", \"sinDdec\", \"sigma deg\"]\n",
    "fig, ax = hlp.corner_hist(h, bins=bins,\n",
    "                          label=label,\n",
    "                          hist2D_args={\"cmap\": \"inferno\", \"norm\": LogNorm()},\n",
    "                          hist_args={\"color\":\"C1\", \"alpha\": 0.5, \"log\": True})\n",
    "\n",
    "# plt.savefig(\"./data/figs/bg_corner_scaled.png\", dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Kernel Density Estimation\n",
    "\n",
    "Use adaptive width KDE to describe BG data and be able to smoothly draw new events from it.\n",
    "We fitted one set of params to the full data and stored it to avoid lengthy (~60 mins.) refitting when testing.\n",
    "A optimal set of parameters gets determined in a cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Assign model from CV, which has already evaluated adaptive kernels\n",
    "kde_inj = json2kde(\"data/awKDE_CV/CV10_glob_bw_alpha_EXP_IC86I_\" +\n",
    "                   \"CUT_sig.ll.20_PARS_diag_True_pass2.json\")\n",
    "\n",
    "# Sample with bounds, because gaussian KDEs have no border by default\n",
    "bounds = np.array([[None, None], [-np.pi / 2. , np.pi / 2.], [0, None]])\n",
    "n_samples = int(1e6)\n",
    "kde_sam = kde_inj.sample(n_samples)\n",
    "\n",
    "# Plot in degrees and in sinDec\n",
    "_sam_kde = np.vstack((kde_sam[:, 0],\n",
    "                      np.sin(kde_sam[:, 1]),\n",
    "                      np.rad2deg(kde_sam[:, 2]))).T\n",
    "\n",
    "bins = [np.linspace(1, 7, 50), np.linspace(-1, 1,50), np.linspace(0, 10, 50)]\n",
    "h, _ = np.histogramdd(sample=_sam_kde, bins=bins, normed=False)\n",
    "fig, ax = hlp.corner_hist(h, bins=bins,\n",
    "                         label=label,\n",
    "                         hist2D_args={\"cmap\": \"inferno\", \"norm\": LogNorm()},\n",
    "                         hist_args={\"color\":\"C1\", \"alpha\": 0.5, \"log\": True})\n",
    "\n",
    "# plt.savefig(\"./data/figs/bg_corner_scaled.png\", dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Compare KDE to original data\n",
    "\n",
    "Make a ratio histogram of the KDE sample and the original data sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 2D marginalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create 2D hists, by leaving out one parameter\n",
    "xlabel = [label[0], label[0], label[1]]\n",
    "ylabel = [label[1], label[2], label[2]]\n",
    "\n",
    "for i, axes in enumerate([[0, 1], [0, 2], [1, 2]]):\n",
    "    _b = np.array(bins)\n",
    "    h_exp, b_exp = np.histogramdd(_sam[:, axes],\n",
    "                                  bins=_b[axes], normed=True)\n",
    "    h_kde, b_kde = np.histogramdd(_sam_kde[:, axes],\n",
    "                                  bins=_b[axes], normed=True)\n",
    "    \n",
    "    # KDE is expectation, but sampled with much more events.\n",
    "    # Weights would simply scale the total number of KDE events to match the\n",
    "    # number of original events. That would be the mean for the poisson\n",
    "    # distribution in each bin. So to get OK KDE expectation sqrt(n) errors\n",
    "    # in each bin, we divide not by the number of drawn KDE but by the number\n",
    "    # of original events.   \n",
    "    # Again shapes of meshgrid and hist are transposed\n",
    "    diffXX, _ = np.meshgrid(np.diff(_b[0]), np.diff(_b[1]))\n",
    "    norm_kde = len(exp) * diffXX.T\n",
    "    sigma_kde = np.sqrt(h_kde / norm_kde)\n",
    "\n",
    "    # Make 3 different diff/ratio hists to estimate KDE quality in\n",
    "    # 1D marginalization.\n",
    "    m = (h_exp > 0.)\n",
    "    ratio_h = np.zeros_like(h_exp)\n",
    "    ratio_h[m] = h_kde[m] / h_exp[m]\n",
    "\n",
    "    diff_h = h_kde - h_exp\n",
    "\n",
    "    m = (sigma_kde > 0.)\n",
    "    sigma_ratio_h = np.zeros_like(h_exp)\n",
    "    sigma_ratio_h[m] = (h_exp[m] - h_kde[m]) / sigma_kde[m]\n",
    "\n",
    "    # Bin mids and hist grid\n",
    "    _b = b_exp\n",
    "    m = get_binmids(_b)\n",
    "    xx, yy = map(np.ravel, np.meshgrid(m[0], m[1]))\n",
    "    \n",
    "    \n",
    "    # Big plot on the left and three right\n",
    "    fig = plt.figure(figsize=(10, 6))\n",
    "    gs = gridspec.GridSpec(3, 3)\n",
    "    axl = fig.add_subplot(gs[:, :2])\n",
    "    axrt = fig.add_subplot(gs[0, 2])\n",
    "    axrc = fig.add_subplot(gs[1, 2])\n",
    "    axrb = fig.add_subplot(gs[2, 2])\n",
    "    \n",
    "    # Steal space for colorbars\n",
    "    caxl = split_axis(axl, \"right\")\n",
    "    caxrt = split_axis(axrt, \"left\")\n",
    "    caxrc = split_axis(axrc, \"left\")\n",
    "    caxrb = split_axis(axrb, \"left\")\n",
    "\n",
    "    # Unset top and center xticklabels as they are shared with the bottom plot\n",
    "    axrt.set_xticklabels([])\n",
    "    axrc.set_xticklabels([])\n",
    "        \n",
    "    # Left: Difference over KDE sigma\n",
    "    # cbar_extr = max(np.amax(sigma_ratio_h),  # Center colormap to min/max\n",
    "    #                         abs(np.amin(sigma_ratio_h)))\n",
    "    _, _, _, imgl = axl.hist2d(xx, yy, bins=_b, weights=sigma_ratio_h.T.ravel(),\n",
    "                               cmap=\"seismic\", vmax=5, vmin=-5)\n",
    "    cbarl = plt.colorbar(cax=caxl, mappable=imgl)\n",
    "    axl.set_xlabel(xlabel[i])\n",
    "    axl.set_ylabel(ylabel[i])\n",
    "    axl.set_title(\"(exp - kde) / sigma_kde\")\n",
    "    \n",
    "    # Right top: Ratio\n",
    "    _, _, _, imgrt = axrt.hist2d(xx, yy, bins=_b, weights=ratio_h.T.ravel(),\n",
    "                                 cmap=\"seismic\", vmax=2, vmin=0);\n",
    "    cbarrt = plt.colorbar(cax=caxrt, mappable=imgrt)\n",
    "    axrt.set_title(\"kde / exp\")\n",
    "\n",
    "    # Right center: Data hist\n",
    "    _, _, _, imgrc = axrc.hist2d(xx, yy, bins=_b, weights=h_exp.T.ravel(),\n",
    "                                 cmap=\"inferno\", norm=LogNorm());\n",
    "    cbarrc = plt.colorbar(cax=caxrc, mappable=imgrc)\n",
    "    axrc.set_title(\"exp logscale\")\n",
    "\n",
    "    # Right bottom: KDE hist, same colorbar scale as on data\n",
    "    _, _, _, imgrb = axrb.hist2d(xx, yy, bins=_b, weights=h_kde.T.ravel(),\n",
    "                                 cmap=\"inferno\", norm=LogNorm());\n",
    "    # Set with same colormap as on data\n",
    "    imgrb.set_clim(cbarrc.get_clim())\n",
    "    cbarrb = plt.colorbar(cax=caxrb, mappable=imgrb)\n",
    "    axrb.set_title(\"kde logscale\")\n",
    "    \n",
    "    # Set tick and label positions\n",
    "    for ax in [caxrt, caxrc, caxrb]:\n",
    "        ax.yaxis.set_label_position(\"right\")\n",
    "        ax.yaxis.tick_left()\n",
    "    \n",
    "    fig.tight_layout()\n",
    "#     plt.savefig(\"./data/figs/kde_data_2d_{}_{}.png\".format(\n",
    "#         xlabel[i], ylabel[i]), dpi=200)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 1D marginalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Pseudo smooth marginalization is done by sampling many point from KDE an\n",
    "# using a finely binned 1D histogram, so it looks smooth\n",
    "xlabel = label\n",
    "\n",
    "for i, axes in enumerate([0, 1, 2]):\n",
    "    _b = np.array(bins)\n",
    "    h_exp, b_exp = np.histogram(_sam[:, axes],\n",
    "                                bins=_b[axes], normed=True)\n",
    "    h_kde, b_kde = np.histogram(_sam_kde[:, axes],\n",
    "                                bins=_b[axes], normed=True)\n",
    "    \n",
    "#     h_exp, b_exp = hist_marginalize(h, bins, axes=axes)\n",
    "#     h_kde, b_kde = hist_marginalize(bg_h, bg_bins, axes=axes)\n",
    "      \n",
    "    # KDE errorbars as in 2D case\n",
    "    norm_kde = len(exp) * np.diff(b_kde)\n",
    "    sigma_kde = np.sqrt(h_kde / norm_kde)\n",
    "\n",
    "    # Make 3 different diff/ratio hists to estimate KDE quality in\n",
    "    # 1D marginalization.\n",
    "    m = (h_exp > 0.)\n",
    "    ratio_h = np.zeros_like(h_exp)\n",
    "    ratio_h[m] = h_kde[m] / h_exp[m]\n",
    "\n",
    "    diff_h = h_kde - h_exp\n",
    "\n",
    "    m = (sigma_kde > 0.)\n",
    "    sigma_ratio_h = np.zeros_like(h_exp)\n",
    "    sigma_ratio_h[m] = (h_exp[m] - h_kde[m]) / sigma_kde[m]\n",
    "\n",
    "    # Bin mids\n",
    "    _b = b_exp\n",
    "    m = get_binmids([_b])[0]\n",
    "    \n",
    "    # Plot both and the ration normed. Big plot on the left and three right\n",
    "    fig = plt.figure(figsize=(10, 6))\n",
    "    gs = gridspec.GridSpec(3, 3)\n",
    "    axl = fig.add_subplot(gs[:, :2])\n",
    "    axrt = fig.add_subplot(gs[0, 2])\n",
    "    axrc = fig.add_subplot(gs[1, 2])\n",
    "    axrb = fig.add_subplot(gs[2, 2])\n",
    "\n",
    "    axrt.set_xticklabels([])\n",
    "    axrc.set_xticklabels([])\n",
    "\n",
    "    # Set ticks and labels right\n",
    "    for ax in [axrt, axrc, axrb]:\n",
    "        ax.yaxis.set_label_position(\"right\")\n",
    "        ax.yaxis.tick_right()\n",
    "\n",
    "    # Limits\n",
    "    for ax in [axl, axrt, axrc, axrb]:\n",
    "        ax.set_xlim(_b[0], _b[-1])\n",
    "        \n",
    "    # Main plot:\n",
    "    # Plot more dense to mimic a smooth curve\n",
    "    __h, __b = np.histogram(_sam_kde[:, i], bins=200,\n",
    "                            range=[_b[0], _b[-1]], density=True)\n",
    "    __m = get_binmids([__b])[0]\n",
    "    axl.plot(__m, __h, lw=3, alpha=0.5)\n",
    "    \n",
    "    _ = axl.hist(m, bins=_b, weights=h_exp, label=\"exp\", histtype=\"step\",\n",
    "                 lw=2, color=\"k\")\n",
    "    _ = axl.errorbar(m, h_kde, yerr=sigma_kde, fmt=\",\", color=\"r\")\n",
    "    _ = axl.hist(m, bins=_b, weights=h_kde, label=\"kde\", histtype=\"step\",\n",
    "                 lw=2, color=\"r\")    \n",
    "    \n",
    "    axl.set_xlabel(xlabel[i])\n",
    "    axl.legend(loc=\"upper right\")\n",
    "\n",
    "    # Top right: Difference\n",
    "    _ = axrt.axhline(0, 0, 1, color=\"k\", ls=\"-\")\n",
    "    _ = axrt.hlines([-.02, -.01, .01, .02], _b[0], _b[-1],\n",
    "                    colors='#353132', linestyles='dashed')\n",
    "    _ = axrt.hist(m, bins=_b, weights=diff_h, histtype=\"step\", lw=2, color=\"r\")\n",
    "    axrt.set_ylim(-.05, +.05)\n",
    "    axrt.set_ylabel(\"kde - exp\")\n",
    "\n",
    "    # Center right: Ratio\n",
    "    _ = axrc.axhline(1, 0, 1, color=\"k\", ls=\"-\")\n",
    "    _ = axrc.hlines([0.8, 0.9, 1.1, 1.2], _b[0], _b[-1],\n",
    "                    colors='#353132', linestyles='dashed')\n",
    "    _ = axrc.hist(m, bins=_b, weights=ratio_h, histtype=\"step\", lw=2, color=\"r\")\n",
    "    axrc.set_ylim(.5, 1.5)\n",
    "    axrc.set_ylabel(\"kde / exp\")\n",
    "\n",
    "    # Bottom right: Ratio of diff to sigma of expectation\n",
    "    _ = axrb.axhline(0, 0, 1, color=\"k\", ls=\"-\")\n",
    "    _ = axrb.hlines([-2, -1, 1, 2], _b[0], _b[-1],\n",
    "                    colors='#353132', linestyles='dashed')\n",
    "    _ = axrb.hist(m, bins=_b, weights=sigma_ratio_h, histtype=\"step\", lw=2, color=\"r\")\n",
    "    axrb.set_ylim(-3, +3)\n",
    "    axrb.set_ylabel(\"(exp-kde)/sigma_kde\")\n",
    "    \n",
    "#     plt.savefig(\"./data/figs/kde_data_1d_{}.png\".format(\n",
    "#             xlabel[i]), dpi=200)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Define the Likelihoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here we define our Likelihoods.\n",
    "We are given a source event occurence (can be GRB, GW, HESE or anything else) at a given position in space and time.\n",
    "We want to search for a significant contribution of other events, within a predefined region in time and space around the source events.\n",
    "For this we need to derive the expected signal and background contributions in that frame.\n",
    "\n",
    "The Likelihood that describes this scenario can be derived from counting statistics.\n",
    "If we expect $n_S$ signal and $n_B$ background events in the given frame, then the probability of observing $N$ events is given by a poisson pdf:\n",
    "\n",
    "$$\n",
    "    P_\\text{Poisson}(N\\ |\\ n_S + n_B) = \\mathcal{L}(N | n_S, n_b) = \\frac{(n_S + n_B)^{N}}{N!}\\cdot \\exp{-(n_S + n_B)}\n",
    "$$\n",
    "\n",
    "We want to fit for the number of signal events $n_S$ in the frame.\n",
    "But each event doesn_t have the same probability of contributing to either signal or background, because we don't have that information on a per event basis.\n",
    "So we include prior information on a per event basis to account for that.\n",
    "\n",
    "$$\n",
    "    \\mathcal{L}(N | n_S, n_B) = \\frac{(n_S + n_B)^{N}}{N!}\\cdot \\exp{-(n_S + n_B)} \\cdot \\prod_{i=1}^N P_i\n",
    "$$\n",
    "\n",
    "Also the simple poisson pdf above only has one parameter, the total number of events, which can be fit for.\n",
    "So we need to resolve this degeneracy in $n_S$, $n_B$ by giving additional information.\n",
    "For that we include a weighted combination of the probability for an event to be signal, denoted by the PDF $S_i$ and for it to background, denoted by $B_i$.\n",
    "Because the simple counting probabilities are $n_S / (n_S + n_B)$ to count a signal event and likewise $n_B / (n_S + n_B)$ to count a background event we construct the per event prior $P_i$ as:\n",
    "\n",
    "$$\n",
    "    P_i = \\frac{n_S}{n_S + n_B}\\cdot S_i + \\frac{n_B}{n_S + n_B}\\cdot B_i\n",
    "        = \\frac{n_S \\cdot S_i + n_B \\cdot B_i}{n_S + n_B}\n",
    "$$\n",
    "\n",
    "Note, that for equal probabilities $S_i$ and $B_i$, we simply and up with the normal poisson counting statistic.\n",
    "\n",
    "Plugging that back into the likelihood we get:\n",
    "\n",
    "$$\n",
    "    \\mathcal{L}(N | n_S, n_B) = \\frac{(n_S + n_B)^{N}}{N!}\\cdot \\exp{(-(n_S + n_B))} \\cdot \\prod_{i=1}^N \\frac{n_S \\cdot S_i + n_B \\cdot B_i}{n_S + n_B}\n",
    "$$\n",
    "\n",
    "Taking the natrual logarithm to get the log-likelihood we arrive at:\n",
    "\n",
    "$$\n",
    "    \\ln\\mathcal{L}(N | n_S, n_B) = -(n_S + n_B) -\\ln(N!) + \\sum_{i=1}^N \\ln((n_S + n_B) P_i)\n",
    "$$\n",
    "\n",
    "If we weight up $n_S$ then every events signal PDF is contributing a bit more than the background pdf.\n",
    "So the fitter tries to find the combination of $n_S$ and $n_B$ that maximizes the likelihood.\n",
    "\n",
    "To further simplify, we can use a measured and fixed background expectation rate $\\langle n_B\\rangle$ and fit only for the number of signal events.\n",
    "Then we only fit for the number of signal events $n_S$.\n",
    "Also we are only interested in the number of signal events. \n",
    "The fixed background rate can be extracted from data by using the pdf of a larger timescale and average over that (or fit a function) to ensure that local fluctuations don't matter.\n",
    "\n",
    "Then we end up with our full Likelihood (the denominator in $P_i$ cancels with the term from the poisson PDF):\n",
    "\n",
    "$$\n",
    "    \\ln\\mathcal{L}(N | n_S) = -(n_S + \\langle n_B\\rangle) -\\ln(N!) + \\sum_{i=1}^N \\ln(n_S S_i + \\langle n_B\\rangle B_i)\n",
    "$$\n",
    "\n",
    "For the test statistic $\\Lambda=-2T$ we want to test the hypothesis of having no signal $n_S=0$ vs. the alternative with a free parameter $n_S$:\n",
    "\n",
    "\\begin{align}\n",
    "    T &= \\ln\\frac{\\mathcal{L}_0}{\\mathcal{L}_1}\n",
    "          = \\ln\\frac{\\mathcal{L}(n_S=0)}{\\mathcal{L}{\\hat{n}_S}} \\\\\n",
    "         &= -(\\hat{n}_S + \\langle n_B\\rangle) -\\ln(N!) +\n",
    "              \\sum_{i=1}^N \\ln(\\hat{n}_S S_i + \\langle n_B\\rangle B_i) \\\\\n",
    "         &\\phantom{=} +\\langle n_B\\rangle +\\ln(N!) -\n",
    "               \\sum_{i=1}^N \\ln(\\langle n_B\\rangle B_i) \\\\\n",
    "         &= -\\hat{n}_S + \\sum_{i=1}^N\n",
    "             \\ln\\left( \\frac{\\hat{n}_S S_i}{\\langle n_B\\rangle B_i} + 1 \\right)\n",
    "\\end{align}\n",
    "\n",
    "The per event PDFs $S_i$ and $B_i$ can depend on arbitrary parameters.\n",
    "The common choise here is to use a time, energy proxy and spatial proxy depency which has most seperation power:\n",
    "\n",
    "$$\n",
    "    S_i(x_i, t_i, E_i) = S_T(t_i) \\cdot S_S(x_i) \\cdot S_E(E_i) \\\\ \n",
    "    B_i(x_i, t_i, E_i) = B_T(t_i) \\cdot B_S(x_i) \\cdot B_E(E_i) \n",
    "$$\n",
    "\n",
    "Because the Likelihood only contains ratios of the PDF, we only have to construct functions of the signal to background ratio for each time, spatial and energy distribution.\n",
    "\n",
    "For the energy PDFs $S_E, B_E$ we use a 2D representation in reconstructed energy and declination because this has the most seperation power (see coenders & skylab models).\n",
    "The spatial part $S_S, B_S$ is only depending on the distance from source to event, not on the absilute position on the sphere.\n",
    "The time part $S_T, B_T$ is equivalent to that, only using the distance in time between source event and event."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Time PDF ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Background in uniformly distributed in the time window.\n",
    "Signal distribtution is falling off gaussian-like at both edges so normalization is different.\n",
    "So the ratio $S_T / B_T$ is simply the the signal pdf divided by the uniform normalization $1 / (t_1 - t_0)$ in the time frame.\n",
    "\n",
    "The signal PDFs written out explicitely, where $t_0$ is the source events time and $t$ the events time:\n",
    "\n",
    "$$\n",
    "    N \\cdot S_T(t, t_0) = \\begin{cases}\n",
    "                     \\frac{1}{\\sqrt{2\\pi}\\sigma_T}\\exp\\left(-\\frac{(t-T_0)^2}{2\\sigma_T^2}\n",
    "                     \\right)&\\quad\\mathrm{, if }\\ t \\in [a, T_0]\\\\                \n",
    "                     \\frac{1}{\\sqrt{2\\pi}\\sigma_T}&\\quad\\mathrm{, if }\\ t \\in [T_0, T_1]\\\\\n",
    "                     \\frac{1}{\\sqrt{2\\pi}\\sigma_T}\\exp\\left(-\\frac{(t-T_1)^2}\n",
    "                     {2\\sigma_T^2}\\right)&\\quad\\mathrm{, if }\\ t \\in [T_1, b]\\\\ \n",
    "                    0 &\\quad\\mathrm{, else}\n",
    "                  \\end{cases}\n",
    "$$\n",
    "\n",
    "where $a, b$ are the bounds of the total time window, $T_0, T_1$ are the part, in which the signal is assumed to be uniformly distributed in time and $\\sigma_T$ is the width of the gaussian edges.\n",
    "The gaussian width $\\sigma_T$ is as wide as the interval $T_1-T_0$ but constraint to the nearest value in $[2, 30]$ seconds if the frame gets too large or too small.\n",
    "The total normalization $N$ is given by integrating over $S_T$ in $[a, b]$, resulting in:\n",
    "\n",
    "$$\n",
    "    N = \\Phi(b) - \\Phi(a) + \\frac{T_1-T_0}{\\sqrt{2\\pi}\\sigma_T}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "    \\Phi(x) = \\int_{-\\infty}^{x}\\frac{1}{\\sqrt{2\\pi}\\sigma_T}\n",
    "      \\exp\\left(-\\frac{(t-T_0)^2}{2\\sigma_T^2}\\right)\\mathrm{d}t\n",
    "$$\n",
    "the CDF of the gaussian PDF.\n",
    "\n",
    "The background PDF respectively is simply:\n",
    "\n",
    "$$\n",
    "    B_T(t, t_0) = \\begin{cases}\n",
    "                     \\frac{1}{b-a}&\\quad\\mathrm{, if }\\ t \\in [a, b]\\\\ \n",
    "                    0 &\\quad\\mathrm{, else}\n",
    "                  \\end{cases}    \n",
    "$$\n",
    "\n",
    "To get finite support we truncate the gaussian edges at $n\\cdot\\sigma_T$.\n",
    "Though arbitrarliy introduced the concrete cutoff of the doesn't really matter (so say 4, 5, 6 sigma, etc).\n",
    "\n",
    "This is because in the LLH we get the product of $\\langle b_B \\rangle B_i$.\n",
    "A larger cutoff make the normalization of the BG pdf larger, but in the same time makes the number of expected BG event get higher in the same linear fashion.\n",
    "So as long as we choose a cutoff which ensures that $S \\approx 0$ outside, we're good to go.\n",
    "\n",
    "**Note:** The time PDF doesn't really have seperation power, it more or less acts as a theta function cutting out regions around the source time.\n",
    "We add a little artificial seperation power for very small time windows by using gaussian edge model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def time_soverb(t, t0, dt, nsig):\n",
    "    \"\"\"\n",
    "    Time signal over background PDF.\n",
    "    \n",
    "    Signal and background PDFs are each normalized over seconds.\n",
    "    Signal PDF has gaussian edges to smoothly let it fall of to zero, the\n",
    "    stddev is dt when dt is in [2, 30]s, otherwise the nearest edge.\n",
    "\n",
    "    To ensure finite support, the edges are truncated after nsig * dt.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    t : array-like\n",
    "        Times given in MJD for which we want to evaluate the ratio.\n",
    "    t0 : float\n",
    "        Time of the source event.\n",
    "    dt : array-like, shape (2)\n",
    "        Time window [start, end] in seconds centered at t0 in which the \n",
    "        signal pdf is assumed to be uniform.\n",
    "    nsig : float\n",
    "        Clip the gaussian edges at nsig * dt\n",
    "    \"\"\"\n",
    "    dt = np.atleast_1d(dt)\n",
    "    if len(dt) != 2:\n",
    "        raise ValueError(\"Timeframe 'dt' must have [start, end] in seconds.\")\n",
    "    if dt[0] >= dt[1]:\n",
    "        raise ValueError(\"Interval 'dt' must not be negative or zero.\")\n",
    "\n",
    "    secinday = 24. * 60. * 60.\n",
    "\n",
    "    # Normalize times from data relative to t0 in seconds\n",
    "    # Stability: Multiply before subtracting avoids small number rounding?\n",
    "    _t = t * secinday - t0 * secinday\n",
    "   \n",
    "    # Create signal PDF\n",
    "    # Constrain sig_t to [2, 30]s regardless of uniform time window\n",
    "    dt_tot = np.diff(dt)\n",
    "    sig_t = np.clip(dt_tot, 2, 30)\n",
    "    sig_t_clip = nsig * sig_t\n",
    "    gaus_norm = (np.sqrt(2 * np.pi) * sig_t)\n",
    "    \n",
    "    # Split in def regions gaus rising, uniform, gaus falling\n",
    "    gr = (_t < dt[0]) & (_t >= dt[0] - sig_t_clip)\n",
    "    gf = (_t > dt[1]) & (_t <= dt[1] + sig_t_clip)\n",
    "    uni = (_t >= dt[0]) & (_t <= dt[1])\n",
    "    \n",
    "    pdf = np.zeros_like(t, dtype=np.float)\n",
    "    pdf[gr] = scs.norm.pdf(_t[gr], loc=dt[0], scale=sig_t)\n",
    "    pdf[gf] = scs.norm.pdf(_t[gf], loc=dt[1], scale=sig_t)\n",
    "    # Connect smoothly with the gaussians\n",
    "    pdf[uni] = 1. / gaus_norm\n",
    "    \n",
    "    # Normalize signal distribtuion: Prob in gaussians + uniform part\n",
    "    dcdf = (scs.norm.cdf(dt[1] + sig_t_clip, loc=dt[1], scale=sig_t) -\n",
    "            scs.norm.cdf(dt[0] - sig_t_clip, loc=dt[0], scale=sig_t))\n",
    "    norm = dcdf + dt_tot / gaus_norm\n",
    "    pdf /= norm\n",
    "    \n",
    "    # Calculate the ratio\n",
    "    bg_pdf = 1. / (dt_tot + 2 * sig_t_clip)\n",
    "    ratio = pdf / bg_pdf\n",
    "    return ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "h = hlp._create_runtime_bins(exp[\"timeMJD\"], goodrun_dict=goodrun_dict,\n",
    "                             remove_zero_runs=True)\n",
    "\n",
    "# Make a plot with ratios for different time windows as in the paper\n",
    "# Arbitrary start date from data\n",
    "t0 = h[\"start_mjd\"][100]\n",
    "t0_sec = t0 * secinday\n",
    "\n",
    "# dt from t0 in seconds, clip at 4 sigma\n",
    "dts = [[-1, 5], [-5, 50], [-20, 200]]\n",
    "nsig = 4\n",
    "\n",
    "# Make t values for plotting in MJD around t0, to fit all in one plot\n",
    "max_dt, min_dt = np.amax(dts), np.amin(dts)\n",
    "dt_tot = max_dt - min_dt\n",
    "clip = np.clip(dt_tot, 2, 30) * nsig\n",
    "plt_range = np.array([min_dt - clip, max_dt + clip])\n",
    "t = np.linspace(t0_sec + 1.2 * plt_range[0],\n",
    "                t0_sec + 1.2 * plt_range[1], 1000) / secinday\n",
    "_t = t * secinday - t0 * secinday\n",
    "\n",
    "# Mark event time\n",
    "plt.axvline(0, 0, 1, c=\"#353132\", ls=\"--\", lw=2)\n",
    "\n",
    "colors = [\"C0\", \"C3\", \"C2\"]\n",
    "for i, dt in enumerate(dts):\n",
    "    # Plot ratio S/B\n",
    "    SoB = time_soverb(t, t0, dt, nsig)\n",
    "    plt.plot(_t, SoB, lw=2, c=colors[i],\n",
    "             label=r\"$T_\\mathrm{{uni}}$: {:>3d}s, {:>3d}s\".format(*dt))\n",
    "    # Fill uniform part, might look nicely\n",
    "    # fbtw = (_t > 0) & (_t < dt)\n",
    "    # plt.fill_between(_t[fbtw], 0, SoB[fbtw], color=\"C7\", alpha=0.1)\n",
    "\n",
    "# Make it look like the paper plot, but with slightly extended borders, to\n",
    "# nothing breaks outside the total time frame\n",
    "plt.xlim(1.2 * plt_range)\n",
    "plt.ylim(0, 3)\n",
    "plt.xlabel(\"t - t0 in sec\")\n",
    "plt.ylabel(\"S / B\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.grid(ls=\"--\", lw=1)\n",
    "\n",
    "# plt.savefig(\"./data/figs/time_pdf_ratio.png\", dpi=200)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Spatial Pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The spatial pdf is holding information on how close the event was to the source position.\n",
    "Close events are more likely to originate from the source.\n",
    "\n",
    "To model this behavioure we use a Kent distribution (gaussian correctly normalized on a sphere).\n",
    "\n",
    "$$\n",
    "    S_S(x_\\mathrm{evt}; x_S, \\kappa) = \\frac{\\kappa}{4\\pi \\sinh{\\kappa}}\\cdot \\exp(\\kappa\\cos(\\psi))\n",
    "$$\n",
    "\n",
    "where $x_\\mathrm{evt}$ is the directional vector of the event, $x_S$ is the directional vector of the source an $\\kappa$ resembles to the uncertainty in the event reconstruction and is connected with the more familiar $\\sigma$ error by.\n",
    "\n",
    "The connections between $\\kappa$ and $\\sigma$ is valid up to a $\\sigma\\approx 40^\\circ$ and is given by $\\kappa = 1 /\\sigma^2$.\n",
    "\n",
    "Classicaly the background pdf is constructed from data\n",
    "It is assumed to be uniform in right-ascension and the declination dependence is modeled with a spline fitted to a histogram in sinDec.\n",
    "Then the PDF is given by:\n",
    "\n",
    "$$\n",
    "    B_S(x_\\mathrm{evt}) = \\frac{1}{2\\pi}\\cdot p(\\sin\\delta)\n",
    "$$\n",
    "\n",
    "But we already made the work of creating a smooth KDE of our data in logE, declination and sigma.\n",
    "So we can use that KDE to get the values of our declination distribution.\n",
    "Because integrating out the KDE is slow, we just use our previous sample from the KDE, bin it finely (quasi continously) and interpolate it with a spline to get also values from in between.\n",
    "This way we are not dependent on a binning on the data itself, but can use the available validated KDE PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def spatial_signal(src_ra, src_dec, ev_ra, ev_dec, ev_sig, kent=True):\n",
    "        \"\"\"\n",
    "        Spatial distance PDF between source position(s) and event positions.\n",
    "\n",
    "        Signal is assumed to cluster around source position(s).\n",
    "        The PDF is a convolution of a delta function for the localized sources\n",
    "        and a Kent (gaussian on a sphere) distribution with the events\n",
    "        positional reconstruction error as width.\n",
    "        \n",
    "        Multiplie source positions can be given, to use it in a stacked\n",
    "        search.\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        src_ra : array-like\n",
    "            Src positions in equatorial RA in radian: [0, 2pi].\n",
    "        src_dec : array-like\n",
    "            Src positions in equatorial DEC in radian: [-pi/2, pi/2].\n",
    "        ev_ra : array-like\n",
    "            Event positions in equatorial RA in radian: [0, 2pi].\n",
    "        ev_dec : array-like\n",
    "            Event positions in equatorial DEC in radian: [-pi/2, pi/2].\n",
    "        ev_sig : array-like\n",
    "            Event positional reconstruction error in radian (eg. Paraboloid).\n",
    "        \n",
    "        Returns\n",
    "        --------\n",
    "        S : array-like, shape(n_sources, n_events)\n",
    "            Spatial signal probability for each event and each source.\n",
    "\n",
    "        \"\"\"\n",
    "        # Shape (n_sources, 1), suitable for 1 src or multiple srcs\n",
    "        src_ra = np.atleast_1d(src_ra)[:, np.newaxis]\n",
    "        src_dec = np.atleast_1d(src_dec)[:, np.newaxis]\n",
    "\n",
    "        # Dot product in polar coordinates\n",
    "        cosDist = (np.cos(src_ra - ev_ra) *\n",
    "                   np.cos(src_dec) * np.cos(ev_dec) +\n",
    "                   np.sin(src_dec) * np.sin(ev_dec))\n",
    "    \n",
    "        # Handle possible floating precision errors\n",
    "        cosDist = np.clip(cosDist, -1, 1)\n",
    "        \n",
    "        if kent:\n",
    "            # Stabilized version for possibly large kappas\n",
    "            kappa = 1. / ev_sig**2\n",
    "            S = (kappa / (2. * np.pi * (1. - np.exp(-2. * kappa))) *\n",
    "                 np.exp(kappa * (cosDist - 1. )))\n",
    "        else:\n",
    "            # Otherwise use standard symmetric 2D gaussian\n",
    "            dist = np.arccos(cosDist)\n",
    "            ev_sig_2 = 2 * ev_sig**2\n",
    "            S = np.exp(-dist**2 / (ev_sig_2)) / (np.pi * ev_sig_2)\n",
    "        \n",
    "        return S\n",
    "    \n",
    "def create_spatial_bg_spline(sin_dec, bins=100, range=None, k=3):\n",
    "    \"\"\"\n",
    "    Fit an interpolsating spline to the a histogram of sin(dec).\n",
    "    \n",
    "    The spline is fitted to the logarithm of the histogram, to avoid ringing.\n",
    "    Normalization is done by normalizing the hist.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sin_dec : array-like\n",
    "        Sinus declination coorcinates of each event, [-1, 1].\n",
    "    bins : int or array-like\n",
    "        Binning passed to `np.histogram`. (default: 100)\n",
    "    range : array-like\n",
    "        Lower and upper boundary for the histogram. (default: None)\n",
    "    k : int\n",
    "        Order of the spline. (default: 3)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    spl : scipy.interpolate.InterpolatingSpline\n",
    "        Spline object interpolating the histogram. Must be evaluated with\n",
    "        sin(dec) and exponentiated to give the correct values.\n",
    "        Spline is interpolating outside it's definition range.\n",
    "    \"\"\"\n",
    "    hist, bins = np.histogram(sin_dec, bins=bins, \n",
    "                              range=range, density=True)\n",
    "    \n",
    "    if np.any(hist <= 0.):\n",
    "        estr = (\"Declination hist bins empty, this must not happen. Empty \" +\n",
    "                \"bins: {0}\".format(np.arange(len(bins) - 1)[hist <= 0.]))\n",
    "        raise ValueError(estr)\n",
    "    elif np.any((sin_dec < bins[0]) | (sin_dec > bins[-1])):\n",
    "        raise ValueError(\"Data outside of declination bins!\")\n",
    "\n",
    "    mids = 0.5 * (bins[:-1] + bins[1:])\n",
    "    return sci.InterpolatedUnivariateSpline(mids, np.log(hist), k=k, ext=0)\n",
    "\n",
    "def spatial_background(ev_sin_dec, sindec_log_bg_spline):\n",
    "    \"\"\"\n",
    "    Calculate the value of the backgournd PDF for each event from a previously\n",
    "    created spline, interpolating the declination distribution of the data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ev_sin_dec : array-like\n",
    "        Sinus Declination coordinates of each event, [-1, 1].\n",
    "    sindec_log_bg_spline : scipy.interpolate.InterpolatingSpline\n",
    "        Spline returning the logarithm of the bg PDF at given sin_dec values.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    B : array-like\n",
    "        The value of the background PDF for each event.\n",
    "    \"\"\"\n",
    "    return 1. / 2. / np.pi * np.exp(sindec_log_bg_spline(ev_sin_dec))\n",
    "\n",
    "\n",
    "def spatial_SoB(src_ra, src_dec, ev_ra, ev_dec, ev_sig,\n",
    "                sindec_log_bg_spline, kent=True):\n",
    "    S = spatial_signal(src_ra, src_dec, ev_ra, ev_dec, ev_sig, kent)\n",
    "    B = spatial_background(ev_sin_dec, sindec_log_bg_spline)\n",
    "    \n",
    "    SoB = np.zeros_like(S)\n",
    "    B = np.repeat(B[np.newaxis, :], repeats=S.shape[0], axis=0)\n",
    "    m = B > 0\n",
    "    SoB[m] = S[m] / B[m]\n",
    "\n",
    "    return SoB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Signal PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_dec_vs_signal(S, ev_dec, src_ra, src_dec, weights, ax=None):\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots(1, 1)\n",
    "    # Plot signal per source for each event\n",
    "    for i, (sra, sdec) in enumerate(zip(src_ra, src_dec)):\n",
    "        ax.plot(np.rad2deg(ev_dec), S[i], ls=\"-\")\n",
    "        ax.plot(np.rad2deg(sdec), -10, \"k|\")\n",
    "\n",
    "    # Simulate a simple stacking, one weight per source\n",
    "    ax.plot(np.rad2deg(ev_dec), np.sum(weights * S, axis=0) / np.sum(weights),\n",
    "             ls=\"--\", c=dg, label=\"stacked\")\n",
    "\n",
    "    ax.set_xlim([-1 + smin, smax + 1])\n",
    "    ax.set_xlabel(\"DEC in °\")\n",
    "    ax.set_ylabel(\"Signal pdf\")\n",
    "    ax.legend(loc=\"upper right\")\n",
    "    return ax\n",
    "\n",
    "# Simulate a simple case: 5 src and the events are in the same range, but with\n",
    "# tighter spacing\n",
    "smax = 5\n",
    "smin = -5\n",
    "step = 2\n",
    "\n",
    "src_ra = np.deg2rad(np.arange(smin, smax + step, step))\n",
    "src_dec = np.deg2rad(np.arange(smin, smax + step, step))\n",
    "\n",
    "ev_ra = np.deg2rad(np.linspace(smin, smax, 1000))\n",
    "ev_dec = np.deg2rad(np.linspace(smin, smax, 1000))\n",
    "ev_sig = np.deg2rad(np.ones_like(ev_ra))\n",
    "\n",
    "S = spatial_signal(src_ra, src_dec, ev_ra, ev_dec, ev_sig, kent=True)  \n",
    "\n",
    "weights = np.arange(1, len(src_dec) + 1)[:, np.newaxis]\n",
    "_ = plot_dec_vs_signal(S, ev_dec, src_ra, src_dec, weights)\n",
    "plt.show()\n",
    "\n",
    "# Now with the real data. Sort first in dec to show with nice lines\n",
    "idx = np.argsort(exp[\"dec\"])\n",
    "ev_ra = exp[\"ra\"][idx]\n",
    "ev_dec = exp[\"dec\"][idx]\n",
    "# ev_sig = np.deg2rad(np.ones_like(ev_ra))\n",
    "ev_sig = exp[\"sigma\"][idx]\n",
    "\n",
    "S = spatial_signal(src_ra, src_dec, ev_ra, ev_dec, ev_sig, kent=True)\n",
    "\n",
    "weights = np.ones_like(weights)\n",
    "ax = plot_dec_vs_signal(S, ev_dec, src_ra, src_dec, weights)\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_ylim(1, 1e4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Background PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Assign model from CV, which has already evaluated adaptive kernels\n",
    "kde_inj = json2kde(\"data/awKDE_CV/CV10_glob_bw_alpha_EXP_IC86I_\" +\n",
    "                   \"CUT_sig.ll.20_PARS_diag_True_pass2.json\")\n",
    "\n",
    "# Generate some BG samples to compare to the original data hist\n",
    "nsamples_kde = int(1e7)\n",
    "bg_samples = kde_inj.sample(nsamples_kde)\n",
    "\n",
    "# Plot in degrees and in sinDec\n",
    "_sam_kde = np.vstack((bg_samples[:, 0],\n",
    "                      np.sin(bg_samples[:, 1]),\n",
    "                      np.rad2deg(bg_samples[:, 2]))).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, (axl, axr) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# First finely binned KDE. Show the data in the same binning to see the diff\n",
    "bins = np.linspace(-1, 1, 100)\n",
    "h, b, _ = axl.hist(_sam_kde[:, 1], bins=bins, normed=True, alpha=0.5)\n",
    "h, b = np.histogram(_sam_kde[:, 1], bins=bins, density=True)\n",
    "kde_spl = create_spatial_bg_spline(_sam_kde[:, 1], bins=bins)\n",
    "\n",
    "_sin_dec = np.linspace(-1, 1, 1000)\n",
    "pdf = np.exp(kde_spl(_sin_dec))\n",
    "axl.plot(_sin_dec, pdf, lw=2)\n",
    "axl.set_title(\"BG PDF from KDE: PDF constructed with fine bins\")\n",
    "\n",
    "# Now classic with coarse binned data\n",
    "bins = np.linspace(-1, 1, 20 + 1)\n",
    "sin_dec = np.sin(exp[\"dec\"])\n",
    "h, b, _ = axr.hist(sin_dec, bins=bins, normed=True, alpha=0.5)\n",
    "spl = create_spatial_bg_spline(sin_dec, bins=bins)\n",
    "\n",
    "pdf = np.exp(spl(_sin_dec))\n",
    "axr.plot(_sin_dec, pdf)\n",
    "axr.set_title(\"BG PDF from data\")\n",
    "\n",
    "# Quickly integrate BG pdf to check norm is OK (increased subdvivision lim)\n",
    "I = scint.quad(spatial_background, -1, 1, args=(kde_spl), limit=100)[0]\n",
    "print(\"Area under all sky BG PDF is : \", 2. * np.pi * I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Signal over Background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Make srcs across the dec range. SoB should follow the sinDec BG\n",
    "# distribtuion. With a single source we couldn't see that, because it drops\n",
    "# to zero far from the src position\n",
    "bg_sin_dec = _sam_kde[:, 1]\n",
    "smin, smax, step = -90, +90, 10\n",
    "\n",
    "src_ra = np.deg2rad(np.arange(smin, smax + step, step))\n",
    "src_dec = np.deg2rad(np.arange(smin, smax + step, step))\n",
    "\n",
    "ev_ra = np.deg2rad(np.linspace(smin, smax, 1000))\n",
    "ev_dec = np.deg2rad(np.linspace(smin, smax, 1000))\n",
    "ev_sin_dec = np.sin(ev_dec)\n",
    "ev_sig = np.deg2rad(np.ones_like(ev_ra))\n",
    "\n",
    "weights = np.arange(1, len(src_dec) + 1)[:, np.newaxis]\n",
    "\n",
    "fig, ((axtl, axtr), (axbl, axbr)) = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Signal only\n",
    "S = spatial_signal(src_ra, src_dec, ev_ra, ev_dec, ev_sig, kent=True)  \n",
    "_ = plot_dec_vs_signal(S, ev_dec, src_ra, src_dec, weights, ax=axtl)\n",
    "axtl.set_xlim(-90, 90)\n",
    "\n",
    "# Background only\n",
    "bins = 100\n",
    "h, b, _ = axl.hist(bg_sin_dec, bins=bins, normed=True, alpha=0.5)\n",
    "h, b = np.histogram(bg_sin_dec, bins=bins, density=True)\n",
    "_sin_dec = np.linspace(-1, 1, 1000)\n",
    "pdf = np.exp(kde_spl(_sin_dec))\n",
    "axbl.plot(np.rad2deg(np.arcsin(_sin_dec)), pdf, lw=2, label=\"pdf\")\n",
    "axbl.set_ylim(0, 1)\n",
    "# 1 / BG PDF on second axis\n",
    "axbl2 = axbl.twinx()\n",
    "axbl2.plot(np.rad2deg(np.arcsin(_sin_dec)), 1. / pdf, c=\"C1\",\n",
    "           lw=2, label=\"1/pdf\")\n",
    "axbl2.set_ylim(0, 6)\n",
    "axbl.set_xlabel(\"DEC in °\")\n",
    "axbl.set_xlim(-90, 90)\n",
    "axbl.legend(loc=\"upper left\")\n",
    "axbl2.legend(loc=\"upper center\")\n",
    "\n",
    "# SoB on example + BG PDF\n",
    "SoB = spatial_SoB(src_ra, src_dec, ev_ra, ev_dec, ev_sig, kde_spl, kent=True)  \n",
    "weights = np.arange(1, len(src_dec) + 1)[:, np.newaxis]\n",
    "_ = plot_dec_vs_signal(SoB, ev_dec, src_ra, src_dec, weights, ax=axtr)\n",
    "axtr.plot(np.rad2deg(np.arcsin(_sin_dec)), pdf, lw=3, label=\"BG pdf\", c=dg)\n",
    "axtr.set_xlim(-90, 90)\n",
    "axtr.set_yscale(\"log\")\n",
    "axtr.set_ylim(0.1, 1e5)\n",
    "axtr.legend(loc=\"upper left\")\n",
    "\n",
    "# Now with the real data. Sort first in dec to show with nice lines + BG PDF\n",
    "idx = np.argsort(exp[\"dec\"])\n",
    "ev_ra = exp[\"ra\"][idx]\n",
    "ev_dec = exp[\"dec\"][idx]\n",
    "ev_sin_dec = np.sin(ev_dec)\n",
    "ev_sig = exp[\"sigma\"][idx]\n",
    "# ev_sig = np.deg2rad(np.ones_like(ev_ra))  # To match the simple example\n",
    "\n",
    "SoB = spatial_SoB(src_ra, src_dec, ev_ra, ev_dec, ev_sig, kde_spl, kent=True)\n",
    "\n",
    "_ = plot_dec_vs_signal(SoB, ev_dec, src_ra, src_dec, weights, ax=axbr)\n",
    "axbr.plot(np.rad2deg(np.arcsin(_sin_dec)), pdf, lw=3, label=\"BG pdf\", c=\"C0\")\n",
    "axbr.set_yscale(\"log\")\n",
    "axbr.set_ylim(0.1, 1e5)\n",
    "axbr.legend(loc=\"upper left\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Energy-Space Pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This will be the same in skylab and the first time we need a MC set.\n",
    "Make equally binned 2D histograms in logE and sinDec, then take the ratio.\n",
    "Because of the equal binning, the normalization is automatically correct.\n",
    "Then fit a 2D spline to it which gives the signal to background ratio directly.\n",
    "\n",
    "Here we use again a KDE fitted both to data.\n",
    "This way we can sample more events in the sparsely populated areas and obtain a broader ratio distribution.\n",
    "Because we can't use the sklearn KDE for weighted samples we use a normal histogram for the MC, which has more event anyway so the problem is not so urgent.\n",
    "\n",
    "Where data is missing either use background MC or conservatively use the highest ratio where data is available also at positions, where no data is present.\n",
    "This is only relevant for signal injection, because on data we have the ratio defined everywhere, where data is by definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_bg_sample_from_kde(nsamples_kde=int(1e7)):\n",
    "    # Assign model from CV, which has already evaluated adaptive kernels\n",
    "    kde_inj = json2kde(\"data/awKDE_CV/CV10_glob_bw_alpha_EXP_IC86I_\" +\n",
    "                       \"CUT_sig.ll.20_PARS_diag_True_pass2.json\")\n",
    "\n",
    "    bg_samples = kde_inj.sample(nsamples_kde)\n",
    "\n",
    "    # Plot in degrees and in sinDec\n",
    "    _sam_kde = np.vstack((kde_sam[:, 0],\n",
    "                          np.sin(kde_sam[:, 1]),\n",
    "                          np.rad2deg(kde_sam[:, 2]))).T\n",
    "\n",
    "    # Return sinDec and logE\n",
    "    return _sam_kde[:, 1], _sam_kde[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_bg_sample_from_kde(nsamples_kde = int(1e7)):\n",
    "    print(\"Sampling from BG KDE\")\n",
    "    # KDE CV is running on cluster and pickles the GridSearchCV\n",
    "    fname = \"./data/kde_cv/KDE_model_selector_20_exp_IC86_I_followup_2nd_pass.pickle\"\n",
    "    with open(fname, \"rb\") as f:\n",
    "        model_selector = pickle.load(f)\n",
    "\n",
    "    kde = model_selector.best_estimator_\n",
    "    bw = model_selector.best_params_[\"bandwidth\"]\n",
    "    print(\"Best bandwidth : {:.3f}\".format(bw))\n",
    "\n",
    "    # We maybe just want to stick with the slightly overfitting kernel to\n",
    "    # be as close as possible to data\n",
    "    OVERFIT = True\n",
    "    if OVERFIT:\n",
    "        bw = 0.075\n",
    "        kde = skn.KernelDensity(bandwidth=bw, kernel=\"gaussian\", rtol=1e-8)\n",
    "    print(\"Used bandwidth : {:.3f}\".format(bw))\n",
    "\n",
    "    # KDE sample must be cut in sigma before fitting, similar to range in hist\n",
    "    _exp = exp[exp[\"sigma\"] <= np.deg2rad(5)]\n",
    "\n",
    "    fac_logE = 1.5\n",
    "    fac_dec = 2.5\n",
    "    fac_sigma = 2.\n",
    "\n",
    "    _logE = fac_logE * _exp[\"logE\"]\n",
    "    _sigma = fac_sigma * np.rad2deg(_exp[\"sigma\"])\n",
    "    _dec = fac_dec * _exp[\"dec\"]\n",
    "\n",
    "    # Fit KDE best model to background sample\n",
    "    kde_sample = np.vstack((_logE, _dec, _sigma)).T\n",
    "    kde.fit(kde_sample)\n",
    "\n",
    "    # Generate some BG samples to compare to the original data hist.\n",
    "    # Use more statistics, histograms get normalized and we want the best estimate\n",
    "    # for the pdf\n",
    "    bg_samples = kde.sample(n_samples=nsamples_kde)\n",
    "\n",
    "    # Restore the orignal scaling and cut away spillovers from the finite width\n",
    "    bg_logE = bg_samples[:, 0] / fac_logE\n",
    "    bg_dec = bg_samples[:, 1] / fac_dec\n",
    "    bg_sigma = bg_samples[:, 2] / fac_sigma\n",
    "\n",
    "    m = (bg_dec > -np.pi / 2.) & (bg_dec < np.pi / 2.)\n",
    "    m = m & (bg_sigma > 0 )\n",
    "\n",
    "    bg_logE = bg_logE[m]\n",
    "    bg_dec = bg_dec[m]\n",
    "    bg_sindec = np.sin(bg_dec)\n",
    "    bg_sigma = np.deg2rad(bg_sigma[m])\n",
    "    \n",
    "    return bg_sindec, bg_logE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Prepare the MC data, signal weighted to astro unbroken power law\n",
    "gamma = 2.\n",
    "# No flux norm, because we normalize anyway\n",
    "mc_w = mc[\"ow\"] * mc[\"trueE\"]**(-gamma)\n",
    "\n",
    "# Make 2D hist from data KDE and from MC, use the MC binning\n",
    "mc_sindec = np.sin(mc[\"dec\"])\n",
    "mc_logE = mc[\"logE\"]\n",
    "bins = [50, 40]\n",
    "rnge = [[-1, 1], [1, 10]]\n",
    "mc_h, bx, by = np.histogram2d(mc_sindec, mc_logE, bins=bins, range=rnge,\n",
    "                              weights=mc_w, normed=True)\n",
    "\n",
    "b = [bx, by]\n",
    "\n",
    "MODE = \"DATA\"  # \"DATA\", \"KDE_SAM\", \"KDE_INT\"\n",
    "if MODE == \"DATA\":\n",
    "    bg_logE = exp[\"logE\"]\n",
    "    bg_sindec = np.sin(exp[\"dec\"])\n",
    "    bg_h, _, _ = np.histogram2d(bg_sindec, bg_logE, bins=b,\n",
    "                                range=rnge, normed=True)\n",
    "elif MODE == \"KDE_SAM\":\n",
    "    bg_sindec, bg_logE = get_bg_sample_from_kde(int(2e7))\n",
    "    bg_h, _, _ = np.histogram2d(bg_sindec, bg_logE, bins=b,\n",
    "                                range=rnge, normed=True)\n",
    "elif MODE == \"KDE_INT\":\n",
    "    _bins = np.load(\"data/1d_integrate_kde/logE_sinDec_bins_50x50.npy\")\n",
    "    vals = np.load(\"data/1d_integrate_kde/logE_sinDec_int_50x50.npy\")\n",
    "    mids = get_binmids(_bins)\n",
    "    xx, yy  = map(np.ravel, np.meshgrid(mids[0], mids[1]))\n",
    "    bg_h, _, _ = np.histogram2d(xx, yy, bins=_bins, weights=vals,\n",
    "                                normed=True, range=rnge)\n",
    "    # Turn around to have sinDec vs logE like in the other examples\n",
    "    bg_h = bg_h.T\n",
    "    # KDE_INT is not so good, because it falls too quickly. Need to clip it\n",
    "    bg_h = np.clip(bg_h, 1e-10, 1)\n",
    "    \n",
    "# 3 cases:\n",
    "#   - Data & MC: Calculate the ratio\n",
    "#   - No data or no MC: Assign nearest value in energy bin\n",
    "#   - No data and no MC: Assign any value (eg 1), these are never accessed\n",
    "# Get logE value per bin in entrie histogram\n",
    "m = get_binmids(b)\n",
    "\n",
    "# Fill value: 1) min/max for low/hig edge or 2) nearest in column\n",
    "FILLVAL = \"MINMAX\"  # \"COL\" | \"MINMAX\"\n",
    "\n",
    "# This assumes at least one valid point in one sinDec slice\n",
    "m1 = (bg_h > 0) & (mc_h > 0)\n",
    "SoB = np.ones_like(bg_h) * -1  # Init with unphysical value\n",
    "SoB[m1] = mc_h[m1] / bg_h[m1]\n",
    "SOBmin, SoBmax = np.amin(SoB[m1]), np.amax(SoB[m1])\n",
    "\n",
    "# In each energy bin assign nearest value to bins with no data or no MC\n",
    "for i in np.arange(bins[0]):\n",
    "    bghi = bg_h[i]  # Get sinDec slice\n",
    "    mchi = mc_h[i]\n",
    "    _m = (bghi <= 0) | (mchi <= 0)  # All invalid points\n",
    "    # Only fill missing logE border values and then proceed to interpolation\n",
    "\n",
    "    # First lower edge (argmax stops at first True, argmin at first False)\n",
    "    low_first_invalid_id = np.argmax(_m)\n",
    "    if low_first_invalid_id == 0:\n",
    "        # Set lower edge with first valid point from bottom\n",
    "        low_first_valid_id = np.argmin(_m)\n",
    "        if FILLVAL == \"COL\":\n",
    "            SoB[i, 0] = SoB[i, low_first_valid_id]\n",
    "        elif FILLVAL == \"MINMAX\":\n",
    "            SoB[i, 0] = np.amin(SoB[m1])\n",
    "\n",
    "    # Repeat with turned around array for upper edge\n",
    "    hig_first_invalid_id = np.argmax(_m[::-1])\n",
    "    if hig_first_invalid_id == 0:\n",
    "        # Set lower edge with first valid point from bottom\n",
    "        hig_first_valid_id = len(_m) - 1 - np.argmin(_m[::-1])\n",
    "        if FILLVAL == \"COL\":\n",
    "            SoB[i, -1] = SoB[i, hig_first_valid_id]\n",
    "        elif FILLVAL == \"MINMAX\":\n",
    "            SoB[i, -1] = np.amax(SoB[m1])\n",
    "        \n",
    "    # Interpolate in each slice over missing entries\n",
    "    _m = SoB[i] > 0\n",
    "    x = m[1][_m]\n",
    "    y = SoB[i, _m]\n",
    "    fi = sci.interp1d(x, y, kind=\"linear\")\n",
    "    SoB[i] = fi(m[1])\n",
    "\n",
    "# These do never occur, so set them to 1to be identified quickly in the plot\n",
    "m4 = (bg_h <= 0) & (mc_h <= 0)\n",
    "SoB[m4] = 1.\n",
    "\n",
    "# Now fit a spline to the ratio\n",
    "SoB_spl = sci.RegularGridInterpolator(m, np.log(SoB), method=\"linear\",\n",
    "                                      bounds_error=False, fill_value=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Coenders style sindec vs logE\n",
    "m = get_binmids(b)\n",
    "xx, yy = map(np.ravel, np.meshgrid(*m))\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "(axtl, axtr), (axbl, axbr) = ax\n",
    "\n",
    "# Data\n",
    "_, _, _, img = axtl.hist2d(xx, yy, bins=b, weights=bg_h.T.flatten(),\n",
    "                         norm=LogNorm())\n",
    "axtl.set_title(\"Exp events : {}\".format(len(exp)))\n",
    "caxtl = split_axis(axtl, cbar=True)\n",
    "plt.colorbar(cax=caxtl, mappable=img)\n",
    "\n",
    "# MC\n",
    "_, _, _, img = axtr.hist2d(xx, yy, bins=b, weights=mc_h.T.flatten(),\n",
    "                         norm=LogNorm())\n",
    "axtr.set_title(\"Signal. gamma = {:.1f}\".format(gamma))\n",
    "caxtr = split_axis(axtr, cbar=True)\n",
    "plt.colorbar(cax=caxtr, mappable=img)\n",
    "\n",
    "# Ratio hist\n",
    "cnorm = max(np.amin(SoB), np.amax(SoB))  # coenders: 1e-3, 1e3\n",
    "_, _, _, img = axbl.hist2d(xx, yy, bins=b, weights=SoB.T.flatten(),\n",
    "                         norm=LogNorm(), cmap=\"coolwarm\",\n",
    "                         vmin=1. / cnorm, vmax=cnorm)\n",
    "axbl.set_title(\"Signal over background\".format(gamma))\n",
    "caxbl = split_axis(axbl, cbar=True)\n",
    "plt.colorbar(cax=caxbl, mappable=img)\n",
    "\n",
    "# Ratio spline\n",
    "x = np.linspace(*rnge[0], num=500 + 1)\n",
    "y = np.linspace(*rnge[1], num=500 + 1)\n",
    "XX, YY = np.meshgrid(x, y)\n",
    "xx, yy = map(np.ravel, [XX, YY])\n",
    "gpts = np.vstack((xx, yy)).T\n",
    "zz = np.exp(SoB_spl(gpts))\n",
    "ZZ = zz.reshape(XX.shape)\n",
    "# Plotting with hist creates strange effects... Use pcolormesh instead\n",
    "img = axbr.pcolormesh(XX, YY, ZZ, norm=LogNorm(), cmap=\"coolwarm\",\n",
    "                    vmin=1. / cnorm, vmax=cnorm)\n",
    "axbr.set_title(\"Spline interpolation\".format(gamma))\n",
    "caxbr = split_axis(axbr, cbar=True)\n",
    "plt.colorbar(cax=caxbr, mappable=img)\n",
    "\n",
    "# plt.savefig(\"./data/figs/energy_ratio_spline_minmaxfill.png\", dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Sensitivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Sensitivity calculation example using a simple rayleigh distribution (positive values, single shape parameter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Set background TS value against which the alternative is compared. This is\n",
    "# connected to an alpha value = percentage of pdf lying right of H0 TS_val\n",
    "TS_val = 2\n",
    "\n",
    "# beta value = percentage of pdf lying right of TS_val\n",
    "beta = 0.9\n",
    "\n",
    "def rayleigh_quantiles(TS_val, p):\n",
    "    \"\"\"\n",
    "    Get the scale factor for a given probability and TS value.\n",
    "    https://www.npmjs.com/package/distributions-rayleigh-quantile\n",
    "    \"\"\"\n",
    "    return TS_val / np.sqrt(-np.log((1. - p)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## What we want to have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We have a TS value that we define our threshold for (is equivalent to setting an alpha for the type I error).\n",
    "We need to find how many signal like events we need to inject to get the transformed TS with a fraction beta above the predefined alpha value (or TS value) of the null hypothesis.\n",
    "Here we do that by adapting the scale parameter of the rayleigh distribution which is the same but easier to follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# For a showcase let's make some distrubtions of the alternative hypothesis\n",
    "scales = np.array([3., 4., 5., 6.])\n",
    "nsamples = 10000\n",
    "H1 = [np.random.rayleigh(scale=s, size=nsamples) for s in scales]\n",
    "\n",
    "# Their 1-beta fraction lies further to the right with incresing scale\n",
    "i = 0\n",
    "xmax = 10\n",
    "x = np.linspace(0, xmax, 200)\n",
    "bins = np.linspace(0, xmax, 50)\n",
    "for H1i in H1:\n",
    "    plt.hist(H1i, bins=bins, alpha=0.3, color=\"C{}\".format(i))\n",
    "    plt.hist(H1i, bins=bins, color=\"C{}\".format(i), histtype=\"step\", lw=2)\n",
    "    plt.axvline(np.percentile(H1i, (1 - beta) * 100), 0, 1, lw=3,\n",
    "                color=\"C{}\".format(i),\n",
    "                label=\"Scale: {:.1f}\".format(scales[i]))\n",
    "    plt.plot(x, nsamples * np.diff(bins)[0] *\n",
    "             scs.rayleigh.pdf(x, loc=0, scale=scales[i]),\n",
    "             color=\"C{}\".format(i), lw=2, ls=\"--\")\n",
    "    i += 1\n",
    "\n",
    "plt.axvline(TS_val, 0, 1, color=\"k\", ls=\"--\", lw=2)\n",
    "plt.xlim(0, xmax)\n",
    "plt.xlabel(\"TS value\")\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For the rayleigh distribution we even can get the analytic solution to the problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "true_scale = rayleigh_quantiles(TS_val, p=1. - beta)\n",
    "print(\"Analytic scale factor for TS value {:.1f} is: {:.3f}\".format(\n",
    "      TS_val, true_scale))\n",
    "\n",
    "# For a showcase let's make some distrubtions of the alternative hypothesis\n",
    "nsamples = 10000\n",
    "H1 = np.random.rayleigh(scale=true_scale, size=nsamples)\n",
    "\n",
    "# Their 1-beta fraction lies further to the right with incresing scale\n",
    "plt.hist(H1, bins=bins, alpha=0.3, color=\"C7\")\n",
    "plt.hist(H1, bins=bins, color=\"C7\", histtype=\"step\", lw=2)\n",
    "plt.axvline(np.percentile(H1, (1. - beta) * 100), 0, 1, lw=3, color=\"C7\")\n",
    "plt.plot(x, nsamples * np.diff(bins)[0] *\n",
    "         scs.rayleigh.pdf(x, loc=0, scale=true_scale),\n",
    "         color=\"C7\", lw=2, ls=\"--\")\n",
    "\n",
    "plt.axvline(TS_val, 0, 1, color=\"k\", ls=\"--\", lw=2)\n",
    "plt.xlim(0, xmax)\n",
    "plt.xlabel(\"TS value\")\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Fitting it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now instead of guessing where the right scale is, we try to fit it.\n",
    "For this we choose a scale, create a sample and check how close the desired 1-beta percentile is to the TS value we defined.\n",
    "From the plot above, we already see, that the sought after scale should be between 4 and 5.\n",
    "\n",
    "The more we sample the more accurate the result is.\n",
    "Here we know the distribution and could get exact results, but in reality we often have to sample to get the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Root Finder\n",
    "\n",
    "**This can be implemented easily as an automated method, as seen below**\n",
    "\n",
    "Here the fit is a root search.\n",
    "The target function is defined as the difference between the targeted TS values and the percentile of the current sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def do_trials(scale, nsamples=10000):\n",
    "    return np.random.rayleigh(scale=scale, size=nsamples)\n",
    "\n",
    "def fit_root(scale, nsamples):\n",
    "    sam = do_trials(scale, nsamples)\n",
    "    return TS_val - np.percentile(sam, 100 * (1. - beta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "res = sco.brentq(fit_root, 0., 10., args=(10000))\n",
    "print(\"TS value: {:.3f}\".format(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "See how much we spread around the true value with that method.\n",
    "\n",
    "We see:\n",
    "\n",
    "1. The spread gets smaller with more trials per sample, as expected\n",
    "2. The estimator is pretty unbiased\n",
    "\n",
    "Note: Setting xtol and rtol has not a real influence on the spread.\n",
    "It still get really close to the break condition by chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "res = [sco.brentq(fit_root, 0., 10., args=(1000), xtol=1e-10, rtol=1e-10)\n",
    "       for i in range(1000)]\n",
    "plt.hist(res, bins=30)\n",
    "plt.axvline(rayleigh_quantiles(TS_val, p=0.1), 0, 1,\n",
    "            ls=\"--\", color=\"C1\", lw=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Least Squares Scan with Parabola Fit\n",
    "\n",
    "**This is more or less a manual method, where we select the support point by hands**\n",
    "\n",
    "Alternatively we can define a least squares fit function with the squared distance and do a minimization.\n",
    "But the simple minimization doesn't work because the samples are different even for very close scale parameters due to the finite sample size.\n",
    "So instead we scant he loss function and fit a parabole (exact shape) to the scan points.\n",
    "The minimum of the parabola is then the sought after scale parameter.\n",
    "Also an error estimate can be given using the covariance of the fit parameter.\n",
    "\n",
    "Note: The parabola might only be exact in a small range around the minimum, because the percentile might be a very nonlinear function in the scale parameter.\n",
    "Nevertheless the approcimation should be good close around the minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def do_trials(scale, nsamples=10000):\n",
    "    return np.random.rayleigh(scale=scale, size=nsamples)\n",
    "\n",
    "def fit_ls(scale, nsamples):\n",
    "    sam = do_trials(scale, nsamples)\n",
    "    return (TS_val - np.percentile(sam, 100 * (1. - beta)))**2\n",
    "\n",
    "def parabola(x, a, b):\n",
    "    return a * (x - b)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# This sticks to the seed, because the function always changes due to the\n",
    "# sampling in each step\n",
    "for seed in [1., 2., 3., 4.]:\n",
    "    res = sco.minimize(fit_ls, seed, bounds=[[0., 10.]], args=(10000))\n",
    "    print(\"Seed is {:.1f} -> TS value: {:.3f}\".format(seed, res.x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "So we do a scan and fit a parabola:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Fit a parabola to the sample points to make the estimate more robust\n",
    "# scale = np.linspace(0.1, 4., 5)\n",
    "scale = np.arange(10)\n",
    "y = np.array([fit_ls(s, nsamples=1000) for s in scale])\n",
    "bf, cov = sco.curve_fit(parabola, scale, y, p0=[1., 1.],\n",
    "                      bounds=[[0., 0.], [np.inf, np.inf]])\n",
    "\n",
    "x = np.linspace(0., 10., 100)\n",
    "y_fit = parabola(x, *bf)\n",
    "\n",
    "plt.plot(scale, y)\n",
    "plt.plot(x, y_fit, ls=\"--\")\n",
    "pb_min = bf[1]\n",
    "plt.axvline(pb_min, 0, 1, ls=\"--\", color=\"C1\")\n",
    "\n",
    "stddev =  np.sqrt(cov[1, 1])\n",
    "print(\"TS value: {:.3f} ± {:.3f} ({:.1f}%)\".format(\n",
    "    pb_min, stddev, stddev / pb_min * 100))\n",
    "\n",
    "plt.xlabel(\"TS value\")\n",
    "plt.ylabel(\"Least Squares Value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "See how much we spread around the true value with that method.\n",
    "\n",
    "We see:\n",
    "\n",
    "1. More samples, less spreads, as before\n",
    "2. Seems to be unbiased\n",
    "3. Spread seems as big as in root finder method, see below for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "res = []\n",
    "scale = np.linspace(0.1, 10., 10)\n",
    "ntrials = 1000\n",
    "nsampels = 1000\n",
    "\n",
    "for i in range(ntrials):\n",
    "    y = np.array([fit_ls(s, nsamples=nsamples) for s in scale])\n",
    "    bf, _ = sco.curve_fit(parabola, scale, y, p0=[1., 1.],\n",
    "                          bounds=[[0., 0], [np.inf, np.inf]])\n",
    "    pb_min = bf[1]\n",
    "    res.append(pb_min)\n",
    "    \n",
    "plt.hist(res, bins=15)\n",
    "plt.axvline(rayleigh_quantiles(TS_val, p=0.1), 0, 1,\n",
    "            ls=\"--\", color=\"C1\", lw=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Compare to root finder trials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "res_bq = [sco.brentq(fit_root, 0., 10., args=(nsamples),\n",
    "                     xtol=1e-10, rtol=1e-10) for i in range(ntrials)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "h_pb, b, _ = plt.hist(res, bins=30, histtype=\"step\", label=\"Parabola\", lw=3)\n",
    "h_bq, _, _ = plt.hist(res_bq, bins=b, histtype=\"step\", label=\"Roots BQ\", lw=3)\n",
    "\n",
    "plt.errorbar(get_binmids(b), h_pb, yerr=np.sqrt(h_pb), c=\"C0\", fmt=\"none\")\n",
    "plt.errorbar(get_binmids(b), h_bq, yerr=np.sqrt(h_bq), c=\"C1\", fmt=\"none\")\n",
    "\n",
    "plt.axvline(rayleigh_quantiles(TS_val, p=0.1), 0, 1,\n",
    "            ls=\"--\", color=\"C7\", lw=3, label=\"True\")\n",
    "\n",
    "plt.axvline(np.mean(res), 0, 1, color=\"C0\", ls=\"--\", label=\"Mean PB\")\n",
    "plt.axvline(np.mean(res_bq), 0, 1, color=\"C1\", ls=\"--\", label=\"Mean BQ\")\n",
    "\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Fitting the CDF\n",
    "\n",
    "**This would also be a manual method. We choose some injection values and do trials for each to get supprt point for the CDF fit. Problem here is, what is the exact shape of the CDF?**\n",
    "\n",
    "Instead of fitting the loss function asnd looking for its minimum, we could also note the percentile above a certain threshold for each set of trials with a different expectation.\n",
    "Then we can fit a CDF directly to the percentile vs mu plot.\n",
    "Also we can reuse a set of trials for every answer we want to have, sensitivitys, disc. potential etc. by altering the percentile definiton afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def do_trials(scale, nsamples=10000):\n",
    "    return np.random.rayleigh(scale=scale, size=nsamples)\n",
    "\n",
    "def percentile(sample, thresh):\n",
    "    \"\"\"\n",
    "    Returns the percentage of sample points above a threshold.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sample : array-like\n",
    "        Data values on which the percentile is calculated.\n",
    "    thresh : float\n",
    "        Threshold in x-space to calculate the percentile against.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    perc : float\n",
    "        Percentile in [0, 1], fraction of data point > val.\n",
    "    err : float\n",
    "        Estimated relative error on the percentile.\n",
    "    \"\"\"\n",
    "    sample = np.atleast_1d(sample)\n",
    "    n = len(sample)\n",
    "    perc = np.sum(sample > thresh) / n\n",
    "    z = 1.  # 1 sigma error (gaussian approximation)\n",
    "    err = z * np.sqrt(perc * (1. - perc) / n)\n",
    "    return perc, err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ntrials = 10000\n",
    "scale_scan = np.arange(0.5, 7, 0.5)\n",
    "\n",
    "percs = []\n",
    "errors = []\n",
    "for si in scale_scan:\n",
    "    perc, err = percentile(do_trials(si, nsamples=ntrials), TS_val)\n",
    "    percs.append(perc)\n",
    "    errors.append(err)\n",
    "\n",
    "def chi2_cdf_fit(x, df, loc, scale):\n",
    "    return scs.chi2.cdf(x, df, loc, scale)\n",
    "\n",
    "def exp_fit(x, a, b, c):\n",
    "    return 1. - a * np.exp(-(x / b)**c)\n",
    "\n",
    "\n",
    "chi2_res, chi2_err = sco.curve_fit(f=chi2_cdf_fit, xdata=scale_scan,\n",
    "                                   ydata=percs, sigma=errors)\n",
    "exp_res, exp_err = sco.curve_fit(f=exp_fit, xdata=scale_scan,\n",
    "                                 ydata=percs, sigma=errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x = np.linspace(0, 7, 200)\n",
    "y_chi2 = chi2_cdf_fit(x, *chi2_res)\n",
    "y_exp = exp_fit(x, *exp_res)\n",
    "\n",
    "plt.errorbar(scale_scan, percs, yerr=errors, fmt=\"o\")\n",
    "plt.plot(x, y_chi2, label=\"chi2({:.2f}, {:.2f}, {:.2f})\".format(*chi2_res))\n",
    "plt.plot(x, y_exp, label=\"1-{:.2f}*exp(-(x/{:.2f})^{:.2f})\".format(*exp_res))\n",
    "\n",
    "plt.xlabel(\"scale\")\n",
    "plt.ylabel(\"CDF\")\n",
    "plt.title(\"Rayleigh. Percentile above TS={:.1f} for {} trials each\".format(\n",
    "    TS_val, ntrials))\n",
    "\n",
    "# Also plot the true percentile vs scale function\n",
    "true_p = 1. - scs.rayleigh.cdf(x=TS_val, scale=x)\n",
    "plt.plot(x, true_p, ls=\"--\", color=\"k\", label=\"True CDF(scale)\")\n",
    "\n",
    "# At which scale is chi2 with beta above TS_val?\n",
    "est_scale = scs.chi2.ppf(beta, *chi2_res)\n",
    "print(\"Estimated true scale: {:.2f}\".format(est_scale))\n",
    "print(\"True scale: {:.2f}\".format(rayleigh_quantiles(p=1.-beta,\n",
    "                                                     TS_val=TS_val)))\n",
    "plt.axhline(beta, 0, 1, ls=\"--\", color=\"k\")\n",
    "plt.axvline(est_scale, 0, 1, ls=\"--\", color=\"k\")\n",
    "\n",
    "plt.axhline(1, 0, 1, ls=\"-\", color=\"C7\")\n",
    "plt.xlim(0, None)\n",
    "plt.ylim(0, 1.1)\n",
    "\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Also test how strong the spread is for repeated experiments.\n",
    "\n",
    "We see:\n",
    "\n",
    "1. For too few trials, the fit often does not converge\n",
    "2. It is overestimating the true scale, because the fit is not good for small scales. Problem here is, in general we don't know the true CDF as in the parabola case, where we had the quadratic loss function (here we can simply plot the truth because we know the Rayleigh CDF)\n",
    "3. Spread seems to be as big as in the previous cases\n",
    "\n",
    "The good thing is, that we always can check, if the fit is good and choose support points where we need them to be.\n",
    "Also we should consider not using the fit, if the estimated parameter falls in a region where the fit is describing the data badly.\n",
    "So we should guess the location of the correct value with a scane, sample densly around it and try to find a CDF that is fitting good in this region, just to interpolate.\n",
    "Could use a spline too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nscans = 1000\n",
    "ntrials = 10000\n",
    "scale_scan = np.arange(0.5, 7, 0.5)\n",
    "est_scales = []\n",
    "\n",
    "for i in range(nscans):\n",
    "    percs = []\n",
    "    errors = []\n",
    "    for si in scale_scan:\n",
    "        perc, err = percentile(do_trials(si, nsamples=ntrials), TS_val)\n",
    "        percs.append(perc)\n",
    "        errors.append(err)\n",
    "\n",
    "    chi2_res, chi2_err = sco.curve_fit(f=chi2_cdf_fit, xdata=scale_scan,\n",
    "                                       ydata=percs, sigma=errors)\n",
    "    est_scales.append(scs.chi2.ppf(beta, *chi2_res))\n",
    "    \n",
    "# We see some failing fits for too few trials. Otherwise the spread is\n",
    "# similar to other methods\n",
    "_ = plt.hist(est_scales, bins=50)\n",
    "plt.axvline(rayleigh_quantiles(p=1-beta, TS_val=TS_val), 0, 1, color=\"C1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Iterative Poisson Reweighting\n",
    "\n",
    "**This Method is used in skylab and csky.**\n",
    "\n",
    "The idea is to do a coarse scan in the scale parameter first to get an idea where the minimum is.\n",
    "From there on we iteratively generate trials around the current best fitting poisson mean mu to enhance the statistics, so we can reuse all trials made so far.\n",
    "\n",
    "So we do:\n",
    "\n",
    "1. Make a coarse grid scan until the 1-beta quantile is close enough to the desired TS value to get a good starting seed for the fit..\n",
    "2. For the corrsponding number of events to inject we start the iterative minimization.\n",
    "   We iterate until the error on the quantile is lower than a given tolerance.\n",
    "3. We get the error from the Wald (gaussian) approximation to the binomial error interval on the estimator $\\hat{p}$, because we have a fixed number of trials and estimate the number of trials falling in one of two regions TS > TS_val.\n",
    "   The loss function is a least squares loss with the distance from the current quantile TS value to the desired TS value.\n",
    "3. With the current best fit poisson mean number of injected events we do more batches of trials by drawing the actual number of events to inject from a poisson distribution with the best fit mean until our precision is good enough.\n",
    "\n",
    "**Read here for weighted Wald Binomial error:**\n",
    "1. https://stats.stackexchange.com/questions/159204/how-to-calculate-the-standard-error-of-a-proportion-using-weighted-data\n",
    "2. https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval#Normal_approximation_interval\n",
    "\n",
    "**And to not forget the reference, weighted Poisson errors:** http://www.hep.uiuc.edu/e687/memos/weight_err.PS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def make_ns_poisson_weights(mu, ns):\n",
    "    \"\"\"\n",
    "    The values of ns are drawn from poisson PDF with a specific mu.\n",
    "    To reuse trials generated from a different mu, we can reweight the ns\n",
    "    values to match the current mu.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mu : float\n",
    "        Possion expectation value, >= 0.\n",
    "    ns : array-like\n",
    "        ns values from various trials.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    weights : array-like\n",
    "        Weights to reweights ns values to the poissnian with expectaion mu.\n",
    "    hist : array-like\n",
    "        Number of events falling in each ns bin.\n",
    "    \"\"\"\n",
    "    if mu < 0.:\n",
    "        raise ValueError(\"Poisson expectation 'mu' must be >= 0.\")\n",
    "    # Bin ns values to assign proper weight\n",
    "    hist = np.bincount(ns)\n",
    "    nbins = len(hist)\n",
    "    bins = np.arange(nbins)\n",
    "    # Get probability per bin for current mu\n",
    "    pmf = scs.poisson.pmf(bins, mu)\n",
    "    # Get weights, split equally per bin\n",
    "    weights = np.zeros(nbins)\n",
    "    mask = hist > 0.\n",
    "    weights[mask] = pmf[mask] / hist[mask]\n",
    "    # Reassign weight to per trial ns\n",
    "    idx = np.digitize(ns, bins, right=True)\n",
    "    return weights[idx], hist\n",
    "\n",
    "def get_weighted_percentile(x, val, w=None):\n",
    "    \"\"\"\n",
    "    Calculate the weighted percentile of data `x` with weight `w`.    \n",
    "\n",
    "    This calculates the amount of data in `x` lying under the threshold\n",
    "    `val` (analogue to np.percentile).\n",
    "    The relativ error is estimated from weighted counting statistics:\n",
    "    \n",
    "        sum(w[x <= val]) / sum(w)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array-like\n",
    "        Data values on which the percentile is calculated.\n",
    "    val : float\n",
    "        Threshold in x-space to calculate the percentile against.\n",
    "    w : array-like\n",
    "        Weight for each data point. If None, all weights are assumed to be 1.\n",
    "        (default: None)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    perc : float\n",
    "        Percentile in [0, 1], fraction of data point > val.\n",
    "    err : float\n",
    "        Estimated relative error on the percentile.\n",
    "    \"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    \n",
    "    if w is None:\n",
    "        w = np.zeros_like(x) + 1. / len(x)\n",
    "    elif np.sum(w**2) == 0:\n",
    "        raise ValueError(\"Squared weight sum is zero, so weights are zero.\")\n",
    "        \n",
    "    # Get weighted percentile\n",
    "    mask = x > val\n",
    "    perc = np.sum(w[mask]) / np.sum(w)\n",
    "    # Binomial error on weighted percentile in Wald approximation\n",
    "    err = np.sqrt(perc * (1. - perc) * np.sum(w**2))\n",
    "    \n",
    "    return perc, err\n",
    "\n",
    "def performance(ts_val, beta, trial_func, mu0=None, ntrials=100,\n",
    "                tol_perc_err=5e-3, tol_mu_rel=1e-3, maxloops=100,\n",
    "                verb=False):\n",
    "    \"\"\"\n",
    "    Iteratively search for the best fit `mu`, so that a fraction `beta` of\n",
    "    the scaled PDF lies above the background test statistic value `ts_val`.\n",
    "\n",
    "    Performance search on a PDF parameter `mu` which is the expectation value\n",
    "    for a poisson PDF via a second variable defining a test statistic which\n",
    "    is directly influenced by the choice of `mu`.\n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    ts_val : float\n",
    "        Test statistic value of the BG distribution, which is connected to\n",
    "        the alpha value (Type I error).\n",
    "    beta : float\n",
    "        Fraction of alternative hypothesis PDF that should lie right of the\n",
    "        `ts_val`.\n",
    "    trial_func : callable\n",
    "        Function returning a single trial depending on the value of the\n",
    "        current parameter `mu`.\n",
    "    mu0 : float, optional\n",
    "        Seed value to begin the minimization at. If None a region close to\n",
    "        the minimum is searched for automatically. If explicitely given, it\n",
    "        must be >= 0. (default: None)\n",
    "    ntrials : int, optional\n",
    "        How many new trials to make per new iteration. (default: 100)\n",
    "    tol_perc_err, tol_mu_rel : float, optional\n",
    "        The iteration stops when BOTH of the following conditons are met:\n",
    "        \n",
    "        - The error on the estimated percentile for the current best fit\n",
    "          ``mu`` is ``errors[-1] <= tol_perc_err`` AND\n",
    "        - The relative difference in the best fit ``mus`` is\n",
    "          ``abs(mus[-1]-mus[-2])/mus[-1]<= tol_mu_rel``.\n",
    "\n",
    "        Furthermore the conditions must be met in BOTH the last AND second to\n",
    "        last trial loops to avoid a break on accidental fluctuations.\n",
    "        (default: tol_perc_err: 5e-3, tol_mu_rel: 1e-3)\n",
    "    maxloops : int, optional\n",
    "        Break the minimization process after this many loops with ntrials\n",
    "        trials each. (default: 100)\n",
    "    verb : bool, optional\n",
    "        If ``True``print convergence message during fit. (default: False)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    res : dict\n",
    "        Result dictionary with keys:\n",
    "        \n",
    "        - \"mu_bf\": Best fit mu, equal to mu[-1].\n",
    "        - \"mu\": List of visited mu values during minimization.\n",
    "        - \"ts\": List of generated TS values during minimization.\n",
    "        - \"ns\": List of generared ns values during minimization.\n",
    "        - \"err\": List of errors on the weighted TS percentile per iteration.\n",
    "        - \"perc\": List of estimated TS percentiles per iteration.\n",
    "        - \"nloops\": Number of iterations needed to converge.\n",
    "        - \"ninitloops\": Number of initial scan iterations done.\n",
    "        - \"lastfitres\": scipy.optimize.OptimizeResult of the last fit.\n",
    "        - \"converged\": Boolean, if ``True`` fit converged within maxloops.\n",
    "    \"\"\"\n",
    "    def loss(mu, ns, ts):\n",
    "        \"\"\"\n",
    "        Logged least squares loss for percentile distance to beta. No\n",
    "        gradient is returned, because is has a pole at the minimum and the\n",
    "        minimizer doesn't like that.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        mu : float\n",
    "            Current expectation value for poisson PDF.\n",
    "        ns : array-like\n",
    "            ns values from all trials done so far.\n",
    "        ts : array-like\n",
    "            Test statistic values from all trials done so far.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        loss : float and array-like\n",
    "            Value of the loss function at the current mu.\n",
    "        \"\"\"\n",
    "        # Reweight TS trials using the poisson statistics of ns\n",
    "        w, _ = make_ns_poisson_weights(mu=mu, ns=ns)\n",
    "        perc, _ = get_weighted_percentile(x=ts, val=ts_val, w=w)\n",
    "        return np.log10((perc - beta)**2)\n",
    "        \n",
    "    def append_batch_of_trials(n, mu, ns, ts):\n",
    "        \"\"\"Do n trials and append result to ns and ts arrays\"\"\"\n",
    "        for ni in np.random.poisson(mu, size=n):\n",
    "            ns = np.append(ns, ni)\n",
    "            ts = np.append(ts, trial_func(ni))\n",
    "        return ns, ts\n",
    "    \n",
    "    def get_perc_and_err(mu, ns, ts):\n",
    "        \"\"\"\n",
    "        Get the percentile and its relative error for all trials under\n",
    "        the current best fit mu.\n",
    "        \"\"\"\n",
    "        w, _ = make_ns_poisson_weights(mu=mu, ns=ns)\n",
    "        perc, err = get_weighted_percentile(x=ts, val=ts_val, w=w)\n",
    "        return perc, err\n",
    "    \n",
    "    # Keep track of progress\n",
    "    mus = np.array([], dtype=np.float)\n",
    "    ts = np.array([], dtype=np.float)\n",
    "    ns = np.array([], dtype=np.int)\n",
    "    errors = np.array([], dtype=np.float)\n",
    "    percs = np.array([], dtype=np.float)\n",
    "    n_init_loops = 0\n",
    "    n_loops = 0\n",
    "    converged = False\n",
    "    \n",
    "    # If no seed given, start initial scan to get close to the minimum\n",
    "    dmu = 5.  # Must be handtuned to the problem...\n",
    "    if mu0 is None:\n",
    "        n_init_trials = 10  # Not too few but also not too many for 1st scan\n",
    "        def frac_over_tsval(ts):\n",
    "            \"\"\"Fraction of 'n_init_trials' last trials above 'ts_val'\"\"\"\n",
    "            if len(ts) < n_init_trials:\n",
    "                return 0.\n",
    "            return (np.sum(ts[-n_init_trials:] > ts_val) /\n",
    "                    n_init_trials)\n",
    "        \n",
    "        mu = 1.        \n",
    "        stop = False\n",
    "        while not stop:\n",
    "            ns, ts = append_batch_of_trials(n_init_trials, mu, ns, ts)\n",
    "            \n",
    "            # Save progress\n",
    "            mus = np.append(mus, mu)\n",
    "            # Err est. is not reliable here, too few trials, so use worst err\n",
    "            perc, _ = get_perc_and_err(mu, ns, ts)\n",
    "            errors = np.append(errors, 1.)\n",
    "            percs = np.append(percs, perc)       \n",
    "            \n",
    "            # Overshoot in the last two batches to have trials above best fit\n",
    "            if n_init_loops > 2:\n",
    "                stop = ((frac_over_tsval(ts) > beta) and\n",
    "                        (frac_over_tsval(ts[:-n_init_trials]) > beta))\n",
    "                \n",
    "            mu += dmu\n",
    "            n_init_loops += 1\n",
    "           \n",
    "        if verb:\n",
    "            print(\"Made {} intitial scan loops with {} trials total\".format(\n",
    "                n_init_loops, len(ts)))\n",
    "\n",
    "    elif mu0 < 0.:\n",
    "        raise ValueError(\"Seed `mu0` must be >= 0.\")\n",
    "    else:\n",
    "        # Init and do first batch of trials\n",
    "        mu = mu0\n",
    "        mus = np.append(mus, mu)\n",
    "        errors = np.append(errors, 1.)\n",
    "        percs = np.append(percs, -1.)\n",
    "        ns, ts = append_batch_of_trials(ntrials, mu, ns, ts)\n",
    "\n",
    "    # Process minimizer loop until last two rel. error are below tolerance\n",
    "    stop = False\n",
    "    while n_loops <= 2 or not stop:\n",
    "        # Make new batch of trials\n",
    "        ns, ts = append_batch_of_trials(ntrials, mu, ns, ts)\n",
    "           \n",
    "        # Now fit the poisson expectation by reusing all (reweighted) trials\n",
    "        # Bounds: 90% CL central interval around best mu\n",
    "        bl, bu = scs.poisson.interval(0.90, mu)\n",
    "        bounds = [bl, max(bu, 1.)]  # Avoid [0, 0] for small mu\n",
    "        # Do a seed scan prior to fitting to avoid local minima\n",
    "        seeds = np.arange(*bounds)\n",
    "        seed = seeds[np.argmin([loss(mui, ns, ts) for mui in seeds])]\n",
    "               \n",
    "        res = sco.minimize(loss, [seed], bounds=[bounds], args=(ns, ts),\n",
    "                           jac=False, method=\"L-BFGS-B\",\n",
    "                           options={\"ftol\": 100, \"gtol\": 1e-8})\n",
    "        mu = res.x\n",
    "        perc, err = get_perc_and_err(mu, ns, ts)\n",
    "              \n",
    "        # Make some manual tweaks to help the minimizer: (credit: mrichman)\n",
    "        oldmu = mus[-1]\n",
    "        # New fit is suddenly more than 50% above old fit: Truncate change\n",
    "        if np.abs(mu - oldmu) / oldmu > 0.5:\n",
    "            if mu > oldmu:\n",
    "                mu = 1.5 * oldmu\n",
    "            else:\n",
    "                mu = 0.5 * oldmu\n",
    "            err = errors[-1]  # Make sure we definitely do another trial\n",
    "        # Fit is identically to previous fit\n",
    "        if mu == oldmu:\n",
    "            mu = 1.1 * oldmu  # Choose larger mu: conservative -> more flux\n",
    "            err = errors[-1]\n",
    "\n",
    "        # Save the progress\n",
    "        mus = np.append(mus, mu)\n",
    "        errors = np.append(errors, err)\n",
    "        percs = np.append(percs, perc)\n",
    "        n_loops += 1\n",
    "               \n",
    "        # Error conditions must match in the last and last to last trials\n",
    "        if n_loops > 2:\n",
    "            mu_rel_err1 = np.abs(mus[-1] - mus[-2]) / mus[-1]\n",
    "            mu_rel_err2 = np.abs(mus[-2] - mus[-3]) / mus[-2]\n",
    "            if ((mu_rel_err1 <= tol_mu_rel) and\n",
    "                    (mu_rel_err2 <= tol_mu_rel) and\n",
    "                    (errors[-1] < tol_perc_err) and\n",
    "                    (errors[-2] < tol_perc_err)):\n",
    "                if verb:\n",
    "                    print(\"Break: below tol_mu_rel and tol_perc_err.\")\n",
    "                converged = True\n",
    "                stop = True\n",
    "\n",
    "        if n_loops == maxloops:\n",
    "            if verb:\n",
    "                print(\"Manual break after {} loops with {} main \".format(\n",
    "                      n_loops, n_loops * ntrials) + \"trials: Reached \" +\n",
    "                      \"`maxloops` loops.\")\n",
    "            break\n",
    "        \n",
    "    return {\"mu_bf\": mus[-1], \"mus\": mus, \"ts\": ts, \"ns\": ns, \"err\": errors,\n",
    "            \"perc\": percs, \"nloops\": n_loops, \"ninitloops\": n_init_loops,\n",
    "            \"lastfitres\": res, \"converged\": converged}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Test reweighting\n",
    "\n",
    "Show that the reweighting works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mu = 3.9\n",
    "# mu = 0.  # Uncomment to verify mu = 0 works too\n",
    "xmax = 2 * np.ceil((mu + 1))\n",
    "mids = np.arange(0.5, xmax, 1.)\n",
    "ns = np.random.randint(0, xmax, 10)\n",
    "w, nevts = make_ns_poisson_weights(mu, ns)\n",
    "\n",
    "x = np.arange(0, xmax + 1)\n",
    "y = scs.poisson.pmf(x, mu)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.hist(ns, bins=np.arange(xmax + 1), weights=w)\n",
    "ax.plot(np.r_[x[0], x], np.r_[0, y], drawstyle=\"steps-post\",\n",
    "        label=\"$\\mu={:.1f}$\".format(mu))\n",
    "\n",
    "# Show bin content on 2nd axis\n",
    "ax2 = ax.twiny()\n",
    "ax2.set_xlim(ax.get_xlim())\n",
    "ax2.set_xticks(mids)\n",
    "ax2.set_xticklabels([\"{:d}\".format(ni) for ni in nevts])\n",
    "\n",
    "# Lines to bins helping to track the 2nd axis down\n",
    "for m in mids:\n",
    "    ax.axvline(m, 0, 1, ls=\"--\", color=\"C7\", zorder=-1, alpha=0.5)\n",
    "ax.fill_between(np.r_[x[0], x], 0, np.r_[0, y], color=\"w\", step=\"post\")\n",
    "\n",
    "ax.set_xlabel(\"k\")\n",
    "ax2.set_xlabel(\"Events per bin\")\n",
    "ax.set_ylabel(\"Probability\")\n",
    "\n",
    "ax.legend(loc=\"best\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Get weighted percentiles and compare unweighted case to numpy percentile function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mu = 5\n",
    "xmax = 2 * np.ceil((mu + 1))\n",
    "ns = np.arange(3 * xmax, dtype=int)\n",
    "w, nevts = make_ns_poisson_weights(mu, ns)\n",
    "\n",
    "perc_val = 2\n",
    "\n",
    "print(\"Sorted ns values:\")\n",
    "print(np.sort(ns))\n",
    "\n",
    "print(\"Requested quantile at ns={}\".format(perc_val))\n",
    "\n",
    "print(\"\\nWeighted percentile: {:.3f}\\nRel. Error: {:.3f}\".format(\n",
    "    *get_weighted_percentile(ns, val=perc_val, w=w)))\n",
    "\n",
    "print(\"\\n\\nNow with weights all ones (Should be close to numpy):\\n\")\n",
    "alpha = np.linspace(0, 100, 11)\n",
    "for a in alpha:\n",
    "    np_val = np.percentile(ns, a, interpolation=\"lower\")\n",
    "    print(\" Get a numpy.percentile at alpha = {:.1f}: {:.2f}\".format(\n",
    "        a / 100., np_val))\n",
    "    print(\" Numpy value put back in the function: {:.2f}\".format(\n",
    "        1 - get_weighted_percentile(ns, val=np_val, w=None)[0]))\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Test error estimate\n",
    "\n",
    "Use the known Rayleigh distribution for that, so we can use the exact position of the percentile.\n",
    "Generate trials and construct the percentile and error intervall.\n",
    "Then count how many times the intervalls hit the correct percentile.\n",
    "They should do so in the correct fraction alpha as they were constructed with.\n",
    "\n",
    "Note:\n",
    "\n",
    "The Wald approximation only works, if the estimated percentile is not close to 0 or 1.\n",
    "If we have to few events to test, we are more likely to end up with the stimated p close to 0 or 1 even if we test for a beta fartger away of 0 or 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "niter = 10000\n",
    "ntrials = 100  # See how it breaks down with ntrials < ~30\n",
    "true_scale = rayleigh_quantiles(TS_val, 1. - beta)\n",
    "\n",
    "print(\"Doing {:d} times {:d} trials with \".format(niter, ntrials) +\n",
    "      \"true scale = {:.2f} ({:.1f}% over TS val of {:.1f})\".format(\n",
    "          true_scale, beta * 100, TS_val))\n",
    "\n",
    "perc, err = [], []\n",
    "contains_true = []\n",
    "for i in range(niter):\n",
    "    sample = np.random.rayleigh(scale=true_scale, size=ntrials)\n",
    "    pi, ei = get_weighted_percentile(sample, val=TS_val, w=None)\n",
    "    perc.append(pi)\n",
    "    err.append(ei)\n",
    "    if (beta <= pi + ei) & (beta >= pi - ei):\n",
    "        contains_true.append(True)\n",
    "    else:\n",
    "        contains_true.append(False)\n",
    "        \n",
    "plt.errorbar(np.arange(niter), perc, yerr=err, fmt=\"none\")\n",
    "l = \"Coverage = {:.2f}%\".format(100. * np.sum(contains_true) / niter)\n",
    "plt.axhline(beta, 0, 1, ls=\"--\", color=\"C1\", label=l)\n",
    "plt.title(r\"{:d} times {:d} trials. {} ($1\\sigma$)\".format(\n",
    "    niter, ntrials, \"Poisson Error\" if poisson_error else \"Binomial Error\"))\n",
    "plt.legend()\n",
    "\n",
    "# plt.savefig(\"data/figs/sensitivity_error_coverage_{}.png\".format(\n",
    "#     \"poisson\" if poisson_error else \"binomial\"), dpi=200)\n",
    "\n",
    "print(\"Coverage = {:.2f}%\".format(100. * np.sum(contains_true) / niter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here we test the standard poisson interval on the estimate of the expectation mu for completeness.\n",
    "\n",
    "The case of a weighted histogram translate as follows:\n",
    "We have a single weighted measurement, so the estimated expectaion is sum of weights.\n",
    "Using the formula for variance we get the error on that by using sum of squared weights.\n",
    "\n",
    "For multiple trials we divide by the number of trials as usual and the error decreases with sqrt(N).\n",
    "Similar to the binomial case the error interval is only reliable if the expectation value is high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "niter = 1000\n",
    "# If the true mu is high the number of trials don't matter. The estimate on\n",
    "# mu gets better and the error decreases, but the coverage is always correct\n",
    "ntrials = 1\n",
    "true_mu = rayleigh_quantiles(TS_val, 1. - beta) * 100\n",
    "poisson_error = True\n",
    "alpha = 1. - 0.68  # 1 sigma\n",
    "\n",
    "print(\"Doing {:d} times {:d} trials with \".format(niter, ntrials) +\n",
    "      \"true mu = {:.2f} ({:.1f}% over TS val of {:.1f})\".format(\n",
    "          true_mu, beta * 100, TS_val))\n",
    "\n",
    "mus, err = [], []\n",
    "contains_true = []\n",
    "for i in range(niter):\n",
    "    sample = np.random.poisson(true_mu, size=ntrials)\n",
    "    mu_est = np.mean(sample)\n",
    "    ei = np.sqrt(mu_est / ntrials)\n",
    "    err.append(ei)\n",
    "    mus.append(mu_est)\n",
    "    if (true_mu <= mu_est + ei) & (true_mu >= mu_est - ei):\n",
    "        contains_true.append(True)\n",
    "    else:\n",
    "        contains_true.append(False)\n",
    "        \n",
    "plt.errorbar(np.arange(niter), mus, yerr=err, fmt=\".\",\n",
    "             markersize=1)\n",
    "l = \"Coverage = {:.2f}%\".format(100. * np.sum(contains_true) / niter)\n",
    "plt.axhline(true_mu, 0, 1, ls=\"--\", color=\"C1\", label=l)\n",
    "plt.title(r\"{:d} times {:d} trials. {} ($1\\sigma$)\".format(\n",
    "    niter, ntrials, \"Poisson Error\" if poisson_error else \"Binomial Error\"))\n",
    "plt.legend()\n",
    "\n",
    "# plt.savefig(\"data/figs/sensitivity_error_coverage_{}.png\".format(\n",
    "#     \"poisson\" if poisson_error else \"binomial\"), dpi=200)\n",
    "\n",
    "print(\"Coverage = {:.2f}%\".format(100. * np.sum(contains_true) / niter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Test iterative fit\n",
    "\n",
    "Fit a single example and plot some performance curves of the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "def get_one_trial(n):\n",
    "    \"\"\"Wrapper to convert ns to scale parameter\"\"\"\n",
    "    scale = n / 100.\n",
    "    return np.random.rayleigh(scale=scale, size=1)\n",
    "\n",
    "ntrials = 100\n",
    "mu0 = None  # Performs better with intial scan because we have more stats\n",
    "tol_perc_err = 5e-3\n",
    "tol_mu_rel = 1e-3\n",
    "maxloops = 100\n",
    "\n",
    "res = performance(ts_val=TS_val, beta=beta, trial_func=get_one_trial,\n",
    "                  ntrials=ntrials, mu0=mu0, tol_perc_err=tol_perc_err,\n",
    "                  tol_mu_rel=tol_mu_rel, maxloops=maxloops)\n",
    "\n",
    "mu = res[\"mu_bf\"]\n",
    "mus = res[\"mus\"]\n",
    "ns = res[\"ns\"]\n",
    "ts = res[\"ts\"]\n",
    "percs = res[\"perc\"]\n",
    "nloops = res[\"nloops\"]\n",
    "\n",
    "true_scale = rayleigh_quantiles(TS_val, p=1. - beta)\n",
    "print(\"Analytic scale factor for TS value \" +\n",
    "      \"{:.1f} and beta {:.2f} is: {:.3f}\".format(TS_val, beta, true_scale))\n",
    "print(\"Best scale: {:.3f}\".format(mu / 100.))\n",
    "print(\"Last fit res:\", res[\"lastfitres\"])\n",
    "print(\"Total trials: {:d}\".format(len(ns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Plot fit summary over the iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mu_bf = res[\"mu_bf\"]\n",
    "mus = res[\"mus\"]\n",
    "ns = res[\"ns\"]\n",
    "ts = res[\"ts\"]\n",
    "true_scale = rayleigh_quantiles(TS_val, p=1. - beta)\n",
    "\n",
    "fig, [[axtl, axtr], [axbl, axbr]] = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# Plot the scale evolution\n",
    "axtl.plot(res[\"mus\"] / 100.)\n",
    "axtl.axhline(true_scale, 0, 1, color=\"C1\", ls=\"--\", label=\"True scale\")\n",
    "axtl.axvline(res[\"ninitloops\"] - 1, 0, 1, color=\"C7\", ls=\"--\",\n",
    "             label=\"Init loops\")\n",
    "axtl.set_title(\"scale after iteration\")\n",
    "axtl.set_xlabel(\"Iteration i\")\n",
    "axtl.set_ylabel(\"Scale\")\n",
    "axtl.legend(loc=\"best\")\n",
    "\n",
    "# Plot the rel/abs error evolution\n",
    "axtr.plot(res[\"err\"], color=\"C0\", label=\"perc err\")\n",
    "axtr.plot(np.abs(np.diff(mus)) / mus[1:], color=\"C1\", label=\"mu diff abs\")\n",
    "axtr.axhline(tol_perc_err, 0, 1, color=\"C0\", ls=\"--\", label=\"tol perc\")\n",
    "axtr.axhline(tol_mu_rel, 0, 1, color=\"C1\", ls=\"--\", label=\"mu diff tol\")\n",
    "axtr.axvline(res[\"ninitloops\"] - 1, 0, 1, color=\"C7\", ls=\"--\",\n",
    "             label=\"Init loops\")\n",
    "axtr.set_title(\"Performance after ith Iteration\")\n",
    "axtr.set_xlabel(\"Iteration i\")\n",
    "axtr.set_yscale(\"log\", nonposy=\"clip\")\n",
    "axtr.legend(loc=\"best\")\n",
    "\n",
    "# Plot all drawn, unweighted ns values\n",
    "_ = axbl.hist(ns, bins=200)\n",
    "axbl.set_title(\"all ns unweighted\")\n",
    "axbl.axvline(100 * true_scale, 0, 1, color=dg, ls=\"--\",\n",
    "             label=\"100 * (true scale)\")\n",
    "axbl.set_xlabel(\"ns\")\n",
    "axbl.set_ylabel(\"Count\")\n",
    "axbl.legend(loc=\"best\")\n",
    "\n",
    "# Plot all drawn, weighted test statistic values\n",
    "bins = np.arange(0., 3 * np.ceil(mu_bf) / 100., 0.25)\n",
    "w, _ = make_ns_poisson_weights(mu_bf, ns)\n",
    "perc, _ = get_weighted_percentile(ts, TS_val, w)\n",
    "axbr.set_title(\"all ts weighted, perc = {:.6f}\".format(perc))\n",
    "_ = axbr.hist(ts, bins=bins, weights=w)\n",
    "axbr.axvline(TS_val, 0, 1, color=\"C1\", ls=\"--\", label=\"TS val\")\n",
    "axbr.axvline(true_scale, 0, 1, color=dg, ls=\"--\", label=\"True scale\")\n",
    "axbr.set_xlabel(\"TS\")\n",
    "axbr.set_ylabel(\"PDF\")\n",
    "axbr.legend(loc=\"best\")\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"data/figs/performance_poisson_reweight.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Plot loss function for final iteration and best fit location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mu_bf = res[\"mu_bf\"]\n",
    "ns = res[\"ns\"]\n",
    "ts = res[\"ts\"]\n",
    "true_scale = rayleigh_quantiles(TS_val, p=1. - beta)\n",
    "\n",
    "# Repeat function inside `performance` to see the steps\n",
    "def loss(mu, ns, ts):\n",
    "    w, _ = make_ns_poisson_weights(mu=mu, ns=ns)\n",
    "    perc, _ = get_weighted_percentile(x=ts, val=TS_val, w=w)\n",
    "    return np.log10((perc - beta)**2)\n",
    "\n",
    "bounds = scs.poisson.interval(0.90, mu_bf)\n",
    "seeds = np.arange(*bounds)\n",
    "seed = seeds[np.argmin([loss(mui, ns, ts) for mui in seeds])]\n",
    "\n",
    "# Scan the loss function\n",
    "mu_low = 1.\n",
    "mu_hig = 2. * mu_bf\n",
    "mu_rng = np.arange(mu_low, mu_hig).astype(np.float)\n",
    "f = []\n",
    "for mui in mu_rng:\n",
    "    f.append(loss(mui, ns, ts))\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(mu_rng, f, color=\"C0\", label=\"loss\")\n",
    "ax.axvline(mu_bf, 0, 1, ls=\"-\", color=\"C1\", label=\"best fit\")\n",
    "ax.axvline(bounds[0], 0, 1, ls=\":\", color=dg, label=\"fit bounds\")\n",
    "ax.axvline(bounds[1], 0, 1, ls=\":\", color=dg)\n",
    "ax.axvline(true_scale * 100, 0, 1, ls=\"--\", color=dg, label=\"truth\")\n",
    "plt.xlabel(\"100 * scale\")\n",
    "plt.ylabel(\"Loss function\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Test many trials and see best fit distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "def get_one_trial(n):\n",
    "    \"\"\"Wrapper to convert ns to scale parameter\"\"\"\n",
    "    scale = n / 100.\n",
    "    return np.random.rayleigh(scale=scale, size=1)\n",
    "\n",
    "ntrials = 100\n",
    "tol_perc_err = 1e-3\n",
    "tol_mu_rel = 1e-3\n",
    "maxloops = 100\n",
    "\n",
    "nexps = 100\n",
    "res = []\n",
    "for i in tqdm(range(nexps)):\n",
    "    resi = performance(ts_val=TS_val, beta=beta, trial_func=get_one_trial,\n",
    "                       ntrials=ntrials, mu0=None, tol_perc_err=tol_perc_err,\n",
    "                       tol_mu_rel=tol_mu_rel, maxloops=maxloops, verb=False)\n",
    "    res.append(resi)\n",
    "    \n",
    "mu_bfs = np.array([resi[\"mu_bf\"] for resi in res])\n",
    "errs = np.array([resi[\"err\"][-1] for resi in res])\n",
    "percs = np.array([resi[\"perc\"][-1] for resi in res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "true_scale = rayleigh_quantiles(TS_val, p=1. - beta)\n",
    "fig, (axl, axr) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "iteration = np.arange(nexps)\n",
    "mu_errs = np.sqrt(mu_bfs)\n",
    "true_mu = 100 * true_scale\n",
    "\n",
    "# Coverages: They may not make much sense, because the parameters are not\n",
    "# estimated from a random process but are fitted parameters\n",
    "# If the fit is good, more and more intervals hit the true value, but not\n",
    "# as expected by real coverage.\n",
    "perc_cov = (beta <= percs + errs) & (beta >= percs - errs)\n",
    "perc_cov = np.sum(perc_cov) / nexps\n",
    "mu_cov = (true_mu <= mu_bfs + mu_errs) & (true_mu >= mu_bfs - mu_errs)\n",
    "mu_cov = np.sum(mu_cov) / nexps\n",
    "\n",
    "axl.errorbar(iteration, percs, yerr=errs, fmt=\".\")\n",
    "axl.axhline(beta, 0, 1, color=\"C1\", ls=\"--\")\n",
    "axl.set_title(\"Coverage: {:.2%}\".format(perc_cov))\n",
    "\n",
    "axr.errorbar(iteration, mu_bfs, yerr=mu_errs, fmt=\".\")\n",
    "axr.axhline(true_scale * 100, 0, 1, ls=\"--\", color=\"C1\")\n",
    "axr.set_title(\"Coverage: {:.2%}\".format(mu_cov))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Animate it!\n",
    "\n",
    "Same code as above but modified so it animates all the single trials and fits in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_one_trial(n):\n",
    "    \"\"\"Wrapper to convert ns to scale parameter\"\"\"\n",
    "    scale = n / 100.\n",
    "    return np.random.rayleigh(scale=scale, size=1)\n",
    "\n",
    "\n",
    "def performance(ts_val, beta, trial_func,\n",
    "                mu0=None, ntrials=100, eps_rel=1e-3):\n",
    "    def loss(mu, ns, ts):\n",
    "        w, _ = make_ns_poisson_weights(mu=mu, ns=ns)\n",
    "        perc, _ = get_weighted_percentile(x=ts, val=ts_val, w=w)\n",
    "        return np.log10((perc - beta)**2)\n",
    "    \n",
    "    mus = np.array([], dtype=np.float)\n",
    "    ts = np.array([], dtype=np.float)\n",
    "    ns = np.array([], dtype=np.int)\n",
    "    errors = np.array([], dtype=np.float)\n",
    "    n_loops = 0\n",
    "    \n",
    "    if mu0 is None:\n",
    "        mu = -0.9            \n",
    "        n_init_trials = 100  \n",
    "        def frac_over_tsval(ts):\n",
    "            \"\"\"Fraction of 'n_init_trials' last trials above 'ts_val'\"\"\"\n",
    "            if len(ts) < n_init_trials:\n",
    "                return 0.\n",
    "            return (np.sum(ts[-n_init_trials:] > ts_val) /\n",
    "                    n_init_trials)\n",
    "        \n",
    "        n_init_loops = 1\n",
    "        while frac_over_tsval(ts) < beta:\n",
    "            mu += 2.\n",
    "            for n in np.random.poisson(mu, size=n_init_trials):\n",
    "                ns = np.append(ns, n)\n",
    "                ts = np.append(ts, trial_func(n))\n",
    "            n_init_loops += 1\n",
    "            \n",
    "        print(\"Made {} intitial scan loops with {} trials total\".format(\n",
    "            n_init_loops, len(ts)))\n",
    "    elif mu0 < 0.:\n",
    "        raise ValueError(\"Seed `mu0` must be >= 0.\")\n",
    "    else:\n",
    "        mu = mu0\n",
    "        \n",
    "    mus = np.append(mus, mu)\n",
    "    errors = np.append(errors, 1)\n",
    "    for n in np.random.poisson(mu, size=ntrials):\n",
    "            ns = np.append(ns, n)\n",
    "            ts = np.append(ts, trial_func(n))\n",
    "        \n",
    "    while True:\n",
    "        for n in np.random.poisson(mu, size=ntrials):\n",
    "            ns = np.append(ns, n)\n",
    "            ts = np.append(ts, trial_func(n))\n",
    "            \n",
    "        bounds = np.percentile(ns[-ntrials:], [5, 95])\n",
    "        seeds = np.arange(bounds)\n",
    "        seed = seeds[np.argmin([loss(mui, ns, ts) for mui in seeds])]\n",
    "\n",
    "        res = sco.minimize(loss, [seed], bounds=[bounds], args=(ns, ts),\n",
    "                           jac=False, method=\"L-BFGS-B\")       \n",
    "        \n",
    "        mu = res.x\n",
    "        w, _ = make_ns_poisson_weights(mu=mu, ns=ns)\n",
    "        perc, err = get_weighted_percentile(x=ts, val=ts_val, w=w)\n",
    "              \n",
    "        if mu == mus[-1]:\n",
    "            mu = 0.9 * mus[-1]      \n",
    "\n",
    "        mus = np.append(mus, mu)\n",
    "        errors = np.append(errors, err)\n",
    "        n_loops += 1\n",
    "        \n",
    "        yield {\"mu_bf\": mus[-1], \"mus\": mus, \"ts\": ts, \"ns\": ns,\n",
    "                \"err\": errors, \"nloops\": n_loops}\n",
    "\n",
    "\n",
    "def animate(j):\n",
    "    def loss(mu, ns, ts):\n",
    "        w, _ = make_ns_poisson_weights(mu=mu, ns=ns)\n",
    "        perc, _ = get_weighted_percentile(x=ts, val=TS_val, w=w)\n",
    "        return np.log10((perc - beta)**2)\n",
    "\n",
    "    plt.clf()\n",
    "    print(\"Anim {}\".format(j))\n",
    "    \n",
    "    # Add a new trial\n",
    "    res = next(perf_gen)\n",
    "    mu = res[\"mu_bf\"]\n",
    "    ns = res[\"ns\"]\n",
    "    ts = res[\"ts\"]\n",
    "    true_scale = rayleigh_quantiles(TS_val, p=1. - beta)\n",
    "    \n",
    "    rnge = [0, 800]\n",
    "    f = np.zeros(np.diff(rnge), dtype=float)\n",
    "    mu_rng = np.arange(*rnge)\n",
    "\n",
    "    for i, mui in enumerate(mu_rng.astype(float)):\n",
    "        fi = loss(mui, ns, ts)\n",
    "        f[i] = fi\n",
    "    \n",
    "    # Reproduce next step before running new trial to visulaize\n",
    "    bounds = np.percentile(ns[-1000:], [5, 95])\n",
    "    seed = np.argmin([loss(mui, ns, ts) for mui in np.arange(*bounds)])\n",
    "    min_res = sco.minimize(loss, [seed], bounds=[bounds], args=(ns, ts),\n",
    "                           jac=False, method=\"L-BFGS-B\")\n",
    "\n",
    "    plt.plot(mu_rng, f)\n",
    "    plt.axhline(0, 0, 1, ls=\"--\", color=\"k\")\n",
    "    plt.axvline(true_scale * 100, 0, 1, ls=\"--\", color=\"k\")\n",
    "    plt.axvline(mu, 0, 1, ls=\"-\", color=\"k\")\n",
    "    plt.axvline(seed, 0, 1, ls=\"--\", color=\"C1\")\n",
    "    plt.axvline(min_res.x, 0, 1, ls=\"-\", color=\"C1\")\n",
    "    plt.axvline(bounds[0], 0, 1, ls=\":\", color=\"k\")\n",
    "    plt.axvline(bounds[1], 0, 1, ls=\":\", color=\"k\")\n",
    "    return\n",
    "\n",
    "ntrials = 1000\n",
    "perf_gen = performance(ts_val=TS_val, beta=beta, trial_func=get_one_trial,\n",
    "                       ntrials=ntrials, mu0=None)\n",
    "\n",
    "nanim = 30\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ani = animation.FuncAnimation(fig, animate, range(nanim))\n",
    "\n",
    "Writer = animation.writers[\"ffmpeg\"]\n",
    "writer = Writer(fps=2, bitrate=2000)\n",
    "ani.save(filename=\"vid.mp4\", writer=writer)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## csky comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_poisson_weights (mu, ns):\n",
    "    \"\"\"\n",
    "    Weight trials to Poisson distribution.\n",
    "    \"\"\"\n",
    "    bin_counts = np.bincount (ns)\n",
    "    bin_ns = np.arange (bin_counts.size)\n",
    "    if mu > 0:\n",
    "        bin_ps = scs.poisson (mu).pmf (bin_ns)\n",
    "    else:\n",
    "        bin_ps = np.r_[1., np.zeros (bin_counts.size - 1)]\n",
    "    bin_ws = np.zeros (bin_counts.size)\n",
    "    mask = bin_counts > 0\n",
    "    bin_ws[mask] = bin_ps[mask] / bin_counts[mask]\n",
    "    ws = bin_ws[np.searchsorted (bin_ns, ns)]\n",
    "    return ws\n",
    "\n",
    "def get_poisson_mu (ns, tss, thresh, beta, mu_guess):\n",
    "    \"\"\"\n",
    "    Get best guess of mu for ts > thresh at rate ``beta`` with error estimate.\n",
    "    \"\"\"\n",
    "    mask = np.array (tss) > thresh\n",
    "    def log10miss2 (mu):\n",
    "        ws = get_poisson_weights (mu, ns)\n",
    "        f = np.sum (ws[mask]) / np.sum (ws)\n",
    "        return np.log10 ((f - beta)**2)\n",
    "\n",
    "    #bounds = np.percentile (ns[ns > 0], [10, 80])\n",
    "    if mu_guess < 0:\n",
    "        mu_guess = np.median (ns)\n",
    "    bounds = [np.sqrt (mu_guess), mu_guess**2]\n",
    "    mu, f, info = sco.fmin_l_bfgs_b (\n",
    "        log10miss2, [mu_guess],\n",
    "        bounds=[bounds],\n",
    "        approx_grad=True\n",
    "    )\n",
    "    warnflag = info['warnflag']\n",
    "    if False and warnflag:\n",
    "        note = ' ({})'.format (info['task']) if warnflag == 2 else ''\n",
    "        print (22 * ' ' + '- note: warnflag = {}{}'.format (warnflag, note))\n",
    "    mu = mu[0]\n",
    "    ws = get_poisson_weights (mu, ns)\n",
    "    err = np.sqrt (np.sum (ws[mask]**2)) / np.sum (ws)\n",
    "    return mu, err, info\n",
    "\n",
    "def get_n_sig_generic (ts, beta, get_one_trial,\n",
    "                       n_sig_start=-1,\n",
    "                       n_batch=500, tol=4e-3,\n",
    "                       log=True, full_output=False):\n",
    "    \"\"\"\n",
    "    Obtain an n_sig reaching the ``ts`` threshold ``beta`` fraction of trials.\n",
    "    \"\"\"\n",
    "    n_batch = int (n_batch)\n",
    "    assert 0 < tol < 1\n",
    "\n",
    "    # bookkeeping setup\n",
    "    n_batchs = [0]\n",
    "    errs = [1]\n",
    "    ns, tss = np.array ([], dtype=int), np.array ([])\n",
    "\n",
    "    # make a first guess\n",
    "    first_tss = np.array ([])\n",
    "    n_sig = 0 if n_sig_start <= 0 else max (5, n_sig_start - 5)\n",
    "    err =errs[-1]\n",
    "    i = 0\n",
    "    while 1. * np.sum (first_tss > ts) / 10 < beta:\n",
    "        n_sig += 5\n",
    "        i += 10\n",
    "        first_tss = np.array ([])\n",
    "        for i_trial in range (10):\n",
    "            n = np.random.poisson (n_sig)\n",
    "            first_tss = np.r_[first_tss, get_one_trial (n)]\n",
    "        f = 1. * np.sum (np.array (first_tss) > ts) / len (first_tss)\n",
    "\n",
    "    n_sigs = [n_sig]\n",
    "\n",
    "    # loop until stable convergence seems like a realistic possibility\n",
    "    while len (n_batchs) <= 2 or (errs[-1] > tol or errs[-2] / errs[-1] > 10):\n",
    "        new_ns, new_tss = np.array ([], dtype=int), np.array ([])\n",
    "\n",
    "        # try n_sig distributed about best fit so far\n",
    "        for n in np.random.poisson (n_sig, n_batch):\n",
    "            new_ns = np.r_[new_ns, n]\n",
    "            new_tss = np.r_[new_tss, get_one_trial (n)]\n",
    "        # note this batch of trials\n",
    "        ns = np.r_[ns, new_ns]\n",
    "        tss = np.r_[tss, new_tss]\n",
    "        n_sig_last = n_sig\n",
    "        err_last = err\n",
    "        # fit poisson distribution\n",
    "        n_sig, err, info = get_poisson_mu (ns, tss, ts, beta, n_sig)\n",
    "        # slow your roll if the change is > 50%\n",
    "        if np.abs (n_sig_last - n_sig) / n_sig_last > .5:\n",
    "            n_sig = n_sig_last * (1.5 if n_sig > n_sig_last else .5)\n",
    "            err= err_last\n",
    "        # more bookkeeping\n",
    "        n_sigs.append (n_sig)\n",
    "        errs.append (err)\n",
    "        n_batchs.append (n_batchs[-1] + n_batch)\n",
    "        # more fine adjustments\n",
    "        if n_sigs[-2] == n_sigs[-1]:\n",
    "            n_sig *= .9\n",
    "        if (errs[-1] < tol and len (n_batchs) <= 2):\n",
    "            n_sig *= 1.05\n",
    "            errs[-1] = tol + np.finfo (float).eps\n",
    "        if not errs[-1]:\n",
    "            n_sig *= 2\n",
    "\n",
    "    if full_output:\n",
    "        return dict (n_trial=np.array (n_batchs),\n",
    "                     n_sig=np.array (n_sigs),\n",
    "                     err=np.array (errs),\n",
    "                     ns=np.array (ns),\n",
    "                     tss=np.array (tss),\n",
    "                     ws=get_poisson_weights (n_sig, ns),\n",
    "                     n_sig_best=n_sigs[-1])\n",
    "    else:\n",
    "        return n_sig\n",
    "    \n",
    "def get_one_trial(n):\n",
    "    scale = n / 100.\n",
    "    return do_trials(scale=scale, nsamples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "res_csky = get_n_sig_generic(TS_val, beta, get_one_trial, n_sig_start=-1,\n",
    "                             n_batch=500, tol=4e-3, full_output=True)\n",
    "\n",
    "true_scale = rayleigh_quantiles(TS_val, p=1-beta)\n",
    "print(\"Analytic scale factor for TS value {:.1f} is: {:.3f}\".format(\n",
    "      TS_val, true_scale))\n",
    "print(\"Best scale: {:.3f}\".format(res_csky[\"n_sig_best\"] / 100.))\n",
    "print(\"Used {} trials in total.\".format(res_csky[\"n_trial\"][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mu_bf = res_csky[\"n_sig_best\"]\n",
    "ns = res_csky[\"n_sig\"]\n",
    "ts = res_csky[\"tss\"]\n",
    "true_scale = rayleigh_quantiles(TS_val, p=1. - beta)\n",
    "\n",
    "fig, [[axtl, axtr], [axbl, axbr]] = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# Plot the scale evolution\n",
    "axtl.plot(ns / 100.)\n",
    "axtl.axhline(true_scale, 0, 1, color=\"C1\")\n",
    "axtl.set_title(\"scale after iteration\")\n",
    "axtl.set_xlabel(\"iteration i\")\n",
    "\n",
    "# Plot the rel/abs error evolution\n",
    "axtr.plot(np.log10(res_csky[\"err\"]), color=\"C0\", label=\"rel\")\n",
    "axtr.axhline(np.log10(eps_rel), 0, 1, color=\"C0\", ls=\"--\")\n",
    "axtr.set_title(\"log10(uncertainty) after iteration\")\n",
    "axtr.set_xlabel(\"iteration i\")\n",
    "axtr.legend(loc=\"best\")\n",
    "\n",
    "# Plot all drawn, unweighted ns values\n",
    "# Here not so good, because the single ns values are not saved\n",
    "fig.delaxes(axbl)\n",
    "# _ = axbl.hist(ns, bins=50)\n",
    "# axbl.set_title(\"all ns unweighted\")\n",
    "# axbl.set_xlabel(\"ns\")\n",
    "\n",
    "# Plot all drawn, weighted test statistic values\n",
    "bins = np.arange(0., 3 * np.ceil(mu_bf) / 100., 0.25)\n",
    "w = res_csky[\"ws\"]\n",
    "perc, _ = get_weighted_percentile(ts, TS_val, w)\n",
    "axbr.set_title(\"all ts weighted, perc = {:.2f}\".format(perc))\n",
    "_ = axbr.hist(ts, bins=bins, weights=w)\n",
    "axbr.axvline(TS_val, 0, 1, color=\"C1\", ls=\"-\")\n",
    "axbr.axvline(true_scale, 0, 1, color=\"k\", ls=\"--\")\n",
    "axbr.set_xlabel(\"TS\")\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"data/figs/performance_poisson_reweight_csky.png\", dpi=150)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Initialisation Cell",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2 (OSX)",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

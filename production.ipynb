{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the whole production using package methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import helper as hlp\n",
    "\n",
    "import numpy as np\n",
    "import numpy.lib.recfunctions\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mpldates\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.colors import LogNorm, Normalize\n",
    "%matplotlib inline\n",
    "\n",
    "import scipy.interpolate as sci\n",
    "import scipy.optimize as sco\n",
    "import scipy.integrate as scint\n",
    "import scipy.stats as scs\n",
    "import scipy.signal as scsignal\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import json\n",
    "import datetime\n",
    "import pickle\n",
    "from astropy.time import Time as astrotime\n",
    "from corner import corner\n",
    "\n",
    "import sklearn.neighbors as skn\n",
    "import sklearn.model_selection as skms  # Newer version of grid_search\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "from corner_hist import corner_hist\n",
    "from anapymods3.plots.general import (split_axis, get_binmids,\n",
    "                                      hist_marginalize, dg)\n",
    "from anapymods3.stats.sampling import rejection_sampling\n",
    "from anapymods3.general.misc import (fill_dict_defaults,\n",
    "                                     flatten_list_of_1darrays)\n",
    "\n",
    "import tdepps.bg_injector as BGInj\n",
    "import tdepps.bg_rate_injector as BGRateInj\n",
    "import tdepps.rate_function as RateFunc\n",
    "import tdepps.llh as LLH\n",
    "import tdepps.analysis as Analysis\n",
    "from tdepps.utils import rejection_sampling\n",
    "\n",
    "secinday = 24. * 60. * 60."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load IC86 data from epinat, which should be the usual IC86-I (2011) PS sample, but pull corrected and OneWeights corrected by number of events generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "exp = np.load(\"data/IC86_I_data.npy\")\n",
    "mc = np.load(\"data/IC86_I_mc.npy\")\n",
    "# Use the officially stated livetime, not the ones from below\n",
    "livetime = 332.61"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# Make a global sigma cut (only removes a handful of badly reconstructed evts)\n",
    "_mc= mc[mc[\"sigma\"] < np.deg2rad(20)]\n",
    "_exp = exp[exp[\"sigma\"] < np.deg2rad(20)]\n",
    "\n",
    "# `sample` is used as a wrapper for plotting, where it is sometimes easier to\n",
    "# have a normal array. Shape is (nevts, nfeatures), each row is a data point\n",
    "sample = np.vstack((_exp[\"logE\"], _exp[\"dec\"], _exp[\"sigma\"], _exp[\"ra\"])).T\n",
    "mc_sample = np.vstack((_mc[\"logE\"], _mc[\"dec\"], _mc[\"sigma\"], _mc[\"ra\"])).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Production Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test if the modules work correctly.\n",
    "They contain the same code as in the main test notebook, but can be used as classes.\n",
    "This should simplyfy production.\n",
    "\n",
    "Currently each submodul only does a very special task:\n",
    "\n",
    "- `bg_injector`: Samples (\"injects\") backgorund events for trials\n",
    "- `bg_rate_injector`: Samples (\"injects\") the number of BG events to be injected per trial.\n",
    "- `rate_function`: Describes the time depence of the background rate.\n",
    "- `llh`: Implements the likelihood function and signal and background PDFs.\n",
    "- `signal_injector`: Same as `bg_injector` but injecting signal evts from MC.\n",
    "- `analysis`: Main module pulling it all together, making trials, fitting llhs, provides methods for advanced tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## BG Injector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Injects information for background-like events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Setup for tests in this chapter\n",
    "n_samples = int(1e5)\n",
    "rnd_seed = 7353\n",
    "\n",
    "X_names = data_inj._X_names + [\"ra\"]\n",
    "xlabel = [\"logE\", \"dec\", \"logE\", \"dec\"]\n",
    "ylabel = [\"dec\", \"sigma\", \"sigma\", \"ra\"]\n",
    "\n",
    "axes = [[0, 1], [1, 2], [0,2], [1, 3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Data Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_inj = BGInj.DataBGInjector()\n",
    "data_inj.fit(_exp)\n",
    "data_sam = data_inj.sample(n_samples, np.random.RandomState(seed=rnd_seed))\n",
    "\n",
    "# shape (n_samples, n_features) for plotting\n",
    "_d_sam = np.vstack((data_sam[n] for n in X_names)).T\n",
    "for i, axis in enumerate(axes):\n",
    "    fig, (al, ar) = hlp.hist_comp(sample[:, axis], _d_sam[:, axis])\n",
    "    al.set_xlabel(xlabel[i])\n",
    "    ar.set_xlabel(xlabel[i])\n",
    "    al.set_ylabel(ylabel[i])\n",
    "    ar.set_ylabel(ylabel[i])\n",
    "    al.set_title(\"Data\")\n",
    "    ar.set_title(\"Data sample: {} evts from original Data\".format(\n",
    "        len(_d_sam)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Adaptive Width KDE sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Assign model from CV, which has already evaluated adaptive kernels.\n",
    "# Otherwise we would have to reevaluate which takes a long time.\n",
    "# This should be an official option, to set KDE values for datasets.\n",
    "with open(\"data/awKDE_CV/CV10_glob_bw_alpha_EXP_IC86I_CUT_sig.ll.20_\" +\n",
    "          \"PARS_diag_True_pass2.pickle\", \"rb\") as f:\n",
    "    model_selector = pickle.load(f)\n",
    "    print(model_selector.best_params_)\n",
    "\n",
    "kde_inj = BGInj.KDEBGInjector()\n",
    "kde_inj.kde_model = model_selector.best_estimator_\n",
    "\n",
    "# We could still change the alpha, but the global bandwidth must stay fixed\n",
    "# kde_inj.kde_model.alpha = 0.3\n",
    "\n",
    "# Fit doesn't take long because all adaptive kernels are set.\n",
    "# Note: The original order cannot be changed now [logE, dec, sigma]\n",
    "bounds = np.array([[None, None], [-np.pi / 2. , np.pi / 2.], [0, None]])\n",
    "kde_inj.fit(_exp, bounds)\n",
    "\n",
    "# Sample (bounds are preventing spillover in undefined regions)\n",
    "kde_sam = kde_inj.sample(n_samples, np.random.RandomState(seed=rnd_seed))\n",
    "_kde_sam = np.vstack((kde_sam[n] for n in X_names)).T\n",
    "\n",
    "for i, axis in enumerate(axes):\n",
    "    fig, (al, ar) = hlp.hist_comp(sample[:, axis], _kde_sam[:, axis])\n",
    "    al.set_xlabel(xlabel[i])\n",
    "    ar.set_xlabel(xlabel[i])\n",
    "    al.set_ylabel(ylabel[i])\n",
    "    ar.set_ylabel(ylabel[i])\n",
    "    al.set_title(\"Data\")\n",
    "    ar.set_title(\"KDE sample: {} evts\".format(len(_kde_sam)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### GRBLLH style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# If False, only sample where data was\n",
    "# If True sample in global min/max bounding box\n",
    "minmax = True\n",
    "\n",
    "mrinj = BGInj.MRichmanBGInjector()\n",
    "ax0_bins, ax1_bins, ax2_bins = mrinj.fit(_exp, nbins=10, minmax=minmax)\n",
    "mr_sam = mrinj.sample(n_samples=n_samples,\n",
    "                      random_state=np.random.RandomState(seed=rnd_seed))\n",
    "_mr_sam = np.vstack((mr_sam[n] for n in X_names)).T\n",
    "\n",
    "for i, axis in enumerate(axes):\n",
    "    fig, (al, ar) = hlp.hist_comp(sample[:, axis], _mr_sam[:, axis])\n",
    "    al.set_xlabel(xlabel[i])\n",
    "    ar.set_xlabel(xlabel[i])\n",
    "    al.set_ylabel(ylabel[i])\n",
    "    ar.set_ylabel(ylabel[i])\n",
    "    al.set_title(\"Data\")\n",
    "    ar.set_title(\"Pseudo MR sample: {} evts\".format(len(_mr_sam)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Pseudo Data (uniform) sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "uni_inj = BGInj.UniformBGInjector()\n",
    "uni_sam = uni_inj.sample(n_samples)\n",
    "_uni_sam = np.vstack([uni_sam[n] for n in X_names]).T\n",
    "\n",
    "for i, axis in enumerate(axes):\n",
    "    fig, (al, ar) = hlp.hist_comp(sample[:, axis], _uni_sam[:, axis])\n",
    "    al.set_xlabel(xlabel[i])\n",
    "    ar.set_xlabel(xlabel[i])\n",
    "    al.set_ylabel(ylabel[i])\n",
    "    ar.set_ylabel(ylabel[i])\n",
    "    al.set_title(\"Data\")\n",
    "    ar.set_title(\"Pseudo (uniform) sample: {} evts\".format(len(_uni_sam)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## BG Rate Injector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This module injects times of background like events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Injector created from runlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "First step is always to fit a RateFunction to rates from detector runs.\n",
    "Here we use a Sinus1yrRateFunction with fixed period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# First create a rate function. We fix the period to 1 year here\n",
    "rate_func = RateFunc.Sinus1yrRateFunction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Now parse the rundict and make the fitted injector from that\n",
    "def filter_runs(run):\n",
    "    \"\"\"\n",
    "    Filter runs as stated in jfeintzig's doc.\n",
    "    \"\"\"\n",
    "    exclude_runs = [120028, 120029, 120030, 120087, 120156, 120157]\n",
    "    if ((run[\"good_i3\"] == True) & (run[\"good_it\"] == True) &\n",
    "        (run[\"run\"] not in exclude_runs)):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "# Let's create an injector using a goodrun list. This creates a run dict\n",
    "runlist=\"data/runlists/ic86-i-goodrunlist.json\"\n",
    "runlist_inj = BGRateInj.RunlistBGRateInjector(runlist, filter_runs, rate_func)\n",
    "\n",
    "# Fit function to exp times to runlist bins\n",
    "times = exp[\"timeMJD\"]\n",
    "rate_func = runlist_inj.fit(T=times, x0=None, remove_zero_runs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Rebin (donglians proposal)\n",
    "rates = runlist_inj.rate_rec\n",
    "start_mjd = rates[\"start_mjd\"]\n",
    "stop_mjd = rates[\"stop_mjd\"]\n",
    "\n",
    "tmin, tmax = np.amin(start_mjd), np.amax(stop_mjd)\n",
    "ntbins = 12\n",
    "tbins = np.linspace(tmin, tmax, ntbins + 1)\n",
    "\n",
    "# Get bin idx in which the runs fall\n",
    "# This is not a 100% correct, because runs may be right over bin edges\n",
    "idx = np.digitize(stop_mjd, tbins) - 1\n",
    "rates_per_bin = np.zeros(ntbins, dtype=np.float)\n",
    "\n",
    "evts_in_run = rates[\"nevts\"]\n",
    "dts = (stop_mjd - start_mjd) * secinday\n",
    "for i in range(ntbins):\n",
    "    rates_per_bin[i] = np.sum(evts_in_run[idx == i]) / np.sum(dts[idx == i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Plot runs (zorder, because errorbar seems to have high zorder for centers)\n",
    "xerr = 0.5 * (stop_mjd - start_mjd)\n",
    "yerr = rates[\"rate_std\"]\n",
    "binmids = 0.5 * (stop_mjd + start_mjd)\n",
    "\n",
    "plt.errorbar(binmids, rates[\"rate\"], xerr=xerr, yerr=yerr,\n",
    "             fmt=\",\", alpha=0.25, zorder=0)\n",
    "plt.ylim(0, None);\n",
    "\n",
    "# Plot fit\n",
    "t = np.linspace(start_mjd[0], stop_mjd[-1], 1000)\n",
    "y = rate_func(t)\n",
    "plt.plot(t, y, zorder=5, lw=2, color=\"C1\")\n",
    "\n",
    "# Plot y shift dashed to see baseline or years average\n",
    "avg = runlist_inj.best_pars[2]\n",
    "plt.axhline(avg, 0, 1, color=\"C1\", ls=\"--\", label=\"\", lw=1.5)\n",
    "\n",
    "plt.xlim(start_mjd[0], stop_mjd[-1])\n",
    "plt.xlabel(\"MJD\")\n",
    "plt.ylabel(\"Rate in Hz\")\n",
    "\n",
    "# Show rebinned (as expected you see nothing new)\n",
    "m = get_binmids([tbins])[0]\n",
    "plt.errorbar(m, rates_per_bin, xerr=np.diff(tbins),\n",
    "             fmt=\",\", lw=2, color=\"C2\", zorder=3)\n",
    "\n",
    "# plt.savefig(\"./data/figs/time_rate_sinus_rebinned.png\", dpi=200)\n",
    "plt.ylim(0, 0.009)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Best fit params:\")\n",
    "for par, name in zip(runlist_inj.best_pars, [\"amp\", \"toff\", \"base\"]):\n",
    "    print(\" {:5} : {:+.3g}\".format(name, par))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Sample some trials for a single src and time with the poisson=True keyword to see if we sample correctly for each trial.\n",
    "\n",
    "Also compare with poisson=False to see if it's working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rndgen = np.random.RandomState(7353)\n",
    "\n",
    "rates = runlist_inj.rate_rec\n",
    "start_mjd = rates[\"start_mjd\"]\n",
    "\n",
    "# Pick some random time and time frame\n",
    "t = np.random.choice(start_mjd, size=1)\n",
    "trange = np.array([-120, 220])\n",
    "\n",
    "# This is a list of times per trial\n",
    "ntrials = int(1e4)\n",
    "trials = []\n",
    "for i in range(ntrials):\n",
    "    trial = runlist_inj.sample(t, trange, poisson=True, random_state=rndgen)\n",
    "    # Make one array of times, because we have only one src here\n",
    "    trials.append(flatten_list_of_1darrays(trial))\n",
    "\n",
    "nevents = np.array(list(map(len, trials)))\n",
    "print(\"Sampled total of {:d} events in {:d} trials.\".format(\n",
    "        np.sum(nevents), ntrials))\n",
    "\n",
    "# Plot poisson distribution of nevents with expectation from integral\n",
    "expect = runlist_inj.best_estimator_integral(t, trange)\n",
    "_ = plt.hist(nevents, bins=np.arange(10), normed=True)\n",
    "plt.axvline(expect, 0, 1, color=\"C1\", ls=\"--\", lw=2, label=\"expect\")\n",
    "x = np.arange(0, 10)\n",
    "y = scs.poisson.pmf(x, mu=expect)\n",
    "_ = plt.plot(x, y, \"C1\", lw=2, drawstyle=\"steps-post\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Now the same for possion=False as a crosscheck\n",
    "trials = []\n",
    "for i in range(ntrials):\n",
    "    trial = runlist_inj.sample(t, trange, poisson=False, random_state=rndgen)\n",
    "    # Make one array of times, because we have only one src here\n",
    "    trials.append(flatten_list_of_1darrays(trial))\n",
    "\n",
    "nevents = np.array(list(map(len, trials)))\n",
    "print(\"Sampled total of {:d} events in {:d} trials.\".format(\n",
    "        np.sum(nevents), ntrials))\n",
    "\n",
    "# Plot poisson distribution of nevents with expectation from integral, here\n",
    "# for comparison to the previous case only\n",
    "expect = runlist_inj.best_estimator_integral(t, trange)\n",
    "_ = plt.hist(nevents, bins=np.arange(10), normed=True)\n",
    "plt.axvline(expect, 0, 1, color=\"C1\", ls=\"--\", lw=2, alpha=0.5,\n",
    "            label=\"expect\")\n",
    "plt.axvline(np.round(expect), 0, 1, color=\"C1\", ls=\"--\", lw=2,\n",
    "            label=\"round expect\")\n",
    "x = np.arange(0, 10)\n",
    "y = scs.poisson.pmf(x, mu=expect)\n",
    "_ = plt.plot(x, y, \"C1\", lw=2, drawstyle=\"steps-post\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now we do the same, but with multiple sources.\n",
    "Each src gets a larger time window, so the expectation gets higher and we can compare different poisson distributions at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rndgen = np.random.RandomState(7353)\n",
    "\n",
    "rates = runlist_inj.rate_rec\n",
    "start_mjd = rates[\"start_mjd\"]\n",
    "\n",
    "# Pick random times and make increasing time frames per source\n",
    "nsrcs = 3\n",
    "t = np.random.choice(start_mjd, size=nsrcs)\n",
    "trange = np.vstack((np.repeat([-100], nsrcs),\n",
    "                    500 * np.arange(1, 3 * nsrcs + 1, 3))).T\n",
    "\n",
    "# This is a list of times per trial\n",
    "ntrials = int(1e4)\n",
    "trials = []\n",
    "for i in range(ntrials):\n",
    "    trial = runlist_inj.sample(t, trange, poisson=True, random_state=rndgen)\n",
    "    # Make one array of times, because we have only one src here\n",
    "    trials.append(trial)\n",
    "\n",
    "# The format of `trials` is list(array_src1, array_src2, ...) for each trial.\n",
    "# We want the number of events sampled per src per trial\n",
    "nevents = []\n",
    "for i in range(nsrcs):\n",
    "    nevents.append([len(trial[i]) for trial in trials])\n",
    "    print(\"Sampled {:d} events in {:d} trials for src {:d}.\".format(\n",
    "          np.sum(nevents[i]), ntrials, i))\n",
    "\n",
    "# Plot poisson distributions of nevents with expectations from integrals\n",
    "expect = runlist_inj.best_estimator_integral(t, trange)\n",
    "colors = [\"C0\", \"C1\", \"C3\"]\n",
    "for i in range(nsrcs):\n",
    "    _ = plt.hist(nevents[i], bins=np.arange(np.amax(nevents)), normed=True,\n",
    "                 color=colors[i], alpha=.25)\n",
    "    plt.axvline(expect[i], 0, 1, ls=\"--\", lw=2, label=\"mu src {}\".format(i),\n",
    "                color=colors[i])\n",
    "    x = np.arange(0, np.amax(nevents))\n",
    "    y = scs.poisson.pmf(x, mu=expect[i])\n",
    "    _ = plt.plot(x, y, lw=2, drawstyle=\"steps-post\", color=colors[i])\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now we want to look at the actual sampled times in each trial.\n",
    "First we sample a single in a small timeframe.\n",
    "It should be approximately uniformly distributed, respectively not to distinguish by eye from a constant PDF, because the sine is way to broad to be resolved on scuh a small time scale.\n",
    "We also show the bg and signal pdf for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rndgen = np.random.RandomState(7353)\n",
    "\n",
    "# First the small time frame\n",
    "# Arbitrary start date from data\n",
    "nsrcs = 1\n",
    "t0 = np.random.choice(start_mjd, size=nsrcs)\n",
    "t0_sec = t0 * secinday\n",
    "\n",
    "# dt from t0 in seconds, clip at 4 sigma\n",
    "dt = 200\n",
    "nsig = 4.\n",
    "\n",
    "# Make t values for plotting in MJD around t0\n",
    "clip = np.clip(dt, 2, 30) * nsig\n",
    "trange = np.array([-clip, dt + clip]).reshape(nsrcs, 2)\n",
    "ntrials = int(1e4)\n",
    "\n",
    "# Sample times for each trial and flatten to single array with all trials\n",
    "trials = []\n",
    "for i in range(ntrials):\n",
    "    trials += runlist_inj.sample(t0, trange, poisson=True,\n",
    "                                 random_state=rndgen)\n",
    "trials = flatten_list_of_1darrays(trials)\n",
    "\n",
    "# Plot them in together with the PDFs\n",
    "def time_bg_pdf(t, t0, a, b):\n",
    "    # Normalize relative to t0 in seconds (first multiply avoids rounding?)\n",
    "    _t = t * secinday - t0 * secinday\n",
    "  \n",
    "    pdf = np.zeros_like(_t, dtype=np.float)\n",
    "    uni = (_t >= a) & (_t <= b)\n",
    "    pdf[uni] = 1. / (b - a)\n",
    "    return pdf\n",
    "\n",
    "def time_sig_pdf(t, t0, dt, nsig=4):\n",
    "    if dt < 0:\n",
    "        raise ValueError(\"dt must not be negative.\")\n",
    "\n",
    "    # Normalize relative to t0 in seconds (first multiply avoids rounding?)\n",
    "    _t = t * secinday - t0 * secinday\n",
    "    \n",
    "    # Constrain sig_t to [2, 30]s regardless of uniform time window\n",
    "    sig_t = np.clip(dt, 2, 30)\n",
    "    sig_t_clip = nsig * sig_t\n",
    "    gaus_norm = (np.sqrt(2 * np.pi) * sig_t)\n",
    "    \n",
    "    # Split in def regions gaus rising, uniform, gaus falling and zero\n",
    "    gr = (_t < 0) & (_t >= -sig_t_clip)\n",
    "    gf = (_t > dt) & (_t <= dt + sig_t_clip)\n",
    "    uni = (_t >= 0) & (_t <= dt)\n",
    "    \n",
    "    pdf = np.zeros_like(t, dtype=np.float)\n",
    "    pdf[gr] = scs.norm.pdf(_t[gr], loc=0, scale=sig_t)\n",
    "    pdf[gf] = scs.norm.pdf(_t[gf], loc=dt, scale=sig_t)\n",
    "    # Connect smoothly with the gaussians\n",
    "    pdf[uni] = 1. / gaus_norm\n",
    "    \n",
    "    # Normalize whole distribtuion\n",
    "    dcdf = (scs.norm.cdf(dt + sig_t_clip, loc=dt, scale=sig_t) -\n",
    "            scs.norm.cdf(-sig_t_clip, loc=0., scale=sig_t))\n",
    "    norm = dcdf + dt / gaus_norm\n",
    "    \n",
    "    return pdf / norm\n",
    "\n",
    "\n",
    "# Plot the pdfs\n",
    "t = np.linspace(t0_sec + trange[:, 0], t0_sec + trange[:, 1], 200) / secinday\n",
    "bg_pdf = time_bg_pdf(t, t0, -clip, dt + clip)\n",
    "sig_pdf = time_sig_pdf(t, t0, dt, nsig)\n",
    "\n",
    "# Plot in normalized time\n",
    "_t = t * secinday - t0 * secinday\n",
    "plt.plot(_t, bg_pdf, \"C0-\")\n",
    "plt.plot(_t, sig_pdf, \"C1-\")\n",
    "plt.axvline(dt, 0, 1, color=\"C3\", ls=\"--\")\n",
    "plt.axvline(0, 0, 1, color=\"C2\", ls=\"--\")\n",
    "\n",
    "# Plot injected events from all trials, relative times\n",
    "times = (trials - t0) * secinday\n",
    "_ = plt.hist(times, bins=50, normed=True, color=dg, alpha=.25)\n",
    "\n",
    "plt.xlabel(\"Time relative to t0 in sec\")\n",
    "plt.ylim(0, None);\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig(\"./data/figs/bg_events_time_sampled_narrow.png\", dpi=200)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rndgen = np.random.RandomState(7353)\n",
    "\n",
    "# Now the really large time frame, over the whole time range\n",
    "t0 = start_mjd[0]\n",
    "t0_sec = t0 * secinday\n",
    "\n",
    "# Maximum dt over all runs\n",
    "dt = (stop_mjd[-1] - start_mjd[0]) * secinday\n",
    "nsig = 4.\n",
    "\n",
    "# Make t values for plotting in MJD around t0\n",
    "clip = np.clip(dt, 2, 30) * nsig\n",
    "trange = [-clip, dt + clip]\n",
    "ntrials = 100  # More trials mean smaller errors, better see the sinus shape \n",
    "\n",
    "# Sample times\n",
    "trials = []\n",
    "for i in range(ntrials):\n",
    "    trials += runlist_inj.sample(t0, trange, poisson=True,\n",
    "                                 random_state=rndgen)\n",
    "trials = flatten_list_of_1darrays(trials)\n",
    "\n",
    "# We choose the same style as in the intial rate plot further above\n",
    "h, b = np.histogram(trials, bins=1081)\n",
    "m = get_binmids([b])[0]\n",
    "scale = np.diff(b) * secinday * ntrials\n",
    "yerr = np.sqrt(h) / scale\n",
    "h = h / scale\n",
    "\n",
    "plt.errorbar(m, h, yerr=yerr, fmt=\",\")\n",
    "\n",
    "# Plot normalized rate function to compare\n",
    "t = np.linspace(start_mjd[0], stop_mjd[-1], 100)\n",
    "r = runlist_inj.best_estimator(t)\n",
    "plt.plot(t, r, lw=2, zorder=5)\n",
    "plt.axhline(runlist_inj.best_pars[2], 0, 1, color=\"C1\",\n",
    "            ls=\"--\", label=\"\", zorder=5)\n",
    "\n",
    "plt.xlim(start_mjd[0], stop_mjd[-1])\n",
    "plt.ylim(0.004, 0.006)\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig(\"./data/figs/bg_events_time_sampled_wide.png\", dpi=200)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Utils - rejection_sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Test the utils.py rejection sampler.\n",
    "We generate trials for multiple sources at once and check how fast this is.\n",
    "Currently the method just loops over sources, rejection sampling for each interval, but it seems fast enough.\n",
    "\n",
    "A short note on the test below:\n",
    "We make nsrcs, each with ordered center times and increasing time windows.\n",
    "Then we sample an increasing number of samples per source.\n",
    "This is the same as a single trial.\n",
    "\n",
    "In the histogram we expect 2 things:\n",
    "\n",
    "1. If the time windows are smaller than 1 day, which is the bin size, then we just get a nice lineraly increaing bin content (triangle shaped, with hard cut at the right edge).\n",
    "2. If the time windows increase, we get spillover resulting in a way more spread distribution. Also the time windows are only widened to to the right, so the spillover occurs only to the right edges. When the time windows are really large, we even begin to see the underlying oscillation of the sinusodial test function we generate samples from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def sample_test_sin(t):\n",
    "    \"\"\"Simple sinus, similar to fitted rate function\"\"\"\n",
    "    return 0.001 * np.sin(2 * np.pi / 365. * (t - 50000)) + 0.005\n",
    "\n",
    "# Make some srcs and incresing time windows\n",
    "nsrcs = 100\n",
    "t = np.arange(0, nsrcs) + 50000\n",
    "scaler = 2 * secinday  # Time window scaler: Increase to see spillover\n",
    "dts = np.vstack((np.zeros(nsrcs), scaler * np.arange(1, nsrcs + 1))).T\n",
    "dts = t.reshape(nsrcs, 1) + dts / secinday\n",
    "\n",
    "# Sample increasing number of events in time windows\n",
    "n_samples = 100 * np.arange(1, nsrcs + 1)\n",
    "sample = rejection_sampling(sample_test_sin, dts,\n",
    "                            n_samples=n_samples, random_state=3537)\n",
    "\n",
    "flatsam = flatten_list_of_1darrays(sample)\n",
    "\n",
    "# If the time windows are larger than one day (the binning) we get spillover\n",
    "_ = plt.hist(flatsam, bins=nsrcs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Let's look at the distribution in the largest time window.\n",
    "# It should be sinusodial\n",
    "_ = plt.hist(sample[-1], bins=20, normed=True)\n",
    "# Plot sampled function as comparison\n",
    "t = np.linspace(dts[-1, 0],dts[-1, 1], 100)\n",
    "intgrl = scint.quad(sample_test_sin, dts[-1, 0],dts[-1, 1])[0]\n",
    "y = sample_test_sin(t) / intgrl\n",
    "plt.plot(t, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# Quickly check how fast we are. Set nsrcs to 100 above for many srcs\n",
    "rejection_sampling(sample_test_sin, dts, n_samples=n_samples,\n",
    "                   random_state=3537)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## BG Rate Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Test if fit, sample and integral works, with a simple example.\n",
    "First for only a single source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### SinusRateFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define parameters for the test function\n",
    "period_days = 300.\n",
    "b = 2 * np.pi / period_days  # Period in 1/MJD\n",
    "c = 0  # t-Offset in MJD\n",
    "d = 1  # Rate offset in Hz = 1 evt/sec is average -> 86400 evts/day\n",
    "a = d / 2.  # Amplitude in Hz = +- 0.5 evts / per second\n",
    "pars = np.array([a, b, c, d])\n",
    "\n",
    "sinfun = RateFunc.SinusRateFunction()\n",
    "\n",
    "# Plot function\n",
    "t0, t1 = c, c + period_days\n",
    "t = np.linspace(0, t1, 200)  # In MJD days\n",
    "y = sinfun.fun(t, pars)\n",
    "\n",
    "_ = plt.plot(t, y, lw=2, label=\"fun\")\n",
    "\n",
    "# Plot integral\n",
    "intgrl = np.zeros_like(t)\n",
    "for i, ti in enumerate(t):\n",
    "    intgrl[i] = sinfun.integral(t=t0, trange=[t0, ti*secinday], pars=pars)\n",
    "    \n",
    "# Scale integral, we expect 24*3600=86400 evts/day * (period_days days)\n",
    "print(\"Expect   : \", secinday * t1)\n",
    "print(\"Integral : \", intgrl[-1])\n",
    "_ = plt.plot(t, intgrl / 1e7, lw=2, label=\"integral/1e7\")\n",
    "\n",
    "# Sample from whole range and scale normed hist with time scale to match rate\n",
    "nsam = [int(1e4),]\n",
    "trange = np.array([[t0, t1],]) * secinday\n",
    "sam = sinfun.sample(t=t0, trange=trange,\n",
    "                    pars=pars, n_samples=nsam)\n",
    "h, b = np.histogram(sam, range=[t0, t1], bins=50, density=True)\n",
    "m = get_binmids([b])[0]\n",
    "_ = plt.hist(m, bins=b, weights=h * (t1 - t0), color=\"C0\",\n",
    "             alpha=0.5, label=\"sampled\")\n",
    "\n",
    "# Finally fit the sampled points again\n",
    "runtime = (t1 - t0)\n",
    "p0 = None  # Test default args\n",
    "bf_pars = sinfun.fit(t=m, rate=h * (t1 - t0), rate_std=None, p0=p0)\n",
    "yfit = sinfun.fun(t, bf_pars)\n",
    "_ = plt.plot(t, yfit, lw=2, color=\"C3\", ls=\"--\", label=\"fitted\")\n",
    "p0 = sinfun._get_default_seed(t=m, rate=h * (t1 - t0),\n",
    "                              rate_std=np.ones_like(m))\n",
    "yseed = sinfun.fun(t, p0)\n",
    "_ = plt.plot(t, yseed, lw=2, color=\"C3\", ls=\"-\", alpha=0.3,\n",
    "             label=\"default seed\")\n",
    "\n",
    "plt.xlabel(\"time in MJD\")\n",
    "plt.ylabel(\"rate in Hz\")\n",
    "_ = plt.ylim(0, None)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig(\"data/figs/rate_function_test.png\", dpi=200)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Sinus1yrRateFunction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The same as above, but now with fixed period of 1 year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Define parameters for the test function\n",
    "period_days = 365.25\n",
    "c = 0  # t-Offset in MJD\n",
    "d = 1  # Rate offset in Hz = 1 evt/sec is average -> 86400 evts/day\n",
    "a = d / 2.  # Amplitude in Hz = +- 0.5 evts / per second\n",
    "pars = np.array([a, c, d])\n",
    "\n",
    "sinfun = RateFunc.Sinus1yrRateFunction()\n",
    "\n",
    "# Plot function\n",
    "t0, t1 = c, c + period_days\n",
    "t = np.linspace(0, t1, 200)  # In MJD days\n",
    "y = sinfun.fun(t, pars)\n",
    "\n",
    "_ = plt.plot(t, y, lw=2, label=\"fun\")\n",
    "\n",
    "# Plot integral\n",
    "intgrl = np.zeros_like(t)\n",
    "for i, ti in enumerate(t):\n",
    "    intgrl[i] = sinfun.integral(t=t0, trange=[t0, ti*secinday], pars=pars)\n",
    "    \n",
    "# Scale integral, we expect 24*3600=86400 evts/day * (period_days days)\n",
    "print(\"Expect   : \", secinday * t1)\n",
    "print(\"Integral : \", intgrl[-1])\n",
    "_ = plt.plot(t, intgrl / 1e7, lw=2, label=\"integral/1e7\")\n",
    "\n",
    "# Sample from whole range and scale normed hist with time scale to match rate\n",
    "nsam = int(1e4)\n",
    "sam = sinfun.sample(t=t0, trange=[t0, t1*secinday], pars=pars, n_samples=nsam)\n",
    "h, b = np.histogram(sam, range=[t0, t1], bins=50, density=True)\n",
    "m = get_binmids([b])[0]\n",
    "_ = plt.hist(m, bins=b, weights=h * (t1 - t0), color=\"C0\",\n",
    "             alpha=0.5, label=\"sampled\")\n",
    "\n",
    "# Finally fit the sampled points again\n",
    "runtime = (t1 - t0)\n",
    "p0 = None  # Test default args\n",
    "bf_pars = sinfun.fit(t=m, rate=h * (t1 - t0), rate_std=None, p0=p0)\n",
    "yfit = sinfun.fun(t, bf_pars)\n",
    "_ = plt.plot(t, yfit, lw=2, color=\"C3\", ls=\"--\", label=\"fitted\")\n",
    "p0 = sinfun._get_default_seed(t=m, rate=h * (t1 - t0),\n",
    "                              rate_std=np.ones_like(m))\n",
    "yseed = sinfun.fun(t, p0)\n",
    "_ = plt.plot(t, yseed, lw=2, color=\"C3\", ls=\"-\", alpha=0.3,\n",
    "             label=\"default seed\")\n",
    "\n",
    "plt.xlabel(\"time in MJD\")\n",
    "plt.ylabel(\"rate in Hz\")\n",
    "_ = plt.ylim(0, None)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig(\"data/figs/rate_function_test.png\", dpi=200)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### ConstantRateFunction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Use constant rate function but leave the sinus to see how the fit behaves.\n",
    "Otherwise it would be boring to just see 3 flat lines over another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define sinus parameters\n",
    "period_days = 365.25\n",
    "c = 0  # t-Offset in MJD\n",
    "d = 1  # Rate offset in Hz = 1 evt/sec is average -> 86400 evts/day\n",
    "a = d / 2.  # Amplitude in Hz = +- 0.5 evts / per second\n",
    "sinpars = np.array([a, c, d])\n",
    "\n",
    "# Same for the constant function.\n",
    "constpars = (d,)\n",
    "\n",
    "sinfun = RateFunc.Sinus1yrRateFunction()\n",
    "constfun = RateFunc.ConstantRateFunction()\n",
    "\n",
    "# Plot sinus and constant function\n",
    "t0, t1 = c, c + period_days\n",
    "t = np.linspace(0, t1, 200)  # In MJD days\n",
    "y = sinfun.fun(t, sinpars)\n",
    "yc = constfun.fun(t, constpars)\n",
    "\n",
    "_ = plt.plot(t, y, color=\"C0\", lw=2, ls=\"--\")\n",
    "_ = plt.plot(t, yc, lw=2, label=\"fun\")\n",
    "\n",
    "# Plot integral\n",
    "intgrl = np.zeros_like(t)\n",
    "for i, ti in enumerate(t):\n",
    "    intgrl[i] = constfun.integral(t=t0, trange=[t0, ti*secinday],\n",
    "                                  pars=constpars)\n",
    "    \n",
    "# Scale integral, we expect 24*3600=86400 evts/day * (period_days days)\n",
    "print(\"Expect   : \", secinday * t1)\n",
    "print(\"Integral : \", intgrl[-1])\n",
    "_ = plt.plot(t, intgrl / 1e7, lw=2, label=\"integral/1e7\")\n",
    "\n",
    "# Sample from whole range and scale normed hist with time scale to match rate\n",
    "nsam = int(1e4)\n",
    "sam = sinfun.sample(t=t0, trange=[t0, t1*secinday], pars=sinpars,\n",
    "                      n_samples=nsam)\n",
    "h, b = np.histogram(sam, range=[t0, t1], bins=50, density=True)\n",
    "m = get_binmids([b])[0]\n",
    "_ = plt.hist(m, bins=b, weights=h * (t1 - t0), color=\"C0\",\n",
    "             alpha=0.5, label=\"sampled\")\n",
    "\n",
    "# Finally fit the sampled points again\n",
    "runtime = (t1 - t0)\n",
    "p0 = None  # Test default args\n",
    "bf_pars = constfun.fit(t=m, rate=h * (t1 - t0), rate_std=None, p0=p0)\n",
    "yfit = constfun.fun(t, bf_pars)\n",
    "_ = plt.plot(t, yfit, lw=2, color=\"C3\", ls=\"--\", label=\"fitted\")\n",
    "p0 = constfun._get_default_seed(t=m, rate=h * (t1 - t0),\n",
    "                                rate_std=np.ones_like(m))\n",
    "yseed = constfun.fun(t, p0)\n",
    "_ = plt.plot(t, yseed, lw=2, color=\"C3\", ls=\"-\", alpha=0.3,\n",
    "             label=\"default seed\")\n",
    "\n",
    "plt.xlabel(\"time in MJD\")\n",
    "plt.ylabel(\"rate in Hz\")\n",
    "_ = plt.ylim(0, None)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig(\"data/figs/rate_function_test.png\", dpi=200)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## LLH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Test the LLH module.\n",
    "\n",
    "It contains all functions for a specific LLH we want to use in our analysis.\n",
    "Currently GRBLLH is implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "sin_dec_bins = np.linspace(-1, 1, 50)\n",
    "\n",
    "min_logE = 1  #  min(np.amin(_exp[\"logE\"]), np.amin(mc[\"logE\"]))\n",
    "max_logE = 10 #  max(np.amax(_exp[\"logE\"]), np.amax(mc[\"logE\"]))\n",
    "logE_bins = np.linspace(min_logE, max_logE, 40)\n",
    "\n",
    "spatial_pdf_args = {\"bins\": sin_dec_bins, \"k\": 3, \"kent\": True}\n",
    "\n",
    "energy_pdf_args = {\"bins\": [sin_dec_bins, logE_bins],\n",
    "                   \"gamma\": 2., \"fillval\": \"col\", \"interpol_log\": False}\n",
    "\n",
    "time_pdf_args = {\"nsig\": 4., \"sigma_t_min\": 2., \"sigma_t_max\": 30.}\n",
    "\n",
    "grbllh = LLH.GRBLLH(X=_exp, MC=mc, srcs=None,\n",
    "                    spatial_pdf_args=spatial_pdf_args,\n",
    "                    energy_pdf_args=energy_pdf_args,\n",
    "                    time_pdf_args=time_pdf_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Time PDF Ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Reproduce the paper plot.\n",
    "\n",
    "Note that we get the PDFs for all srcs at once.\n",
    "Their times are just all the same here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Make a plot with ratios for different time windows as in the paper\n",
    "# dt from t0 in seconds, clip at 4 sigma\n",
    "dts = [[-1, 5], [-5, 50], [-20, 200]]\n",
    "nsrcs = len(dts)\n",
    "nsig = 4\n",
    "\n",
    "# Arbitrary start date from data\n",
    "t0 = np.repeat(np.random.choice(exp[\"timeMJD\"], size=1),\n",
    "               repeats=nsrcs).reshape(nsrcs, 1)\n",
    "t0_sec = t0[0] * secinday  # Only single number needed, t0s are all equal\n",
    "\n",
    "# Make t values for plotting in MJD around t0, to fit all in one plot\n",
    "max_dt, min_dt = np.amax(dts), np.amin(dts)\n",
    "dt_tot = max_dt - min_dt\n",
    "clip = np.clip(dt_tot, 2, 30) * nsig\n",
    "plt_range = np.array([min_dt - clip, max_dt + clip])\n",
    "t = np.linspace(t0_sec + 1.2 * plt_range[0],\n",
    "                t0_sec + 1.2 * plt_range[1], 1000) / secinday\n",
    "_t = t * secinday - t0 * secinday\n",
    "\n",
    "# Mark event time\n",
    "plt.axvline(0, 0, 1, c=\"k\", ls=\"--\", lw=2, alpha=0.8)\n",
    "\n",
    "# Get all at once\n",
    "SoB = grbllh._soverb_time(t=t, src_t=t0, dt=dts)\n",
    "\n",
    "colors = [\"C0\", \"C3\", \"C2\"]\n",
    "for i in range(len(SoB)):\n",
    "    # Plot seperately to give colors and labels\n",
    "    plt.plot(_t[i], SoB[i], lw=2, c=colors[i],\n",
    "             label=r\"$T_\\mathrm{{uni}}$: {:>3d}s, {:>3d}s\".format(*dts[i]))\n",
    "    # Fill uniform part, might look nicely\n",
    "    # fbtw = (_t > 0) & (_t < dt)\n",
    "    # plt.fill_between(_t[fbtw], 0, SoB[fbtw], color=\"C7\", alpha=0.1)\n",
    "\n",
    "# Plot stacked\n",
    "weights = np.ones(nsrcs).reshape(nsrcs, 1) / nsrcs\n",
    "stacked = np.sum(SoB * weights, axis=0) \n",
    "plt.plot(_t[0], stacked, ls=\"--\", c=\"k\", lw=2, label=\"stacked\", alpha=0.8)\n",
    "    \n",
    "# Make it look like the paper plot, but with slightly extended borders\n",
    "plt.xlim(1.2 * plt_range)\n",
    "plt.ylim(0, np.amax(SoB) * 1.05)\n",
    "plt.xlabel(\"t - t0 in sec\")\n",
    "plt.ylabel(\"S / B\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.grid(ls=\"--\", lw=1)\n",
    "\n",
    "# plt.savefig(\"./data/figs/time_pdf_ratio.png\", dpi=200)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Get the injection time window.\n",
    "This is needed for the injector, so only events in regions with non-zero PDF are injected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "grbllh.get_injection_trange(t0, dts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Compare manually\n",
    "dts = np.array([[-1, 5], [-5, 50], [-20, 200]], dtype=np.float)\n",
    "nsig, sig_min, sig_max = time_pdf_args.values()  # Beware if order is wrong :P\n",
    "clip = np.clip(np.diff(dts, axis=1), sig_min, sig_max) * nsig\n",
    "dts[:, 0] -= clip.reshape(len(dts))  # Same as flatten()\n",
    "dts[:, 1] += clip.flatten()\n",
    "\n",
    "dts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Spatial background spline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This is the same technique as used in skylab, but with an extra step of adding the outermost bin edges to the spline gridpoints.\n",
    "This way, the spline behaves reasonable at the edges and doesn't overshoot.\n",
    "\n",
    "We could extend this by using the KDE integrated over every variable and then fitting a spline to that.\n",
    "Or we could sample from the KDE and bin finely and fit a splien again.\n",
    "\n",
    "For now we leave only the option to use data directly.\n",
    "The spline fit is depending on the binning anyway.\n",
    "Only the finely binned KDE version could resolve that issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sin_dec = np.linspace(-1.05, 1.05, 200)\n",
    "y = np.exp(grbllh._spatial_bg_spl(sin_dec))\n",
    "_ = plt.hist(np.sin(_exp[\"dec\"]), bins=50, normed=True)\n",
    "plt.plot(sin_dec, y, lw=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Spatial background pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Should be identical to calling the spline directly, except that the BG PDF is normalized to the whole sphere.\n",
    "So we multiply the values by 2pi to account for that.\n",
    "\n",
    "Here we see the difference to just calling the spline directly: The PDF is zero outside the definition range, the spline extrapolated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "_ = plt.hist(np.sin(_exp[\"dec\"]), bins=grbllh.spatial_pdf_args[\"bins\"],\n",
    "             normed=True)\n",
    "sin_dec = np.linspace(-1.05, 1.05, 200)\n",
    "y = 2 * np.pi * grbllh._pdf_spatial_background(ev_sin_dec=sin_dec)\n",
    "plt.plot(sin_dec, y, lw=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Spatial signal PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Compare signal and BG pdf.\n",
    "\n",
    "First we create multiple sources and a single event and scan the event PDF by moving the event along the declination axis.\n",
    "All PDFs have the height, because the same sigma is used.\n",
    "\n",
    "Note that BG is here usually very small compared to the signal, because we sample the ev positions within 1 sigma around the source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "LOG = False\n",
    "\n",
    "nsrcs = 4\n",
    "# Choose the event sigma from data\n",
    "ev_sigma = np.random.choice(_exp[\"sigma\"], size=1)\n",
    "\n",
    "# Make nsrcs, same ra, but different dec. decs are distributed uniformly in\n",
    "# the range of the largest sigma from the events (for illustration only)\n",
    "src_dec = np.random.uniform(-ev_sigma, ev_sigma, size=nsrcs)\n",
    "src_ra = np.ones_like(src_dec) * np.pi\n",
    "plt_rnge = [np.amin(src_dec) - ev_sigma, np.amax(src_dec) + ev_sigma]\n",
    "\n",
    "# Scan signal PDF for event declination\n",
    "ev_dec = np.sin(np.linspace(plt_rnge[0], plt_rnge[1], 200))\n",
    "ev_sin_dec = np.sin(ev_dec)\n",
    "ev_ra = src_ra[0] * np.ones_like(ev_sin_dec)\n",
    "ev_sig = np.ones_like(ev_sin_dec) * ev_sigma\n",
    "\n",
    "# y has shape (nsrcs, nevts), where nevts are the ev_sin_dec values here (scan)\n",
    "y = grbllh._pdf_spatial_signal(src_ra, src_dec, ev_ra, ev_sin_dec, ev_sig)\n",
    "\n",
    "if LOG:\n",
    "    y = np.log10(y)\n",
    "\n",
    "plt.plot(ev_dec, y.T, lw=2)\n",
    "plt.vlines(src_dec, 0, np.amax(y), color=\"C7\", linestyles=\"--\", lw=2,\n",
    "           label=\"srcs pos\")\n",
    "\n",
    "# Plot BG PDF to compare\n",
    "bg = grbllh._pdf_spatial_background(ev_sin_dec=ev_sin_dec)\n",
    "plt.plot(ev_dec, bg, lw=2, label=\"BG\")\n",
    "\n",
    "plt.xlim(*plt_rnge)\n",
    "if LOG:\n",
    "    plt.ylim(1e-5, 1.1 * np.amax(y))\n",
    "else:\n",
    "    plt.ylim(0, 1.1 * np.amax(y))\n",
    "    \n",
    "    \n",
    "plt.xlabel(\"dec\")\n",
    "plt.ylabel(\"PDF per src\")\n",
    "plt.legend()\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here we use multiple events with different sigmas and scan again in declination by moving a single possible src position.\n",
    "We get different heights, because of the different sigmas.\n",
    "\n",
    "The PDFs each peak where the event position is.\n",
    "If we had a single source, we would just read off the values at that position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "LOG = True\n",
    "# Make nevts, same ra, but different dec. sigmas chosen from data\n",
    "nevts = 4\n",
    "ev_sigma = np.random.choice(_exp[\"sigma\"], size=nevts)\n",
    "# Sample some evt decs uniformly around the horizon with spread of the largest \n",
    "# sigma to get some variation\n",
    "max_sig = np.amax(ev_sigma)\n",
    "ev_dec = np.random.uniform(-max_sig, max_sig, size=nevts)\n",
    "ev_sin_dec = np.sin(ev_dec)\n",
    "ev_ra = np.ones_like(ev_dec) * np.pi\n",
    "\n",
    "# Plot margin PDF scanned for each src position for each event position\n",
    "src_dec = np.linspace(-2. * max_sig, 2 * max_sig, 200)\n",
    "src_ra = ev_ra[0] * np.ones_like(src_dec)\n",
    "\n",
    "# This has shape (nsrcs, nevts)\n",
    "y = grbllh._pdf_spatial_signal(src_ra, src_dec, ev_ra, ev_sin_dec, ev_sigma)\n",
    "\n",
    "if LOG:\n",
    "    y = np.log10(y)\n",
    "\n",
    "plt.plot(src_dec, y, lw=2)\n",
    "\n",
    "plt.vlines(ev_dec, 0, np.amax(y) * 1.1, color=\"C7\",\n",
    "           linestyles=\"--\", label=\"evts pos\")\n",
    "\n",
    "# Plot BG PDF to compare\n",
    "bg = grbllh._pdf_spatial_background(ev_sin_dec=np.sin(src_dec))\n",
    "plt.plot(src_dec, bg, lw=2, label=\"BG\")\n",
    "\n",
    "plt.xlim(src_dec[[0, -1]])\n",
    "if LOG:\n",
    "    plt.ylim(1e-5, 1.1 * np.amax(y))\n",
    "else:\n",
    "    plt.ylim(0, 1.1 * np.amax(y))\n",
    "    \n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Spatial PDF ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_dec_vs_signal(S, ev_dec, src_ra, src_dec, weights, ax=None):\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots(1, 1)\n",
    "    # Plot signal per source for each event\n",
    "    for i, (sra, sdec) in enumerate(zip(src_ra, src_dec)):\n",
    "        ax.plot(np.rad2deg(ev_dec), S[i], ls=\"-\")\n",
    "        ax.plot(np.rad2deg(sdec), -10, \"k|\")\n",
    "\n",
    "    # Simulate a simple stacking, one weight per source\n",
    "    ax.plot(np.rad2deg(ev_dec), np.sum(weights * S, axis=0) / np.sum(weights),\n",
    "             ls=\"--\", c=dg, label=\"stacked\")\n",
    "\n",
    "    ax.set_xlim([-1 + smin, smax + 1])\n",
    "    ax.set_xlabel(\"DEC in °\")\n",
    "    ax.set_ylabel(\"Signal pdf\")\n",
    "    ax.legend(loc=\"upper right\")\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We make 4 plots to test everything:\n",
    "\n",
    "1. [Top left] We place densely packed srcs at the declination range and scan the PDFs by varying the event declinations.\n",
    "   Sigma is fixed to 1 for illustration.\n",
    "   We expect just a row of gaussians along the dec range.\n",
    "   The stacked signal is the weighted sum of all signal contributions at a single event dec position.\n",
    "   \n",
    "2. [Bottom left] We plot just the background PDF and its inverse for the dec range.\n",
    "   The inverse PDF is what modulates the signal PDF.\n",
    "   \n",
    "3. [Top right] This modulation can be seen in this plot.\n",
    "   It is basically the same as the first one, but now it's signal over background.\n",
    "   So the signal peaks are modulated with the inverse BG PDF.\n",
    "   \n",
    "4. [Bottom right] This is the same plot as the third one, but this time we use the real data declination values instead of nicely spaced ones.\n",
    "   The effect is the same but not reall visible, because each event has a different sigma, so the PDFs all have different heights and widths.\n",
    "   It becomes more similar when using an 1° sigma for all events (just comment that line in)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Make srcs across the dec range. The hull of SoB should be shaped like the\n",
    "# 1/(sinDec BG distribtuion). With a single source we couldn't see that,\n",
    "# because it drops to zero far from the src position\n",
    "smin, smax, step = -90, +90, 10\n",
    "src_ra = np.deg2rad(np.arange(smin, smax + step, step))\n",
    "src_dec = np.deg2rad(np.arange(smin, smax + step, step))\n",
    "\n",
    "# Scan in dec by varying the evts dec\n",
    "ev_ra = np.deg2rad(np.linspace(smin, smax, 1000))\n",
    "ev_dec = np.deg2rad(np.linspace(smin, smax, 1000))\n",
    "ev_sin_dec = np.sin(ev_dec)\n",
    "ev_sig = np.deg2rad(np.ones_like(ev_ra))\n",
    "\n",
    "# Some pseudo weights to simulate stacking\n",
    "weights = np.arange(1, len(src_dec) + 1)[:, np.newaxis]\n",
    "\n",
    "fig, ((axtl, axtr), (axbl, axbr)) = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Signal only\n",
    "S = grbllh._pdf_spatial_signal(src_ra, src_dec, ev_ra, ev_sin_dec, ev_sig)\n",
    "_ = plot_dec_vs_signal(S, ev_dec, src_ra, src_dec, weights, ax=axtl)\n",
    "axtl.set_xlim(-90, 90)\n",
    "\n",
    "# Background only\n",
    "bins = grbllh.spatial_pdf_args[\"bins\"]\n",
    "h, b = np.histogram(np.sin(_exp[\"dec\"]), bins=bins, density=True)\n",
    "m = 0.5 * (b[:-1] + b[1:])\n",
    "_ = axbl.hist(m, bins=bins, weights=h / 2 / np.pi, alpha=0.5)\n",
    "_sin_dec = np.linspace(-1, 1, 1000)\n",
    "bg_pdf = grbllh._pdf_spatial_background(_sin_dec)\n",
    "axbl.plot(_sin_dec, bg_pdf, lw=2, label=\"pdf\")\n",
    "axbl.set_ylim(0, 0.2)\n",
    "# 1 / BG PDF on second axis\n",
    "axbl2 = axbl.twinx()\n",
    "axbl2.plot(_sin_dec, 1. / bg_pdf, c=\"C2\", lw=2, ls=\"--\", label=\"1/pdf\")\n",
    "axbl2.set_ylim(0, (1 / bg_pdf).max())\n",
    "axbl.set_xlabel(\"sinus DEC\")\n",
    "axbl.set_xlim(-1, 1)\n",
    "axbl.legend(loc=\"upper left\")\n",
    "axbl2.legend(loc=\"upper center\")\n",
    "\n",
    "# SoB on example + BG PDF\n",
    "SoB = grbllh._soverb_spatial(src_ra, src_dec, ev_ra, ev_sin_dec, ev_sig)\n",
    "weights = np.arange(1, len(src_dec) + 1)[:, np.newaxis]\n",
    "_ = plot_dec_vs_signal(SoB, ev_dec, src_ra, src_dec, weights, ax=axtr)\n",
    "axtr.plot(np.rad2deg(np.arcsin(_sin_dec)), bg_pdf, lw=3, label=\"BG pdf\", c=dg)\n",
    "axtr.set_xlim(-90, 90)\n",
    "axtr.set_yscale(\"log\")\n",
    "axtr.set_ylim(np.amin(bg_pdf), 1e5)\n",
    "axtr.legend(loc=\"upper left\")\n",
    "\n",
    "# Now with the real data. Sort first in dec to show with nice lines + BG PDF\n",
    "idx = np.argsort(exp[\"dec\"])\n",
    "ev_ra = exp[\"ra\"][idx]\n",
    "ev_dec = exp[\"dec\"][idx]\n",
    "ev_sin_dec = np.sin(ev_dec)\n",
    "ev_sig = exp[\"sigma\"][idx]\n",
    "# Comment in to match the simple example (all events have sigma 1°)\n",
    "# ev_sig = np.deg2rad(np.ones_like(ev_ra))\n",
    "SoB = grbllh._soverb_spatial(src_ra, src_dec, ev_ra, ev_sin_dec, ev_sig)\n",
    "\n",
    "_ = plot_dec_vs_signal(SoB, ev_dec, src_ra, src_dec, weights, ax=axbr)\n",
    "axbr.plot(np.rad2deg(np.arcsin(_sin_dec)), bg_pdf,\n",
    "          lw=3, label=\"BG pdf\", c=\"C0\")\n",
    "axbr.set_yscale(\"log\")\n",
    "axbr.set_ylim(np.amin(bg_pdf), 1e5)\n",
    "axbr.legend(loc=\"upper left\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Energy ratio spline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This is the creation of the signal over background ratio for the energy PDF.\n",
    "It is resolved in sinDec and logE to account for different positions on the sky and energies.\n",
    "\n",
    "Missing values, where no data or MC is present is filled with interpolation values, conttrolled by the \"fillval\" option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, (al, ar) = plt.subplots(1, 2, figsize=(14,5))\n",
    "\n",
    "energy_pdf_args = {\"bins\": [sin_dec_bins, logE_bins],\n",
    "                   \"gamma\": 2., \"fillval\": \"col\", \"interpol_log\": False}\n",
    "grbllh = LLH.GRBLLH(X=_exp, MC=mc, srcs=None,\n",
    "                    spatial_pdf_args=spatial_pdf_args,\n",
    "                    energy_pdf_args=energy_pdf_args,\n",
    "                    time_pdf_args=time_pdf_args)\n",
    "\n",
    "# Ratio spline with 'col' filling\n",
    "x = np.linspace(-1.1, 1.1, num=1000 + 1)\n",
    "y = np.linspace(0.5, 10.5, num=1000 + 1)\n",
    "XX, YY = np.meshgrid(x, y)\n",
    "xx, yy = map(np.ravel, [XX, YY])\n",
    "gpts = np.vstack((xx, yy)).T\n",
    "zz = np.exp(grbllh._energy_spl(gpts))\n",
    "ZZ = zz.reshape(XX.shape)\n",
    "# Plotting with hist creates strange effects... Use pcolormesh instead\n",
    "img = al.pcolormesh(XX, YY, ZZ, norm=LogNorm(), cmap=\"coolwarm\",\n",
    "                    vmin=1e-3, vmax=1e3)\n",
    "al.set_title(\"Spline interpolation: 'col'\")\n",
    "plt.colorbar(ax=al, mappable=img)\n",
    "\n",
    "# With 'minmax' filling. Note: The small values in the lower row are due to\n",
    "# plotting in log. We interpolate in linear space, so in log, the jump is\n",
    "# very steep for small values.\n",
    "energy_pdf_args = {\"bins\": [sin_dec_bins, logE_bins],\n",
    "                   \"gamma\": 2., \"fillval\": \"minmax\", \"interpol_log\": False}\n",
    "grbllh = LLH.GRBLLH(X=_exp, MC=mc, srcs=None,\n",
    "                    spatial_pdf_args=spatial_pdf_args,\n",
    "                    energy_pdf_args=energy_pdf_args,\n",
    "                    time_pdf_args=time_pdf_args)\n",
    "\n",
    "zz = np.exp(grbllh._energy_spl(gpts))\n",
    "ZZ = zz.reshape(XX.shape)\n",
    "img = ar.pcolormesh(XX, YY, ZZ, norm=LogNorm(), cmap=\"coolwarm\",\n",
    "                    vmin=1e-3, vmax=1e3)\n",
    "ar.set_title(\"Spline interpolation: 'minmax'\")\n",
    "plt.colorbar(ax=ar, mappable=img)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Energy PDF ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here we see again the difference to the direct spline evaluation.\n",
    "The ratio function set's values outside to zero probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, (al, ar) = plt.subplots(1, 2, figsize=(14,5))\n",
    "\n",
    "energy_pdf_args = {\"bins\": [sin_dec_bins, logE_bins],\n",
    "                   \"gamma\": 2., \"fillval\": \"col\", \"interpol_log\": True}\n",
    "grbllh = LLH.GRBLLH(X=_exp, MC=mc, srcs=None,\n",
    "                    spatial_pdf_args=spatial_pdf_args,\n",
    "                    energy_pdf_args=energy_pdf_args,\n",
    "                    time_pdf_args=time_pdf_args)\n",
    "\n",
    "# Ratio spline with 'col' filling\n",
    "x = np.linspace(-1.1, 1.1, num=1000 + 1)\n",
    "y = np.linspace(0.5, 10.5, num=1000 + 1)\n",
    "XX, YY = np.meshgrid(x, y)\n",
    "xx, yy = map(np.ravel, [XX, YY])\n",
    "gpts = np.vstack((xx, yy)).T\n",
    "zz = grbllh._soverb_energy(xx, yy)\n",
    "ZZ = zz.reshape(XX.shape)\n",
    "# Plotting with hist creates strange effects... Use pcolormesh instead\n",
    "img = al.pcolormesh(XX, YY, ZZ, norm=LogNorm(), cmap=\"coolwarm\",\n",
    "                    vmin=1e-3, vmax=1e3)\n",
    "al.set_title(\"Spline interpolation: 'col'\")\n",
    "plt.colorbar(ax=al, mappable=img)\n",
    "\n",
    "# With 'minmax' filling. Note: The small values in the lower row are due to\n",
    "# plotting in log. We interpolate in linear space, so in log, the jump is\n",
    "# very steep for small values.\n",
    "energy_pdf_args = {\"bins\": [sin_dec_bins, logE_bins],\n",
    "                   \"gamma\": 2., \"fillval\": \"minmax\", \"interpol_log\": True}\n",
    "grbllh = LLH.GRBLLH(X=_exp, MC=mc, srcs=None,\n",
    "                    spatial_pdf_args=spatial_pdf_args,\n",
    "                    energy_pdf_args=energy_pdf_args,\n",
    "                    time_pdf_args=time_pdf_args)\n",
    "\n",
    "zz = grbllh._soverb_energy(xx, yy)\n",
    "ZZ = zz.reshape(XX.shape)\n",
    "img = ar.pcolormesh(XX, YY, ZZ, norm=LogNorm(), cmap=\"coolwarm\",\n",
    "                    vmin=1e-3, vmax=1e3)\n",
    "ar.set_title(\"Spline interpolation: 'minmax'\")\n",
    "plt.colorbar(ax=ar, mappable=img)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Detector source weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We use the same spline method to create a spline describing the sinDec dependence of a signal MC weighted to a specific astrophysical flux modell (usually unbroken power law).\n",
    "\n",
    "Depending on the src position, we expect more or less signal from that src.\n",
    "This is equivalent to folding with the detector exposure function.\n",
    "\n",
    "Our stacking form is described by a multi position search where the signal term gets modified to:\n",
    "\n",
    "$$\n",
    "    S^\\text{tot} = \\sum_{j=1}^{N_\\text{srcs}} w_j S_{ij} \\quad\\text{with}\\quad\n",
    "    \\sum_j w_j = 1 \\quad\\text{with}\\quad w_j = w_j^\\text{theo}\\cdot w_j^\\text{det}\n",
    "$$\n",
    "\n",
    "The weights are a combination of the exposure weights and a-priori fixed intrinsic source weights, eg. from a known gamma flux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Small hack to change the gamma without recreating the grbllh object\n",
    "gamma_override = 2.13\n",
    "\n",
    "grbllh.energy_pdf_args[\"gamma\"] = gamma_override\n",
    "mc_sin_dec = np.sin(mc[\"dec\"])\n",
    "mc_bins = grbllh.energy_pdf_args[\"bins\"][0]\n",
    "mc_dict = {\"trueE\": mc[\"trueE\"], \"ow\": mc[\"ow\"]}\n",
    "\n",
    "grbllh._spatial_signal_spl = grbllh._create_sin_dec_spline(\n",
    "    sin_dec=mc_sin_dec, bins=mc_bins, mc=mc_dict)\n",
    "\n",
    "sin_dec = np.linspace(-1.05, 1.05, 200)\n",
    "y = np.exp(grbllh._spatial_signal_spl(sin_dec))\n",
    "\n",
    "# MC needs proper weighting\n",
    "gamma = grbllh.energy_pdf_args[\"gamma\"]\n",
    "mc_w = mc[\"ow\"] * mc[\"trueE\"]**(-gamma)\n",
    "mc_bins = energy_pdf_args[\"bins\"][0]\n",
    "h, b = np.histogram(np.sin(mc[\"dec\"]), bins=mc_bins, weights=mc_w, normed=True)\n",
    "\n",
    "# Smooth it, charge it, odd it, quick truncate it\n",
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.savgol_filter.html\n",
    "m = get_binmids([b])[0]\n",
    "redux = 10  # Window len is nearest odd number to (number of bins / redux)\n",
    "window_len = int(2 * np.floor((len(b) / redux) / 2) + 1)  # Must be odd\n",
    "_h = scsignal.savgol_filter(h, window_len, 3, mode=\"mirror\")\n",
    "plt.hist(m, bins=b, weights=_h, histtype=\"step\", lw=2, color=\"C1\", ls=\"--\")\n",
    "plt.hist(m, bins=b, weights=h, histtype=\"step\", lw=2, color=\"C3\")\n",
    "\n",
    "# Plot spline (fitted to unsmoothed)\n",
    "plt.plot(sin_dec, y, lw=2, color=\"C2\")\n",
    "\n",
    "# Get weights for some srcs\n",
    "src_sin_dec = np.linspace(-1, 1, 11)\n",
    "src_dec = np.arcsin(src_sin_dec)\n",
    "src_w_theo = np.ones_like(src_dec)\n",
    "w = grbllh.get_src_weights(src_dec=src_dec, src_w_theo=src_w_theo)\n",
    "\n",
    "# Revoke norm for plotting to see if weights are on the curve\n",
    "src_dec_w = np.exp(grbllh._spatial_signal_spl(src_sin_dec))\n",
    "_w = w * np.sum(src_dec_w * src_w_theo)\n",
    "\n",
    "plt.plot(src_sin_dec, _w, \"wo\", ms=7, mew=1.5, mec=\"k\")\n",
    "plt.vlines(np.sin(src_dec), 0, 1.05 * np.amax(y), colors=\"C7\",\n",
    "           lw=1, linestyles=\"--\")\n",
    "\n",
    "plt.xlabel(\"sin(dec)\")\n",
    "plt.ylabel(\"PDF\")\n",
    "plt.title(\"$\\gamma = {:.1f}$\".format(gamma))\n",
    "\n",
    "plt.xlim(-1, 1)\n",
    "plt.ylim(0, 1.05 * np.amax(y))\n",
    "plt.tight_layout()\n",
    "\n",
    "print(w)\n",
    "print(w.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here we just use some ascending theoretical weights in both directions.\n",
    "\n",
    "- w1 should resemble the sindec curve from above\n",
    "- w2 should rise overall to the right (less steep or reversed from w1)\n",
    "- w3 should fall overall to the right (steeper than w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "_src_dec = np.arcsin(np.linspace(-1, 1, 21))\n",
    "\n",
    "# Compare for different theoretical weights\n",
    "src_w_theo = np.ones_like(_src_dec)\n",
    "w1 = grbllh.get_src_weights(src_dec=_src_dec, src_w_theo=src_w_theo)\n",
    "\n",
    "src_w_theo = np.arange(len(_src_dec)) + 1\n",
    "w2 = grbllh.get_src_weights(src_dec=_src_dec, src_w_theo=src_w_theo)\n",
    "\n",
    "src_w_theo = (np.arange(len(_src_dec)) + 1)[::-1]\n",
    "w3 = grbllh.get_src_weights(src_dec=_src_dec, src_w_theo=src_w_theo)\n",
    "\n",
    "plt.plot(np.sin(_src_dec), w1, \"o\", label=\"theo = 1 (orig)\")\n",
    "plt.plot(np.sin(_src_dec), w2, \"o\", label=\"theo = arange\")\n",
    "plt.plot(np.sin(_src_dec), w3, \"o\", label=\"theo = arange[::-1]\")\n",
    "plt.legend(loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Check if weights are the same when using event density (skylab style) instead.\n",
    "It should make no difference because the weights are normalized anyway.\n",
    "\n",
    "**This will be included, if we get to fitting multiple years.\n",
    "We can then use the same weights for the stacking and for normalizing ns per year.**\n",
    "\n",
    "Note: The normalization and the pivot will not be included in the end, because they are constant for every source and for every dataset/year, so they get normalized out anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Weight with numu diffuse 6yr flux norm, but same index as above\n",
    "index = gamma_override\n",
    "norm = 0.9 * 1e-18  # (GeV s sr cm^2)^-1, valid at 100 TeV = 1e5 GeV\n",
    "pivot = 1e5\n",
    "flux = norm * (mc[\"trueE\"] / pivot)**(-index)\n",
    "mc_w = mc[\"ow\"] * flux * livetime * secinday\n",
    "mc_bins = energy_pdf_args[\"bins\"][0]\n",
    "\n",
    "density = True\n",
    "h, b = np.histogram(np.sin(mc[\"dec\"]), bins=mc_bins, weights=mc_w,\n",
    "                    density=density)\n",
    "\n",
    "# Normalize (same as density=True)\n",
    "if not density:\n",
    "    h /= np.diff(b) * np.sum(h)\n",
    "\n",
    "# PDF * Number of total events = Event densitiy\n",
    "_h = h * mc_w.sum()  \n",
    "\n",
    "mids = get_binmids([mc_bins])[0]\n",
    "_ = plt.hist(mids, bins=mc_bins, weights=_h)\n",
    "\n",
    "plt.xlabel(\"sindec\")\n",
    "plt.ylabel(\"Event density Nevts / sindec\")\n",
    "\n",
    "# Total events by integrating _h: Ntot = sum_i (_h_i * diff(bins)_i)\n",
    "plt.title(\"Gamma = {:.2f}: Ntot = {:.2f}\".format(\n",
    "        index, np.sum(_h * np.diff(mc_bins))))\n",
    "\n",
    "# Quick check the weight, no spline involved, directly use hist vals, so the\n",
    "# Weights are not 100% percent equal. But equal enough to confirm\n",
    "# Hack to include the first data point, as digitize is exclusive...\n",
    "src_sin_dec[0] = -0.99\n",
    "idx = np.digitize(src_sin_dec, bins=mc_bins, right=True) - 1\n",
    "src_w_dec = _h[idx]\n",
    "src_w_theo = np.ones_like(src_dec)\n",
    "src_w = src_w_dec * src_w_theo / np.sum(src_w_dec * src_w_theo)\n",
    "\n",
    "plt.plot(src_sin_dec, src_w_dec, \"wo\", ms=7, mew=1.5, mec=\"k\")\n",
    "plt.tight_layout()\n",
    "\n",
    "print(\"Spline from PDF only\")\n",
    "print(w)\n",
    "\n",
    "print(\"\\nSpline from event density with livetime\")\n",
    "print(src_w.reshape(len(src_w), 1))\n",
    "\n",
    "print(\"\\nRatio\")\n",
    "print(w / src_w.reshape(len(src_w), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### ln-LLH ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Plot llh and gradient.\n",
    "The gradient is calculated analytically.\n",
    "With this test, we simply want to check, if the gradient is OK and the likelihood behaves correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Single Source\n",
    "\n",
    "First with only one source.\n",
    "\n",
    "Note: We test here \"super-signal-like\" events. Every event is exactly at the src position and every time is exactly in the rime window, where the ration is max- Only the energy is distributed as background.\n",
    "So only for really large time windows (really large) which have insanely high background rates we drop lower than the injected ns in our prediction.\n",
    "This is because the background term can only counter background-like events, which have a low signal over background ratio.\n",
    "For the events injected here, this rate is super high, so we always \"fit\" the exact amount of injected events.\n",
    "\n",
    "For such a setup for each event SoB is equal.\n",
    "So the gradient is zero at:\n",
    "\n",
    "\\begin{align}\n",
    "    0 &= -1 + \\sum_i \\frac{S}{n_b B}\\cdot \\frac{1}{n_s \\frac{S}{n_b B} + 1}\n",
    "       = -1 + N \\frac{S}{n_b B}\\cdot \\frac{1}{n_s \\frac{S}{n_b B} + 1} \\\\\n",
    "    \\Leftrightarrow \\frac{1}{N} &= \\frac{1}{n_s + \\frac{n_b B}{S}} \\\\\n",
    "    \\Leftrightarrow N &= n_s + \\frac{n_b B}{S}\n",
    "\\end{align}\n",
    "\n",
    "So now if the signal is super large (and that's what we ensured by using our super-signal-like events) the term $\\frac{n_b B}{S} \\rightarrow 0$ and we get $\\hat{n}_S = N$ which is exactly what we observe.\n",
    "\n",
    "Only if we set super high $n_B$, our $\\hat{n}_S$ shrinks as $\\frac{n_b B}{S}$ gets larger and larger and in the end $\\hat{n}_S$ even turns negative, when the 1 / SoB ratio is larger than N."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Snippet to plot and gradient\n",
    "def plot_llh(ns, lnllh, lnllh_grad, ns_max, xmin, xmax):\n",
    "    fig, (al, ar) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    al.plot(ns, lnllh)\n",
    "    if ns_max == 0:\n",
    "        al.set_xlim(-1, 1)\n",
    "    else:\n",
    "        al.set_xlim(xmin, xmax)\n",
    "    al.set_ylim(0, 1.05 * np.amax(lnllh))\n",
    "    al.axvline(ns_max, 0, 1, ls=\"--\", lw=2, color=\"C7\")\n",
    "    al.set_title(\"LLH\")\n",
    "\n",
    "    ar.plot(ns, lnllh_grad)\n",
    "    ar.axhline(0, 0, 1, ls=\"--\", lw=2, color=\"C7\")\n",
    "    ar.axvline(ns_max, 0, 1, ls=\"--\", lw=2, color=\"C7\")\n",
    "    if ns_max == 0:\n",
    "        al.set_xlim(-1, 1)\n",
    "    else:\n",
    "        al.set_xlim(xmin, xmax)\n",
    "    ar.set_ylim(-5, 5)\n",
    "    ar.set_title(\"LLH gradient in ns\")\n",
    "    fig.tight_layout()\n",
    "    return fig, (al, ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Make up some setup\n",
    "nsrcs = 1\n",
    "src_t = np.random.choice(_exp[\"timeMJD\"], size=nsrcs)\n",
    "dt = np.array([-20, 200])\n",
    "\n",
    "# Expected background with rate 5mHz, kind of realistic.\n",
    "# Increase nb scale to see ns best fit shrink.\n",
    "scale = 1e5\n",
    "nb = 0.005 * np.diff(dt) * scale\n",
    "src_ra = np.deg2rad([180])  # Arbitrarily placed single source\n",
    "src_dec = np.deg2rad([10])\n",
    "src_w_theo = np.ones_like(src_dec)\n",
    "\n",
    "# Setup src record array\n",
    "srcs = np.vstack((src_t, [dt[0]], [dt[1]],\n",
    "                  [src_ra], [src_dec], src_w_theo))\n",
    "names = [\"t\", \"dt0\", \"dt1\", \"ra\", \"dec\", \"w_theo\"]\n",
    "srcs = np.core.records.fromarrays(srcs, names=names,\n",
    "                                       formats=len(names) * [\"float64\"])\n",
    "args = {\"nb\": nb, \"srcs\": srcs}\n",
    "\n",
    "# Set the events artificially where the srcs are in space and nicely spaced\n",
    "# times inside the search window, where time sob is large. Otherwise the llh\n",
    "# is almost always peaked at 0\n",
    "N = 10\n",
    "mint, maxt = src_t + dt / secinday  # In MJD\n",
    "timeMJD = np.linspace(mint, maxt, N)\n",
    "X = np.random.choice(_exp, size=N)  # Only to copy the recarray structure\n",
    "X[\"timeMJD\"] = timeMJD\n",
    "X[\"ra\"] = np.ones_like(timeMJD) * src_ra\n",
    "X[\"sinDec\"] = np.ones_like(timeMJD) * np.sin(src_dec)\n",
    "X[\"sigma\"] = np.deg2rad(np.ones_like(timeMJD))\n",
    "\n",
    "# Scan a single LLH for the chosen data above\n",
    "n_ns = 500\n",
    "xmin, xmax = 0, 2 * N\n",
    "ns = np.linspace(xmin, xmax, n_ns)\n",
    "lnllh = np.empty(n_ns)\n",
    "lnllh_grad = np.empty(n_ns)\n",
    "for i in range(n_ns):\n",
    "    theta = {\"ns\": ns[i]}\n",
    "    lnllh[i], lnllh_grad[i] = grbllh.lnllh_ratio(X, theta, args)\n",
    "\n",
    "# Manual \"fit\" by scanning the maximum\n",
    "ns_max = ns[np.argmax(lnllh)]\n",
    "\n",
    "plot_llh(ns, lnllh, lnllh_grad, ns_max, xmin, xmax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Multiple Sources -- All at same position\n",
    "\n",
    "This time we use multiple sources, but all at the exact same location and with the exact same properties.\n",
    "We expect the very same result as in the single source case above, because the weighted sum of the signal terms reduces to\n",
    "\n",
    "\\begin{align}\n",
    "    S^\\text{tot} &= \\sum_{j=1}^{N_\\text{srcs}} w_j S_{ij}\n",
    "                 = S_{i} \\sum_{j=1}^{N_\\text{srcs}} \\frac{1}{N}\n",
    "                 = S_i \\\\\n",
    "    \\Lambda &= -2\\ln\\left(\\frac{\\mathcal{L}_0}{\\mathcal{L}_1}\\right)\n",
    "             = -n_S + \\sum_{i=1}^N\\ln\\left(\\frac{n_S S^\\text{tot}}{\\langle n_B\\rangle B_i} + 1\\right)\n",
    "             = -n_S + \\sum_{i=1}^N\\ln\\left(\\frac{n_S S_i}{\\langle n_B\\rangle B_i} + 1\\right)\n",
    "\\end{align}\n",
    "\n",
    "as all signal terms are exaxtly the same and no further background locations are introduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Repeat sources exactly as the single one from above\n",
    "nsrcs = 5\n",
    "_src_t = np.repeat(src_t, repeats=nsrcs, axis=0)\n",
    "_dt = np.repeat(dt.reshape(1, 2), axis=0, repeats=nsrcs)\n",
    "# Attention here: 100% overlapping windows so total BG is unchanged. To work\n",
    "# in the stacking framework, we just split the expectation equally\n",
    "\n",
    "# Increase nb scale to see ns best fit shrink\n",
    "scale = 1e5\n",
    "_nb = 0.005 * np.diff(_dt, axis=1).flatten() / nsrcs  * scale\n",
    "\n",
    "_src_ra = np.repeat(src_ra, repeats=nsrcs, axis=0)\n",
    "_src_dec = np.repeat(src_dec, repeats=nsrcs, axis=0)\n",
    "_src_w_theo = np.ones_like(_src_dec)\n",
    "\n",
    "# Setup src record array\n",
    "srcs = np.vstack((_src_t, _dt[:, 0], _dt[:, 1],\n",
    "                  _src_ra, _src_dec, _src_w_theo))\n",
    "\n",
    "_srcs = np.core.records.fromarrays(srcs, names=names,\n",
    "                                       formats=len(names) * [\"float64\"])\n",
    "_args = {\"nb\": _nb, \"srcs\": _srcs}\n",
    "\n",
    "# Also use the very same events for all sources here\n",
    "_X = np.copy(X)\n",
    "\n",
    "# Scan a single LLH for the chosen data above\n",
    "n_ns = 500\n",
    "xmin, xmax = 0, 2 * N\n",
    "ns = np.linspace(xmin, xmax, n_ns)\n",
    "_lnllh = np.empty(n_ns)\n",
    "_lnllh_grad = np.empty(n_ns)\n",
    "for i in range(n_ns):\n",
    "    theta = {\"ns\": ns[i]}\n",
    "    _lnllh[i], _lnllh_grad[i] = grbllh.lnllh_ratio(_X, theta, _args)\n",
    "\n",
    "# Manual \"fit\" by scanning the maximum\n",
    "_ns_max = ns[np.argmax(_lnllh)]\n",
    "\n",
    "plot_llh(ns, _lnllh, _lnllh_grad, _ns_max, xmin, xmax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Multiple Sources -- Different Right-Ascensions\n",
    "\n",
    "Now the almost same thing, but with changed right ascensions only.\n",
    "We distribute them equally around a fixed declination.\n",
    "Also everything else is left as before.\n",
    "\n",
    "This case is a bit more tricky, but a combination of the two cases above makes sure we still fit N events.\n",
    "\n",
    "We inject the same number of events (N) but we get nsrcs times the BG (because the windows don't overlap anymore).\n",
    "Each event only contributes to the window where it spatially is placed, so per source only N / nsrcs events (we choosed them so the number distribute nicely) have a SoB > 0.\n",
    "\n",
    "This means, that the total signal term is reduced by a factor of nsrcs, as the zero signal terms can't compensate the unaffected backgound which is still the same for all events.\n",
    "\n",
    "So even though our stacked signal term is reduced by a factor of nsrcs  we still get the same fit result, because the signal term is still huge and we still satisfy the condition $\\frac{n_b B}{S}\\rightarrow 0$.\n",
    "\n",
    "But we need slightly less cranked up background rate to let the best fit ns shrink as in the previous cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Repeat sources exactly as the single one from above\n",
    "nsrcs = 5\n",
    "_src_t = np.repeat(src_t, repeats=nsrcs, axis=0)\n",
    "_dt = np.repeat(dt.reshape(1, 2), axis=0, repeats=nsrcs)\n",
    "\n",
    "# Windows don't overlap anymore, so use full BG for each window\n",
    "# Increase nb scale to see ns best fit shrink\n",
    "scale = 1e5\n",
    "_nb = 0.005 * np.diff(_dt) * scale\n",
    "\n",
    "# Handpick to let windows not overlap\n",
    "_src_ra = np.deg2rad([0, 30, 60, 90, 120])\n",
    "_src_dec = np.repeat(src_dec, repeats=nsrcs, axis=0)\n",
    "_src_w_theo = np.ones_like(_src_dec)\n",
    "\n",
    "# Setup src record array\n",
    "srcs = np.vstack((_src_t, _dt[:, 0], _dt[:, 1],\n",
    "                  _src_ra, _src_dec, _src_w_theo))\n",
    "\n",
    "_srcs = np.core.records.fromarrays(srcs, names=names,\n",
    "                                       formats=len(names) * [\"float64\"])\n",
    "_args = {\"nb\": _nb, \"srcs\": _srcs}\n",
    "\n",
    "# We used 5 srcs and 10 events, so we just repeat the ras once\n",
    "# This is not very obvious on how to scale to arbirary Ns and nsrcs\n",
    "# I'm not very sure here, how many events to inject to exactly match the cases\n",
    "# above.\n",
    "# Here we just have 2 evts per window and still have ns of 10, even though\n",
    "# signal should get donwweighted to 1/5 of the two cases above per source.\n",
    "_X = np.copy(X)\n",
    "_X[\"ra\"] = np.repeat(_src_ra, repeats=2)\n",
    "\n",
    "# Scan a single LLH for the chosen data above\n",
    "n_ns = 500\n",
    "xmin, xmax = 0, 2 * N\n",
    "ns = np.linspace(xmin, xmax, n_ns)\n",
    "_lnllh = np.empty(n_ns)\n",
    "_lnllh_grad = np.empty(n_ns)\n",
    "for i in range(n_ns):\n",
    "    theta = {\"ns\": ns[i]}\n",
    "    _lnllh[i], _lnllh_grad[i] = grbllh.lnllh_ratio(_X, theta, _args)\n",
    "\n",
    "# Manual \"fit\" by scanning the maximum\n",
    "_ns_max = ns[np.argmax(_lnllh)]\n",
    "\n",
    "plot_llh(ns, _lnllh, _lnllh_grad, _ns_max, xmin, xmax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analysis module grabs all the stuf from before and creates trial calculation from it.\n",
    "So we test here, if everything wrapped up correctly and if we get OK looking test statistics from our trials.\n",
    "\n",
    "A genreal note on how our experimental is handled:\n",
    "\n",
    "Before we start out analysis, we split our data in off-time and on-time data.\n",
    "On-time data is data around a a-priori fixed time frame around our sources we want to test.\n",
    "We exclude this data until the very end, because we don't want to bias ourselfes as there is the possibility that the signal we want to find is in that on-time data.\n",
    "\n",
    "The off-time data is everything else and is assumed to not contain the sought after signal.\n",
    "The on-time time frame should be choosen large enough to account for that.\n",
    "It should definitely be larger than the time frames we test for in our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Fit LLH paramters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We test the same cases as with the bare LLH from above.\n",
    "Everything should be the same.\n",
    "This is basically a test to see if we wrapped the LLH correctly in the analysis module and if we get the same result from the fitter as from the manual scan in ns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Snippet to plot and gradient\n",
    "def plot_llh(ns, lnllh, lnllh_grad, ns_max, xmin, xmax):\n",
    "    fig, (al, ar) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    al.plot(ns, lnllh)\n",
    "    if ns_max == 0:\n",
    "        al.set_xlim(-1, 1)\n",
    "    else:\n",
    "        al.set_xlim(xmin, xmax)\n",
    "    al.set_ylim(0, 1.05 * np.amax(lnllh))\n",
    "    al.axvline(ns_max, 0, 1, ls=\"--\", lw=2, color=\"C7\")\n",
    "    al.set_title(\"LLH. ns_max = {:.2f}\".format(ns_max))\n",
    "\n",
    "    ar.plot(ns, lnllh_grad)\n",
    "    ar.axhline(0, 0, 1, ls=\"--\", lw=2, color=\"C7\")\n",
    "    ar.axvline(ns_max, 0, 1, ls=\"--\", lw=2, color=\"C7\")\n",
    "    if ns_max == 0:\n",
    "        al.set_xlim(-1, 1)\n",
    "    else:\n",
    "        al.set_xlim(xmin, xmax)\n",
    "    ar.set_ylim(-5, 5)\n",
    "    ar.set_title(\"LLH gradient in ns\")\n",
    "    fig.tight_layout()\n",
    "    return fig, (al, ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create a grbllh likelihood object we want to test with\n",
    "sin_dec_bins = np.linspace(-1, 1, 50)\n",
    "\n",
    "min_logE = 1  #  min(np.amin(_exp[\"logE\"]), np.amin(mc[\"logE\"]))\n",
    "max_logE = 10 #  max(np.amax(_exp[\"logE\"]), np.amax(mc[\"logE\"]))\n",
    "logE_bins = np.linspace(min_logE, max_logE, 40)\n",
    "\n",
    "spatial_pdf_args = {\"bins\": sin_dec_bins, \"k\": 3, \"kent\": True}\n",
    "\n",
    "energy_pdf_args = {\"bins\": [sin_dec_bins, logE_bins],\n",
    "                   \"gamma\": 2., \"fillval\": \"col\", \"interpol_log\": False}\n",
    "\n",
    "time_pdf_args = {\"nsig\": 4., \"sigma_t_min\": 2., \"sigma_t_max\": 30.}\n",
    "\n",
    "grbllh = LLH.GRBLLH(X=_exp, MC=mc, srcs=None,\n",
    "                    spatial_pdf_args=spatial_pdf_args,\n",
    "                    energy_pdf_args=energy_pdf_args,\n",
    "                    time_pdf_args=time_pdf_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Single source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "First we test if the module simply wrapps the LLH module correctly.\n",
    "This should reproduce same results (not regarding random fluctuations of course) as in the section ln-llh ratio, as we test the same setup as above here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Make up some setup\n",
    "nsrcs = 1\n",
    "src_t = np.random.choice(_exp[\"timeMJD\"], size=nsrcs)\n",
    "dt = np.array([-20, 200])\n",
    "\n",
    "# Expected background with rate 5mHz, kind of realistic.\n",
    "# Increase nb scale to see ns best fit shrink.\n",
    "scale = 1e5\n",
    "nb = 0.005 * np.diff(dt) * scale\n",
    "src_ra = np.deg2rad([180])  # Arbitrarily placed single source\n",
    "src_dec = np.deg2rad([10])\n",
    "src_w_theo = np.ones_like(src_dec)\n",
    "\n",
    "# Setup src record array\n",
    "srcs = np.vstack((src_t, [dt[0]], [dt[1]],\n",
    "                  [src_ra], [src_dec], src_w_theo))\n",
    "names = [\"t\", \"dt0\", \"dt1\", \"ra\", \"dec\", \"w_theo\"]\n",
    "srcs = np.core.records.fromarrays(srcs, names=names,\n",
    "                                       formats=len(names) * [\"float64\"])\n",
    "args = {\"nb\": nb, \"srcs\": srcs}\n",
    "\n",
    "# Build the analysis module\n",
    "ana = Analysis.TransientsAnalysis(srcs=srcs, llh=grbllh)\n",
    "\n",
    "# Set the events artificially where the srcs are in space and nicely spaced\n",
    "# times inside the search window, where time sob is large. Otherwise the llh\n",
    "# is almost always peaked at 0\n",
    "N = 10\n",
    "mint, maxt = src_t + dt / secinday  # In MJD\n",
    "timeMJD = np.linspace(mint, maxt, N)\n",
    "X = np.random.choice(_exp, size=N)  # Only to copy the recarray structure\n",
    "X[\"timeMJD\"] = timeMJD\n",
    "X[\"ra\"] = np.ones_like(timeMJD) * src_ra\n",
    "X[\"sinDec\"] = np.ones_like(timeMJD) * np.sin(src_dec)\n",
    "X[\"sigma\"] = np.deg2rad(np.ones_like(timeMJD))\n",
    "\n",
    "# Scan a single LLH for the chosen data above\n",
    "n_ns = 500\n",
    "xmin, xmax = 0, 2 * N\n",
    "ns = np.linspace(xmin, xmax, n_ns)\n",
    "lnllh = np.empty(n_ns)\n",
    "lnllh_grad = np.empty(n_ns)\n",
    "for i in range(n_ns):\n",
    "    theta = {\"ns\": ns[i]}\n",
    "    lnllh[i], lnllh_grad[i] = ana.llh.lnllh_ratio(X, theta, args)\n",
    "\n",
    "# Manual \"fit\" by scanning the maximum\n",
    "ns_max = ns[np.argmax(lnllh)]\n",
    "\n",
    "plot_llh(ns, lnllh, lnllh_grad, ns_max, xmin, xmax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Also let's quickly see, how the times are distributed within the time PDF\n",
    "inj_trange = ana.llh.get_injection_trange(src_t=srcs[\"t\"], dt=dt)\n",
    "inj_trange = src_t + inj_trange.flatten() / secinday\n",
    "\n",
    "x = np.linspace(inj_trange[0], inj_trange[1], 100)\n",
    "y = ana.llh._soverb_time(t=x, src_t=srcs[\"t\"], dt=dt)\n",
    "\n",
    "plt.plot(x, y.reshape(len(x)))\n",
    "plt.vlines(X[\"timeMJD\"], 0, np.amax(y), colors=\"C7\", linestyles=\"--\", lw=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Test if we can do the same as above, but using the scipy fitter this time to get the maximum.\n",
    "The LLH curve and the maximum should be identical to the ones above (except for small errors in scanning vs fitting ns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Seed for the fitter\n",
    "theta0 = {\"ns\": 1}\n",
    "\n",
    "# Args must only contain \"nb\", everything else is used from self.srcs\n",
    "args = {\"nb\": nb}\n",
    "\n",
    "res = ana.fit_lnllh_ratio_params(X, theta0, args)\n",
    "\n",
    "plot_llh(ns, lnllh, lnllh_grad, res.x[0], xmin, xmax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Multiple Sources -- All at same position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Repeat sources exactly as the single one from above\n",
    "nsrcs = 5\n",
    "_src_t = np.repeat(src_t, repeats=nsrcs, axis=0)\n",
    "_dt = np.repeat(dt.reshape(1, 2), axis=0, repeats=nsrcs)\n",
    "# Attention here: 100% overlapping windows so total BG is unchanged. To work\n",
    "# in the stacking framework, we just split the expectation equally\n",
    "\n",
    "# Increase nb scale to see ns best fit shrink\n",
    "scale = 1e5\n",
    "_nb = 0.005 * np.diff(_dt, axis=1).flatten() / nsrcs  * scale\n",
    "\n",
    "_src_ra = np.repeat(src_ra, repeats=nsrcs, axis=0)\n",
    "_src_dec = np.repeat(src_dec, repeats=nsrcs, axis=0)\n",
    "_src_w_theo = np.ones_like(_src_dec)\n",
    "\n",
    "# Setup src record array\n",
    "srcs = np.vstack((_src_t, _dt[:, 0], _dt[:, 1],\n",
    "                  _src_ra, _src_dec, _src_w_theo))\n",
    "\n",
    "_srcs = np.core.records.fromarrays(srcs, names=names,\n",
    "                                       formats=len(names) * [\"float64\"])\n",
    "_args = {\"nb\": _nb, \"srcs\": _srcs}\n",
    "\n",
    "# Build the analysis module\n",
    "ana = Analysis.TransientsAnalysis(srcs=_srcs, llh=grbllh)\n",
    "\n",
    "# Also use the very same events for all sources here\n",
    "_X = np.copy(X)\n",
    "\n",
    "# Scan a single LLH for the chosen data above\n",
    "n_ns = 500\n",
    "xmin, xmax = 0, 2 * N\n",
    "ns = np.linspace(xmin, xmax, n_ns)\n",
    "_lnllh = np.empty(n_ns)\n",
    "_lnllh_grad = np.empty(n_ns)\n",
    "for i in range(n_ns):\n",
    "    theta = {\"ns\": ns[i]}\n",
    "    _lnllh[i], _lnllh_grad[i] = ana.llh.lnllh_ratio(_X, theta, _args)\n",
    "\n",
    "# Manual \"fit\" by scanning the maximum\n",
    "_ns_max = ns[np.argmax(_lnllh)]\n",
    "\n",
    "plot_llh(ns, _lnllh, _lnllh_grad, _ns_max, xmin, xmax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Again check using the class function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Seed for the fitter\n",
    "theta0 = {\"ns\": 1}\n",
    "# Args must only contain \"nb\", everything else is used from self.srcs\n",
    "_args = {\"nb\": _nb}\n",
    "    \n",
    "res = ana.fit_lnllh_ratio_params(_X, theta0, _args, bounds=None)\n",
    "    \n",
    "plot_llh(ns, _lnllh, _lnllh_grad, res.x[0], xmin, xmax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Multiple Sources -- Different Right-Ascencions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Repeat sources exactly as the single one from above\n",
    "nsrcs = 5\n",
    "_src_t = np.repeat(src_t, repeats=nsrcs, axis=0)\n",
    "_dt = np.repeat(dt.reshape(1, 2), axis=0, repeats=nsrcs)\n",
    "\n",
    "# Windows don't overlap anymore, so use full BG for each window\n",
    "# Increase nb scale to see ns best fit shrink\n",
    "scale = 1e5\n",
    "_nb = 0.005 * np.diff(_dt) * scale\n",
    "\n",
    "# Handpick to let windows not overlap\n",
    "_src_ra = np.deg2rad([0, 30, 60, 90, 120])\n",
    "_src_dec = np.repeat(src_dec, repeats=nsrcs, axis=0)\n",
    "_src_w_theo = np.ones_like(_src_dec)\n",
    "\n",
    "# Setup src record array\n",
    "srcs = np.vstack((_src_t, _dt[:, 0], _dt[:, 1],\n",
    "                  _src_ra, _src_dec, _src_w_theo))\n",
    "\n",
    "_srcs = np.core.records.fromarrays(srcs, names=names,\n",
    "                                       formats=len(names) * [\"float64\"])\n",
    "_args = {\"nb\": _nb, \"srcs\": _srcs}\n",
    "\n",
    "# Build the analysis module\n",
    "ana = Analysis.TransientsAnalysis(srcs=_srcs, llh=grbllh)\n",
    "\n",
    "# We used 5 srcs and 10 events, so we just repeat the ras once\n",
    "# This is not very obvious on how to scale to arbirary Ns and nsrcs\n",
    "# I'm not very sure here, how many events to inject to exactly match the cases\n",
    "# above.\n",
    "# Here we just have 2 evts per window and still have ns of 10, even though\n",
    "# signal should get donwweighted to 1/5 of the two cases above per source.\n",
    "_X = np.copy(X)\n",
    "_X[\"ra\"] = np.repeat(_src_ra, repeats=2)\n",
    "\n",
    "# Scan a single LLH for the chosen data above\n",
    "n_ns = 500\n",
    "xmin, xmax = 0, 2 * N\n",
    "ns = np.linspace(xmin, xmax, n_ns)\n",
    "_lnllh = np.empty(n_ns)\n",
    "_lnllh_grad = np.empty(n_ns)\n",
    "for i in range(n_ns):\n",
    "    theta = {\"ns\": ns[i]}\n",
    "    _lnllh[i], _lnllh_grad[i] = grbllh.lnllh_ratio(_X, theta, _args)\n",
    "\n",
    "# Manual \"fit\" by scanning the maximum\n",
    "_ns_max = ns[np.argmax(_lnllh)]\n",
    "\n",
    "plot_llh(ns, _lnllh, _lnllh_grad, _ns_max, xmin, xmax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Again check with the class function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Seed for the fitter\n",
    "theta0 = {\"ns\": 1}\n",
    "# Args must only contain \"nb\", everything else is used from self.srcs\n",
    "_args = {\"nb\": _nb}\n",
    "    \n",
    "res = ana.fit_lnllh_ratio_params(_X, theta0, _args, bounds=None)\n",
    "    \n",
    "plot_llh(ns, _lnllh, _lnllh_grad, res.x[0], xmin, xmax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using all the modules from above we can run trials with pure background now.\n",
    "\n",
    "A trial is a single pseudo experiment we perform and evaluate to get an idea of the underlying statistical distribtuion and to build our test statistic from which we can infer the significance of real data later.\n",
    "For that we need to generate sets of pseudo-data which has background-like properties.\n",
    "The `bg_injector`, `bg_rate_injector` and `rate_function` classes are used to generate these properties.\n",
    "\n",
    "So each trial consist of the following steps, in which the source positions are always fixed and a-priori known:\n",
    "\n",
    "1. Determine the expected number background events per source time window we test.\n",
    "   This is derived from the `bg_rate_injector` which returns a list of sampled times for each time window.\n",
    "   It knows the expected rate from the given `rate_function`\n",
    "2. In addition to our sampled times, we need all the other event features we have on real data (positions, energy, uncertainty) , because our sampled pseudo-data should have the same properties as real data.\n",
    "   These missing properties are generated by the `bg_injector` class.\n",
    "3. When we sampled our pseudo-events we need to fit the LLH to this set of events and see what best fit we get.\n",
    "4. We do that a lot of times and see how our best fits are distributed which gives us a so called test statistic which describes the distribution of LLH firs using BG only.\n",
    "\n",
    "On background-like events we expect to get a null fit result most of the times, because no signal is present.\n",
    "But out of chance, we sometimes get a combination of background-like events, that has very signal-like properties.\n",
    "The so build test statistic is then used to see how unlikely the single fit to our on-time data was and how lucky we'd have to get to observe that result out of chance from pure background."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BG only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a bg rat einjector model\n",
    "def filter_runs(run):\n",
    "    \"\"\"\n",
    "    Filter runs as stated in jfeintzig's doc.\n",
    "    \"\"\"\n",
    "    exclude_runs = [120028, 120029, 120030, 120087, 120156, 120157]\n",
    "    if ((run[\"good_i3\"] == True) & (run[\"good_it\"] == True) &\n",
    "        (run[\"run\"] not in exclude_runs)):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Let's create an injector using a goodrun list.\n",
    "runlist=\"data/runlists/ic86-i-goodrunlist.json\"\n",
    "rate_func = RateFunc.Sinus1yrRateFunction()\n",
    "runlist_inj = BGRateInj.RunlistBGRateInjector(runlist, filter_runs, rate_func)\n",
    "\n",
    "# `fit` the injector to make it usable\n",
    "times = exp[\"timeMJD\"]\n",
    "rate_func = runlist_inj.fit(T=times, x0=None, remove_zero_runs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create our bg injector, here we use the data resampler\n",
    "data_inj = BGInj.DataBGInjector()\n",
    "data_inj.fit(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Least we need a LLH, use the GRBLLH\n",
    "sin_dec_bins = np.linspace(-1, 1, 50)\n",
    "\n",
    "# Choose borders and bins by eye\n",
    "min_logE = 1 \n",
    "max_logE = 10\n",
    "logE_bins = np.linspace(min_logE, max_logE, 40)\n",
    "\n",
    "spatial_pdf_args = {\"bins\": sin_dec_bins, \"k\": 3, \"kent\": True}\n",
    "energy_pdf_args = {\"bins\": [sin_dec_bins, logE_bins],\n",
    "                   \"gamma\": 2., \"fillval\": \"col\", \"interpol_log\": False}\n",
    "time_pdf_args = {\"nsig\": 4.}\n",
    "\n",
    "grbllh = LLH.GRBLLH(X=_exp, MC=mc,\n",
    "                    spatial_pdf_args=spatial_pdf_args,\n",
    "                    energy_pdf_args=energy_pdf_args,\n",
    "                    time_pdf_args=time_pdf_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create some srcs to test for\n",
    "dt = np.atleast_2d([[-20, 200], [-5, 20], [0, 300]])\n",
    "n_srcs = len(dt)\n",
    "\n",
    "dtype = [(\"t\", np.float), (\"dt0\", np.float), (\"dt1\", np.float),\n",
    "         (\"ra\", np.float), (\"dec\", np.float)]\n",
    "srcs = np.zeros((n_srcs, ), dtype=dtype)\n",
    "srcs[\"t\"] = np.random.choice(_exp[\"timeMJD\"], size=n_srcs)\n",
    "srcs[\"dt0\"] = dt[:, 0]\n",
    "srcs[\"dt1\"] = dt[:, 1]\n",
    "srcs[\"ra\"] = np.random.uniform(0, 2 * np.pi, n_srcs)\n",
    "srcs[\"dec\"] = np.arcsin(np.random.uniform(-1, 1, n_srcs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# And now the analysis object\n",
    "ana = Analysis.TransientsAnalysis(srcs=srcs, llh=grbllh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "self = ana\n",
    "\n",
    "_X = []\n",
    "times = []\n",
    "rnd_ra = []\n",
    "args = []\n",
    "\n",
    "for src_idx in range(len(srcs)):\n",
    "    # Samples times and thus number of bg expectated events\n",
    "    t = self.srcs[\"t\"][src_idx]\n",
    "    dt = [self.srcs[\"dt0\"][src_idx], self.srcs[\"dt1\"][src_idx]]\n",
    "    _times = runlist_inj.sample(t=t, trange=dt, ntrials=1)[0]\n",
    "    nb = len(_times)\n",
    "    times.append(_times)\n",
    "    args.append({\"nb\": nb})\n",
    "    # Sample rest of features\n",
    "    if nb > 0:\n",
    "        _X.append(data_inj.sample(n_samples=nb))\n",
    "        rnd_ra.append(np.random.uniform(0, 2. * np.pi, size=nb))\n",
    "\n",
    "names = [\"ra\", \"sinDec\", \"timeMJD\", \"logE\", \"sigma\"]\n",
    "dtype = [(n, t) for (n, t) in zip(names, len(names) * [np.float])]\n",
    "nb_tot = np.sum([d[\"nb\"] for d in args])\n",
    "X = np.empty((nb_tot, ), dtype=dtype)\n",
    "# Make output array in compatible format\n",
    "_X = flatten_list_of_1darrays(_X)\n",
    "X[\"ra\"] = flatten_list_of_1darrays(rnd_ra)\n",
    "X[\"sinDec\"] = np.sin(_X[:, 1])\n",
    "X[\"logE\"] = _X[:, 0]\n",
    "X[\"sigma\"] = _X[:, 2]\n",
    "X[\"timeMJD\"] = flatten_list_of_1darrays(times)\n",
    "\n",
    "nb_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ana.fit_lnllh_ratio_params(X, theta0={\"ns\": 1}, args=args)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some more side tests to clarify / justify details, that would clutter the main test notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# from importlib import reload\n",
    "import helper as hlp\n",
    "\n",
    "import numpy as np\n",
    "import numpy.lib.recfunctions\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mpldates\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.colors import LogNorm, Normalize\n",
    "%matplotlib inline\n",
    "\n",
    "import scipy.interpolate as sci\n",
    "import scipy.optimize as sco\n",
    "import scipy.integrate as scint\n",
    "import scipy.stats as scs\n",
    "from scipy.stats import rv_continuous\n",
    "import scipy.signal as scsignal\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import json\n",
    "import datetime\n",
    "import pickle\n",
    "from astropy.time import Time as astrotime\n",
    "from corner import corner\n",
    "\n",
    "import sklearn.neighbors as skn\n",
    "import sklearn.model_selection as skms\n",
    "\n",
    "# from corner_hist import corner_hist\n",
    "from anapymods3.plots import dg, split_axis, get_binmids, hist_marginalize\n",
    "from anapymods3.stats import json2kde, GaussianKDE, rejection_sampling\n",
    "from anapymods3.general.misc import fill_dict_defaults\n",
    "\n",
    "import tdepps.bg_injector as BGInj\n",
    "import tdepps.bg_rate_injector as BGRateInj\n",
    "import tdepps.rate_function as RateFunc\n",
    "import tdepps.llh as LLH\n",
    "import tdepps.analysis as Analysis\n",
    "import tdepps.signal_injector as SigInj\n",
    "\n",
    "secinday = 24. * 60. * 60."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Load IC86 data from epinat, which should be the usual IC86-I (2011) PS sample, but pull corrected and OneWeights corrected by number of events generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "exp = np.load(\"data/IC86_I_data.npy\")\n",
    "mc = np.load(\"data/IC86_I_mc.npy\")\n",
    "# Use the officially stated livetime, not the ones from below\n",
    "livetime = 332.61"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Data livetime comparison to v1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's compare to the v1.4 list, as used by jfeintzig.\n",
    "Oddly we have 0.2 days less livetime as he had.\n",
    "The number of runs is correct though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "goodrun_dict, ic_livetime = hlp.create_goodrun_dict(\n",
    "    runlist=\"data/runlists/ic86-i-goodrunlist.json\", filter_runs=None)\n",
    "nruns = len(goodrun_dict[\"run\"])\n",
    "\n",
    "# Make a recarray to use older code below\n",
    "types = [v.dtype for v in goodrun_dict.values()]\n",
    "dtype = [(str(n), f) for n, f in zip(goodrun_dict.keys(), types)]\n",
    "inc_run_arr = np.empty((nruns,), dtype=dtype)\n",
    "\n",
    "for k, v in goodrun_dict.items():\n",
    "    inc_run_arr[k] = v\n",
    "    \n",
    "print(inc_run_arr.dtype.names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# New livetime from iclive runlist\n",
    "print(\"Total runs from iclive     : \", len(inc_run_arr[\"run\"]))\n",
    "print(\"IC86-I livetime from iclive: \", ic_livetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# For comparison, also parse the v1.4 list\n",
    "# Should be: 1081 runs, with a total livetime of 332.61 days.\n",
    "with open(\"data/runlists/Prelim_IC86-I_v1.4a.txt\",'r') as f:\n",
    "    data = []\n",
    "    for line in f.readlines():\n",
    "        data.append(line.replace('\\n',''))\n",
    "        \n",
    "# Skip to beginning of run info\n",
    "data = data[73:]\n",
    "\n",
    "# Split at white space\n",
    "data = [d.split() for d in data]\n",
    "\n",
    "dtype = [(\"runID\", np.int), (\"duration\", np.float), (\"IT\", \"|S2\"),\n",
    "         (\"CONF\", \"|S7\"), (\"FLAG\", \"|S6\")]\n",
    "runlist = np.empty((len(data),), dtype=dtype)\n",
    "\n",
    "runlist[\"runID\"] = np.array([int(d[0]) for d in data])\n",
    "runlist[\"duration\"] = np.array([float(d[3]) for d in data])\n",
    "runlist[\"IT\"] = np.array([d[5] for d in data])\n",
    "runlist[\"CONF\"] = np.array([d[6] for d in data])\n",
    "runlist[\"FLAG\"] = np.array([d[7] for d in data])\n",
    "\n",
    "# Now filter: Include IT=it, CONF=full, FLAG=GOOD, exclude strange rate runs\n",
    "exclude_rate = [120028, 120029, 120030, 120087, 120156, 120157]\n",
    "itgood = runlist[\"IT\"] == b\"IT\"  # Somehow only bitwise comparison is non-empty\n",
    "confgood = runlist[\"CONF\"] == b\"full\"\n",
    "flaggood = runlist[\"FLAG\"] == b\"GOOD\"\n",
    "ratebad = np.in1d(runlist[\"runID\"], exclude_rate)\n",
    "\n",
    "include = itgood & confgood & flaggood & ~ratebad\n",
    "runlist_inc = runlist[include]\n",
    "\n",
    "# Get the livetime of the sample in days\n",
    "hoursindays = 24.\n",
    "secinday = hoursindays * 60. * 60.\n",
    "old_livetime = np.sum(runlist_inc[\"duration\"]) / hoursindays\n",
    "\n",
    "print(\"Total runs from v1.4     : \", len(runlist_inc))\n",
    "print(\"Total livetime from v1.4 : \", old_livetime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's see, if the 120 extra runs in the new runlist make up for the difference of about 10 days in livetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "iclive_in_old = np.in1d(inc_run_arr[\"run\"], runlist_inc[\"runID\"])\n",
    "not_in_old = inc_run_arr[~iclive_in_old]\n",
    "\n",
    "start = not_in_old[\"good_start_mjd\"]\n",
    "stop = not_in_old[\"good_stop_mjd\"]\n",
    "missing_livetime = np.sum(stop - start)\n",
    "\n",
    "print(\"\\nOfficial IC86-I PS livetime: \", livetime)\n",
    "print(\"Total livetime from v1.4   : \", old_livetime)\n",
    "print(\"IC86-I livetime from iclive: \", ic_livetime)\n",
    "\n",
    "print(\"\\nMissing runs in old: \", len(not_in_old))\n",
    "print(\"Livetime icliv - old :\", ic_livetime - old_livetime)\n",
    "\n",
    "print(\"\\nDiff from summing missing runs           : \", missing_livetime)\n",
    "print(\"New iclive livetime with same runs as old: \",\n",
    "      ic_livetime - missing_livetime)\n",
    "\n",
    "print(\"\\nTotal rate [Hz] over total livetime: \",\n",
    "      len(exp) / (livetime * secinday))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "All runs from the new run list that have zero events, make up for the missing runs in the old runlist, so this is consisting.\n",
    "\n",
    "Dont't know though, where the missing 0,2 days come from. Probably some runtimes have shifted a little making some extra livetime in the new list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# Store events in bins with run borders\n",
    "exp_times = exp[\"timeMJD\"]\n",
    "start_mjd = inc_run_arr[\"good_start_mjd\"]\n",
    "stop_mjd = inc_run_arr[\"good_stop_mjd\"]\n",
    "\n",
    "tot = 0\n",
    "evts_in_run = {}\n",
    "for start, stop , runid in zip(start_mjd, stop_mjd, inc_run_arr[\"run\"]):\n",
    "    mask = (exp_times >= start) & (exp_times < stop)\n",
    "    evts_in_run[runid] = exp[mask]\n",
    "    tot += np.sum(mask)\n",
    "    \n",
    "# Crosscheck, if we got all events and counted nothing double\n",
    "print(\"Do we have all events? \", tot == len(exp))\n",
    "print(\"  Events selected : \", tot)\n",
    "print(\"  Events in exp   : \", len(exp))\n",
    "\n",
    "# Create binmids and histogram values in each bin\n",
    "binmids = 0.5 * (start_mjd + stop_mjd)\n",
    "h = np.zeros(len(binmids), dtype=np.float)\n",
    "\n",
    "for i, evts in enumerate(evts_in_run.values()):\n",
    "    h[i] = len(evts)\n",
    "    \n",
    "m = (h > 0)\n",
    "print(\"Runs with 0 events :\", np.sum(~m))\n",
    "print(\"Runtime in those runs: \", np.sum(inc_run_arr[\"good_stop_mjd\"][~m] -\n",
    "                                        inc_run_arr[\"good_start_mjd\"][~m]))\n",
    "\n",
    "# Remove all zero event runs (artifacts from new run list) and calc the rate\n",
    "stop_mjd, start_mjd = stop_mjd[m], start_mjd[m]\n",
    "h = h[m] / ((stop_mjd - start_mjd) * secinday)\n",
    "binmids = binmids[m]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Time dependent rate function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Note: I think it is unnecessary to use a time and declination dependent rate. The spatial part is injected from the data BG from KDE anyways. So we just need to have the rate to determine how much events we inject allsky.**\n",
    "\n",
    "Rate ist time dependent because of seasonal variation.\n",
    "We take this varariation into account by fitting a priodic function to the time resolved rate.\n",
    "\n",
    "The data is built by calculating the rate in each run as seen before.\n",
    "This rate is correctly normalized and smoothes local fluctuations.\n",
    "\n",
    "### Peridoc function with a weighted least squares fit\n",
    "\n",
    "See side_test for comparison to spline fits.\n",
    "The function is a simple sinus scalable by 4 parameters to fit the shape of the rates:\n",
    "\n",
    "$$\n",
    "    f(x) = a\\cdot \\sin(b\\cdot(x - c)) + d\n",
    "$$\n",
    "\n",
    "The least squares loss function is\n",
    "\n",
    "$$\n",
    "    R = \\sum_i (w_i(y_i - f(x_i)))^2\n",
    "$$\n",
    "\n",
    "Weights are standard deviations from poisson histogram error.\n",
    "\n",
    "$$\n",
    "    w_i = \\frac{1}{\\sigma_i}\n",
    "$$\n",
    "\n",
    "Seed values are estimated from plot rate vs time.\n",
    "\n",
    "- Period should be 365 days (MJD) because we have one year of data so we choose $b0 = 2\\pi/365$.\n",
    "- Amplitude is about $a_0=-0.0005$, because sinus seems to start with negative values.\n",
    "- The x-offset is choose as the first start date, to get the right order of magnitude.\n",
    "- The y-axis intersection $d$ schould be close to the weighted average, so we take this as a seed.\n",
    "\n",
    "The bounds are motivated as follows (and if we don't hit them, it's OK to use them).\n",
    "\n",
    "- Amplitude $a$ should be positive, this also resolves a degenracy between a-axis offset.\n",
    "- The period $b$ should scatter around one year, a period larger than +-1 half a year is unphysical.\n",
    "- The x-offset $c$ cannot be greater than the initial +- the period because we have a periodic function.\n",
    "- The y-axis offset $d$ is arbitrarily constrained, but as seen from the plot it should not exceed 0.1. \n",
    "\n",
    "## Proposed was something like this\n",
    "\n",
    "Rate ist time dependent because of seasonal variation and delination dependent because the detector acceptance is declination dependent.\n",
    "A correletation should not exist or be very small.\n",
    "\n",
    "So we express the rate in depence of time and decliantion as\n",
    "\n",
    "$$\n",
    "    R(t,\\delta) = R_T(t)\\cdot R_\\delta(t)\n",
    "$$\n",
    "\n",
    "with independent parts in declination and time each.\n",
    "\n",
    "The time parts is constructed by fitting a periodic function.\n",
    "Then we seperatly fit a spline to the total $\\sin(\\delta)$ distribtuion and normalizes it over the declination range of the sample.\n",
    "For a given time and declination the smooth function $R(t, \\delta)$ gives the correct detector rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Splinefit to sinDec\n",
    "\n",
    "First we fit a spline to the sinDec distribtuion for all events.\n",
    "This is equivalent to what is done in skylab to estimate the per signal background PDF from data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# This is equivalent to skylab's baclground PDF construction\n",
    "sinDec_bins = 25\n",
    "sinDec_range= [-1, 1]\n",
    "sinDec_hist, sinDec_bins = np.histogram(exp[\"sinDec\"], density=True,\n",
    "                                        bins=sinDec_bins, range=sinDec_range)\n",
    "\n",
    "m = get_binmids([sinDec_bins])[0]\n",
    "\n",
    "if np.any(sinDec_hist <= 0.):\n",
    "    raise ValueError((\"Declination hist bins empty, this must not happen. \"\n",
    "                      +\"Empty bin idx: {}\".format(\n",
    "                          np.arange(len(m))[sinDec_hist <= 0.])))\n",
    "\n",
    "# Fit to logarithm, to avoid ringing. Raise err if evaluated outside range\n",
    "sinDec_spline = sci.InterpolatedUnivariateSpline(m, np.log(sinDec_hist),\n",
    "                                                 k=3, ext=\"extrapolate\")\n",
    "\n",
    "# Normalize to area on whole sky = 1, so norm = 2pi * integral(exp(spl))\n",
    "def sinDec_pdf_(x):\n",
    "    return np.exp(sinDec_spline(x))\n",
    "\n",
    "norm = scint.quad(sinDec_pdf_, -1, 1)[0] * 2. * np.pi\n",
    "\n",
    "def sinDec_pdf(x):\n",
    "    return (np.exp(sinDec_spline(x))) / norm\n",
    "\n",
    "print(\"SinDec pdf has area on 4pi = \", scint.quad(\n",
    "    sinDec_pdf, -1, 1)[0] * 2 * np.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x = np.linspace(sinDec_bins[0], sinDec_bins[-1], 100)\n",
    "y = sinDec_pdf(x)\n",
    "\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Try to hist dec dependence of single run\n",
    "# As expected we have not enough statistic to see anything\n",
    "run = 60  # OFten these lead to empty hists...\n",
    "start = start_mjd[run]\n",
    "stop = stop_mjd[run]\n",
    "mask = (exp_times >= start) & (exp_times < stop)\n",
    "_sinDec_run = np.sin(exp[\"dec\"][mask])\n",
    "\n",
    "plt.hist(_sinDec_run, range=[-1, 1], bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Spline to rate distribution\n",
    "\n",
    "Choosing spline weights according to scipy.interpolate.UnivariateSpline manual:\n",
    "\n",
    "    If None (default), s = len(w) which should be a good value if 1/w[i] is an estimate of\n",
    "    the standard deviation of y[i].\n",
    "\n",
    "Internally it is doing a weighted least squares fit with $\\sum_i(w_i(y_i-\\text{spl}(x_i)))^2 \\leq s$.\n",
    "We leave $s$ as the default because we have an estimate for the stddevs: $\\sigma_i = \\sqrt{h_i}$.\n",
    "To match the definition of the weights we use:\n",
    "\n",
    "$$\n",
    "    \\frac{1}{w_i} \\stackrel{!}{=} \\sigma_i = \\sqrt{h_i} \\Leftrightarrow w_i \n",
    "                               = \\frac{1}{\\sqrt{h_i}}\n",
    "$$\n",
    "\n",
    "Because we scaled $h_i$ to get the rate in events per s, we need to scale the errors too:\n",
    "\n",
    "$$\n",
    "    \\tilde{h}_i = \\frac{h_i}{s_i} \\Rightarrow \\tilde{\\sigma}_i = \\frac{\\sqrt{h_i}}{s_i} \n",
    "                = \\frac{\\sqrt{s_i\\tilde{h}_i}}{s} \n",
    "                = \\sqrt{\\frac{\\tilde{h}_i}{s_i}} = \\frac{1}{w_i}\n",
    "$$\n",
    "\n",
    "The smoothing condition `s` chooses the support knots based on the weights.\n",
    "Because we have some oscilating pattern due to to seasonal variations (periode ~1yr) a quadratic spline function is not enough.\n",
    "So we choose the next higher order, a cubic spline, which is able to oscilate up and down exatcly once.\n",
    "\n",
    "**Note:** If a weight is zero, the corresponding point doesn't contribute at all.\n",
    "So we might consider using $w_i = \\sigma_i$ instead.\n",
    "Then point woth high poisson statsitics are preferred over low statistic bins.\n",
    "It doesn't seem to make a huge difference though.\n",
    "\n",
    "Below we try both weights and the unweighted case.\n",
    "For the 'correctly' weighted case with $w_i = 1. / \\sigma_i$ the spline oscillates strongly.\n",
    "So we better try a true perdiodic function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# h is already scaled, so we need to scale the errors too\n",
    "yerr = np.sqrt(h) / np.sqrt((stop_mjd - start_mjd) * secinday)\n",
    "# Originally used w = 1. / yerr, but for poisson bins use rel. error\n",
    "w = h / yerr\n",
    "rate_spline = sci.UnivariateSpline(binmids, h, k=3, w=w,\n",
    "                                   s=None, ext=\"extrapolate\")\n",
    "\n",
    "rate_spline_inv = sci.UnivariateSpline(binmids, h, k=3, w=yerr,\n",
    "                                       s=None, ext=\"extrapolate\")\n",
    "\n",
    "rate_spline_unw = sci.UnivariateSpline(binmids, h, k=3, w=None,\n",
    "                                       s=None, ext=\"extrapolate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Plot runs\n",
    "xerr = 0.5 * (stop_mjd - start_mjd)\n",
    "plt.errorbar(binmids, h, xerr=0, yerr=yerr, fmt=\",\")\n",
    "plt.ylim(0, None);\n",
    "\n",
    "# Plot spline\n",
    "x = np.linspace(start_mjd[0], stop_mjd[-1], 200)\n",
    "y = rate_spline(x)\n",
    "plt.plot(x, y, zorder=5, lw=2, color=\"k\", label=\"w=h/err\")\n",
    "\n",
    "# Plot weighted average. Weights are variance to resemble stddev weighted\n",
    "# least squares fit\n",
    "avg = np.average(h, weights=yerr**2)\n",
    "plt.axhline(avg, 0, 1, color=\"k\", ls=\"--\", zorder=5)\n",
    "\n",
    "# Plot unweighted mean and spline\n",
    "y = rate_spline_unw(x)\n",
    "plt.plot(x, y, zorder=5, lw=2, color=\"r\")\n",
    "\n",
    "avg = np.mean(h)\n",
    "plt.axhline(avg, 0, 1, color=\"r\", ls=\"--\", zorder=5, label=\"unweighted\")\n",
    "\n",
    "# Plot with inverse weights\n",
    "y = rate_spline_inv(x)\n",
    "plt.plot(x, y, zorder=5, lw=2, color=\"g\", label=\"w=err\")\n",
    "avg = np.average(h, weights=1. / yerr**2)\n",
    "plt.axhline(avg, 0, 1, color=\"g\", ls=\"--\", zorder=5)\n",
    "\n",
    "plt.xlim(start_mjd[0], stop_mjd[-1])\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"./data/figs/time_rate_splines.png\", dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Periodic function fit\n",
    "\n",
    "Try a peridoc function with a weighted least squares fit.\n",
    "\n",
    "$$\n",
    "    f(x) = a\\cdot \\sin(b\\cdot(x - c)) + d\n",
    "$$\n",
    "\n",
    "The least squares loss function is\n",
    "\n",
    "$$\n",
    "    R = \\sum_i (w_i(y_i - f(x_i)))^2\n",
    "$$\n",
    "\n",
    "Weights are standard deviations from poisson histogram error.\n",
    "\n",
    "$$\n",
    "    w_i = \\frac{1}{\\sigma_i}\n",
    "$$\n",
    "\n",
    "Seed values are estimated from plot rate vs time.\n",
    "Period should be 365 days (MJD) because we have one year of data so we choose $b0 = 2\\pi/365$.\n",
    "Amplitude is about $a_0=-0.0005$, because sinus seems to start with negative values.\n",
    "The x-offset is choose as the first start date, to get the right order of magnitude.\n",
    "The y-axis intersection $d$ schould be close to the weighted average, so we take this as a seed.\n",
    "\n",
    "The bounds are motivated as follows (and if we don't hit them, it's OK to use them).\n",
    "Amplitude $a$ should be positive, this also resolves a degenracy between a-axis offset.\n",
    "The period $b$ should scatter around one year, a period larger than +-1 half a year is unphysical.\n",
    "The x-offset $c$ cannot be greater than the initial +- the period because we have a periodic function.\n",
    "The y-axis offset $d$ is arbitrarily constrained, but as seen from the plot it should not exceed 0.1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def f(x, args):\n",
    "    a, b, c, d = args\n",
    "    return a * np.sin(b * (x - c)) + d\n",
    "\n",
    "def lstsq(pars, *args):\n",
    "    \"\"\"\n",
    "    Weighted leastsquares min sum((wi * (yi - fi))**2)\n",
    "    \"\"\"\n",
    "    # data x,y-values and weights are fixed\n",
    "    x, y, w = args[0], args[1], args[2]\n",
    "    # Params get fitted\n",
    "    a, b, c, d = pars[0], pars[1], pars[2], pars[3]\n",
    "    # Target function\n",
    "    f = a * np.sin(b * (x - c)) + d\n",
    "    # Least squares loss\n",
    "    return np.sum((w * (y - f))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Seed values from consideration above.\n",
    "a0 = -0.0005\n",
    "b0 = 2. * np.pi / 365.\n",
    "c0 = np.amin(start_mjd)\n",
    "d0 = np.average(h, weights=yerr**2)\n",
    "\n",
    "x0 = [a0, b0, c0, d0]\n",
    "# Bounds as explained above\n",
    "bounds = [[None, None], [0.5 * b0, 1.5 * b0], [c0 - b0, c0 + b0, ], [0, 0.01]]\n",
    "# x, y values, weights\n",
    "w = h / yerr  # Originally used 1 / yerr\n",
    "args = (binmids, h, w)\n",
    "\n",
    "res = sco.minimize(fun=lstsq, x0=x0, args=args, bounds=bounds)\n",
    "\n",
    "for i, name in enumerate([\"Amplitude a\", \"Period b\", \"x-Shift c\", \"y-axis d\"]):\n",
    "    print(name, \" : \", res.x[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Plot runs\n",
    "xerr = 0.5 * (stop_mjd - start_mjd)\n",
    "plt.errorbar(binmids, h, xerr=0, yerr=yerr, fmt=\",\")\n",
    "plt.ylim(0, None);\n",
    "\n",
    "# Plot fit\n",
    "pars = res.x\n",
    "x = np.linspace(start_mjd[0], stop_mjd[-1], 1000)\n",
    "y = f(x, pars)\n",
    "plt.plot(x, y, zorder=5)\n",
    "\n",
    "# Plot y shift dashed to see baseline or years average\n",
    "plt.axhline(res.x[3], 0, 1, color=\"C1\", ls=\"--\")\n",
    "\n",
    "plt.xlim(start_mjd[0], stop_mjd[-1])\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig(\"./data/figs/time_rate_sinus.png\", dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Combine both to make a time-dec rate function\n",
    "\n",
    "Multiply the rate function of time with the pdf in sinDec.\n",
    "This gives the rate per solid angle.\n",
    "Integrated over the whole sphere, we recover the total rate that time.\n",
    "Integrating further over the whole time range, regarding the deadtimes of the detector, we recover the number of total events in all runs in this sample.\n",
    "We can approximate this by using the fitted y-axis offset, which is approximatly the mean and multiply with the livetitme.\n",
    "We recover the number of total events to good approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(\"Number of events from approx : \", res.x[3] * livetime * secinday)\n",
    "print(\"True number of events        : \", len(exp))\n",
    "\n",
    "# Simply integrating doesn't respect the downtimes\n",
    "wrong = scint.quad(f, start_mjd[0], stop_mjd[-1], args=res.x)[0] * secinday\n",
    "print(\"Integrating over whole year  : \", wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Function of time, sinDec and right-ascension to get the rate at that point.\n",
    "def time_sinDec_rate(sinDec, t):\n",
    "    return sinDec_pdf(sinDec) * f(t, res.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    print(sinDec_pdf)\n",
    "except NameError:\n",
    "    raise NameError(\"Execute 'Splinefit to sinDec' cell to get needed functions\")\n",
    "\n",
    "# This should yield ~1. The ratio of the fitted average d and the integral\n",
    "# of the rate function over the whole sky at a time approximately at rate=d\n",
    "_i = 2. * np.pi * scint.quad(time_sinDec_rate, -1, 1, args=55700)[0] / res.x[3]\n",
    "print(\"1D and mukltiply by 2pi : \",_i)\n",
    "\n",
    "# We can also use a 2D integrator to integrate RA as well (same result)\n",
    "def fullsky_rate(ra, sinDec, t):\n",
    "    return sinDec_pdf(sinDec) * f(t, res.x)\n",
    "_i = scint.dblquad(fullsky_rate, -1, 1, lambda x: 0, lambda x: 2.*np.pi,\n",
    "                   args=(55700,))[0] / res.x[3]\n",
    "print(\"2D over dec and ra      : \", _i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Sampling the number of BG events to inject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Testing if the integration of the rate function gives the correct number of events to inject.\n",
    "For small time windows it should be the same as just taking the rate times the window size (rectangular approximation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def rate_integral(trange, pars):\n",
    "        \"\"\"\n",
    "        Match with factor [secinday] = 24 * 60 * 60 s / MJD = 86400/(Hz*MJD)\n",
    "        in the last step.\n",
    "            [a], [d] = Hz, [b], [c], [ti] = MJD\n",
    "            [a / b] = Hz * MJD, [d * (t1 - t0)] = HZ * MJD\n",
    "        \"\"\"\n",
    "        a, b, c, d = pars\n",
    "        \n",
    "        t0 = np.atleast_2d(trange[:, 0]).reshape(len(trange), 1)\n",
    "        t1 = np.atleast_2d(trange[:, 1]).reshape(len(trange), 1)\n",
    "        \n",
    "        per = a / b * (np.cos(b * (t0 - c)) - np.cos(b * (t1 - c)))\n",
    "        lin = d * (t1 - t0)\n",
    "\n",
    "        return (per + lin) * secinday\n",
    "\n",
    "\n",
    "def get_num_of_bg_events(t, trange, ntrials, pars):\n",
    "    \"\"\"\n",
    "    Draw number of background events per trial from a poisson distribution\n",
    "    with the mean of the fitted rate function.\n",
    "    Then draw nevents times via rejection sampling for the time dpeendent rate\n",
    "    function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    t : array-like\n",
    "        Times of the occurance of each source event in MJD.\n",
    "    trange : [float, float] or array_like, shape (len(t), 2)\n",
    "        Time window(s) in seconds relativ to the given time(s) t.\n",
    "        - If [float, float], the same window [lower, upper] is used for every\n",
    "          source.\n",
    "        - If array-like, lower [i, 0] and upper [i, 1] bounds of the time\n",
    "          window per source.\n",
    "    ntrials : int\n",
    "        Number of background trials we need the number of how many events to\n",
    "        inject for.\n",
    "    pars : array-like\n",
    "        Best fit parameters from the fit function used in its integral.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    sample : list, length len(t)\n",
    "        Contains a dict with keys \"times\" and \"nevents\"\n",
    "    times : \n",
    "    nevents : array-like, shape (len(t), ntrials)\n",
    "        The number of events to inject for each trial for each source.\n",
    "    \"\"\"\n",
    "    t = np.array(t)\n",
    "    trange = np.array(trange)\n",
    "    nsrc = len(t)\n",
    "    \n",
    "    # Make shape (nsources, 1) for the times\n",
    "    t = t.reshape(nsrc, 1)\n",
    "    \n",
    "    # If range is 1D (one for all) reshape it to (nsources, 2)\n",
    "    if len(trange.shape) == 1:\n",
    "        print(\"Using the same time window for all sources.\")\n",
    "        trange = np.repeat(trange.reshape(1, 2), repeats=nsrc, axis=0)\n",
    "        \n",
    "    # Prepare time window in MJD\n",
    "    trange = t + trange / secinday\n",
    "    \n",
    "    # Expectation is the integral in the time frame\n",
    "    expect = rate_integral(trange, pars)\n",
    "        \n",
    "    # Sample from poisson\n",
    "    nevts = np.random.poisson(lam=expect, size=(nsrc, ntrials))\n",
    "    \n",
    "    return nevts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "t = start_mjd[100:104]\n",
    "trange = np.array([-120, 220])\n",
    "ntrials = 10\n",
    "\n",
    "nevts = get_num_of_bg_events(t=t, trange=trange, ntrials=ntrials, pars=res.x)\n",
    "print(\"Events to inject:\\n\", nevts)\n",
    "\n",
    "# Test if integral and simple approximation is the same for small time frames\n",
    "t = np.array(t)\n",
    "trange = np.array(trange)\n",
    "t = t.reshape(len(t), 1)\n",
    "trange = np.repeat(trange.reshape(1, 2), repeats=len(t), axis=0)\n",
    "trange = t + trange / secinday\n",
    "rate = f(t, res.x)\n",
    "nevts = rate_integral(trange, res.x)\n",
    "print(\"\\nFor small windows, rate * dt should be equal to integral\")\n",
    "print(\"Time window dt in seconds:\\n\", np.diff(trange))\n",
    "print(\"Rate * dt:\\n\", rate * np.diff(trange, axis=1) * secinday)\n",
    "print(\"Integral:\\n\", nevts)\n",
    "print(np.allclose(rate * np.diff(trange, axis=1) * secinday, nevts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Time PDF ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Background in uniformly distributed in the time window.\n",
    "Signal distribtution is falling off gaussian-like at both edges so normalization is different.\n",
    "So the ratio S/B is simply the the signal pdf divided by the uniform normalization $1 / (t_1 - t_0)$ in the time frame.\n",
    "\n",
    "To get finite support we truncate the gaussian edges at n sigma.\n",
    "Though arbitrarliy introducet to smoothly run to zero, the concrete cutoff of the doesn't really matter (so say 4, 5, 6 sigma, etc).\n",
    "This is because in the LLH we get the product of $\\langle b_B \\rangle B_i$.\n",
    "A larger cutoff make the normalization of the BG pdf larger, but in the same time makes the number of expected BG event get higher in the same linear fashion.\n",
    "So as long as we choose a cutoff which ensures that $S \\approx 0$ outside, we're good to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "secinday = 24. * 60. * 60.\n",
    "\n",
    "def time_bg_pdf(t, t0, a, b):\n",
    "    \"\"\"\n",
    "    BG is uniform for t in [t0 + a, t0 + b] and 0 outside.\n",
    "    \n",
    "    Times t and t0 are given in MJD, the range is given relative to t0\n",
    "    in seconds. t are the times we return pdf values for, t0 is the time of\n",
    "    the source event around which the time frame is defined.\n",
    "    \n",
    "    The PDF is normed to time in seconds!\n",
    "    \"\"\"\n",
    "    # Normalize relative to t0 in seconds (first multiply avoids rounding?)\n",
    "    _t = t * secinday - t0 * secinday\n",
    "  \n",
    "    pdf = np.zeros_like(_t, dtype=np.float)\n",
    "    uni = (_t >= a) & (_t <= b)\n",
    "    pdf[uni] = 1. / (b - a)\n",
    "    return pdf\n",
    "\n",
    "def time_sig_pdf(t, t0, dt, nsig=4):\n",
    "    \"\"\"\n",
    "    Signal falls of with gaussian with sigma = dt outside uniform range dt.\n",
    "    \n",
    "    Times t, t0 are in MJD, dt is in seconds.\n",
    "    t are the times we return pdf values for, t0 is the time of the source\n",
    "    event around which the time frame is defined.\n",
    "    dt is the time window starting from t0 in which signal is uniform.\n",
    "    \n",
    "    The PDF is normed to time in seconds!\n",
    "    \"\"\"\n",
    "    if dt < 0:\n",
    "        raise ValueError(\"dt must not be negative.\")\n",
    "\n",
    "    # Normalize relative to t0 in seconds (first multiply avoids rounding?)\n",
    "    _t = t * secinday - t0 * secinday\n",
    "    \n",
    "    # Constrain sig_t to [2, 30]s regardless of uniform time window\n",
    "    sig_t = np.clip(dt, 2, 30)\n",
    "    sig_t_clip = nsig * sig_t\n",
    "    gaus_norm = (np.sqrt(2 * np.pi) * sig_t)\n",
    "    \n",
    "    # Split in def regions gaus rising, uniform, gaus falling and zero\n",
    "    gr = (_t < 0) & (_t >= -sig_t_clip)\n",
    "    gf = (_t > dt) & (_t <= dt + sig_t_clip)\n",
    "    uni = (_t >= 0) & (_t <= dt)\n",
    "    \n",
    "    pdf = np.zeros_like(t, dtype=np.float)\n",
    "    pdf[gr] = scs.norm.pdf(_t[gr], loc=0, scale=sig_t)\n",
    "    pdf[gf] = scs.norm.pdf(_t[gf], loc=dt, scale=sig_t)\n",
    "    # Connect smoothly with the gaussians\n",
    "    pdf[uni] = 1. / gaus_norm\n",
    "    \n",
    "    # Normalize whole distribtuion\n",
    "    dcdf = (scs.norm.cdf(dt + sig_t_clip, loc=dt, scale=sig_t) -\n",
    "            scs.norm.cdf(-sig_t_clip, loc=0., scale=sig_t))\n",
    "    norm = dcdf + dt / gaus_norm\n",
    "    \n",
    "    return pdf / norm\n",
    "\n",
    "def time_soverb(t, t0, dt, nsig):\n",
    "    \"\"\"\n",
    "    Time signal over background PDF.\n",
    "    \n",
    "    Signal and background PDFs are each normalized over seconds.\n",
    "    Signal PDF has gaussian edges to smoothly let it fall of to zero, the\n",
    "    stddev is dt when dt is in [2, 30]s, otherwise the nearest edge.\n",
    "\n",
    "    To ensure finite support, the edges are truncated after nsig * dt.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    t : array-like\n",
    "        Times given in MJD for which we want to evaluate the ratio.\n",
    "    t0 : float\n",
    "        Time of the source event.\n",
    "    dt : float\n",
    "        Time window in seconds starting from t0 in which the signal pdf is\n",
    "        assumed to be uniform. Must not be negative.\n",
    "    nsig : float\n",
    "        Clip the gaussian edges at nsig * dt\n",
    "    \"\"\"\n",
    "    if dt < 0:\n",
    "        raise ValueError(\"dt must not be negative.\")\n",
    "\n",
    "    secinday = 24. * 60. * 60.\n",
    "\n",
    "    # Normalize relative to t0 in seconds (first multiply avoids rounding?)\n",
    "    _t = t * secinday - t0 * secinday\n",
    "   \n",
    "    # Create signal PDF\n",
    "    # Constrain sig_t to [2, 30]s regardless of uniform time window\n",
    "    sig_t = np.clip(dt, 2, 30)\n",
    "    sig_t_clip = nsig * sig_t\n",
    "    gaus_norm = (np.sqrt(2 * np.pi) * sig_t)\n",
    "    \n",
    "    # Split in def regions gaus rising, uniform, gaus falling\n",
    "    gr = (_t < 0) & (_t >= -sig_t_clip)\n",
    "    gf = (_t > dt) & (_t <= dt + sig_t_clip)\n",
    "    uni = (_t >= 0) & (_t <= dt)\n",
    "    \n",
    "    pdf = np.zeros_like(t, dtype=np.float)\n",
    "    pdf[gr] = scs.norm.pdf(_t[gr], loc=0, scale=sig_t)\n",
    "    pdf[gf] = scs.norm.pdf(_t[gf], loc=dt, scale=sig_t)\n",
    "    # Connect smoothly with the gaussians\n",
    "    pdf[uni] = 1. / gaus_norm\n",
    "    \n",
    "    # Normalize signal distribtuion\n",
    "    dcdf = (scs.norm.cdf(dt + sig_t_clip, loc=dt, scale=sig_t) -\n",
    "            scs.norm.cdf(-sig_t_clip, loc=0., scale=sig_t))\n",
    "    norm = dcdf + dt / gaus_norm\n",
    "    pdf /= norm\n",
    "    \n",
    "    # Calculate the ratio\n",
    "    bg_pdf = 1. / (dt + 2 * sig_t_clip)\n",
    "    ratio = pdf / bg_pdf\n",
    "    return ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Plot a the signal and BG PDFs for a single case\n",
    "# Arbitrary start date from data\n",
    "t0 = start_mjd[100]\n",
    "t0_sec = t0 * secinday\n",
    "\n",
    "# dt from t0 in seconds, clip at 4 sigma\n",
    "dt = 10\n",
    "nsig = 4.\n",
    "\n",
    "# Make t values for plotting in MJD around t0\n",
    "clip = np.clip(dt, 2, 30) * nsig\n",
    "plt_rng = [-clip, dt + clip]\n",
    "t = np.linspace(t0_sec + plt_rng[0], t0_sec + plt_rng[1], 200) / secinday\n",
    "\n",
    "bg_pdf = time_bg_pdf(t, t0, -clip, dt + clip)\n",
    "sig_pdf = time_sig_pdf(t, t0, dt, nsig)\n",
    "\n",
    "# Plot in normalized time\n",
    "_t = t * secinday - t0 * secinday\n",
    "plt.plot(_t, bg_pdf, \"C0-\")\n",
    "plt.plot(_t, sig_pdf, \"C1-\")\n",
    "plt.axvline(dt, 0, 1, color=\"C7\", ls=\"--\")\n",
    "plt.axvline(0, 0, 1, color=\"C1\", ls=\"--\")\n",
    "\n",
    "plt.xlabel(\"Time relative to t0 in sec\")\n",
    "plt.ylim(0, None);\n",
    "plt.show()\n",
    "\n",
    "# Integrate both pdf over time range to show they are correctly normalized\n",
    "# Note that PDFs are defined in second so we multiply by secinday \n",
    "bg_int = scint.quad(time_bg_pdf, t[0], t[-1],\n",
    "                    args=(t0, -clip, dt + clip))[0] * secinday\n",
    "sig_int = scint.quad(time_sig_pdf, t[0], t[-1],\n",
    "                    args=(t0, dt, nsig))[0] * secinday\n",
    "\n",
    "print(\"BG integral     : \", bg_int)\n",
    "print(\"Signal integral : \", sig_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Make a plot with ratios for different time windows as in the paper\n",
    "# Arbitrary start date from data\n",
    "t0 = start_mjd[100]\n",
    "t0_sec = t0 * secinday\n",
    "\n",
    "# dt from t0 in seconds, clip at 4 sigma\n",
    "dts = [5, 50, 200]\n",
    "nsig = 4\n",
    "\n",
    "# Make t values for plotting in MJD around t0, fitting all in one plot\n",
    "max_dt = np.amax(dts)\n",
    "clip = np.clip(max_dt, 2, 30) * nsig\n",
    "plt_rng = np.array([-clip, max_dt + clip])\n",
    "t = np.linspace(t0_sec + 1.2 *plt_rng[0],\n",
    "                t0_sec + 1.2 * plt_rng[1], 1000) / secinday\n",
    "_t = t * secinday - t0 * secinday\n",
    "\n",
    "# Mark event time\n",
    "plt.axvline(0, 0, 1, c=\"#353132\", ls=\"--\", lw=2)\n",
    "\n",
    "colors = [\"C0\", \"C3\", \"C2\"]\n",
    "for i, dt in enumerate(dts):\n",
    "    # Plot ratio S/B\n",
    "    SoB = time_soverb(t, t0, dt, nsig)\n",
    "    plt.plot(_t, SoB, lw=2, c=colors[i],\n",
    "             label=r\"$T_\\mathrm{{uni}}$: {:>3d}s\".format(dt))\n",
    "    # Fill uniform part, might look nicely\n",
    "    # fbtw = (_t > 0) & (_t < dt)\n",
    "    # plt.fill_between(_t[fbtw], 0, SoB[fbtw], color=\"C7\", alpha=0.1)\n",
    "\n",
    "# Make it look like the paper plot, but with slightly extended borders, to\n",
    "# nothing breaks outside the total time frame\n",
    "plt.xlim(1.2 * plt_rng)\n",
    "plt.ylim(0, 3)\n",
    "plt.xlabel(\"t - t0 in sec\")\n",
    "plt.ylabel(\"S / B\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Declination bump in data a south"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "At declination -pi/2 there are a lot of events, that show up as a large spike in sin(dec).\n",
    "Where does this come from?\n",
    "Those events come directly from above (southern sky, zenith=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "bins = 20\n",
    "ev_dec = exp[\"dec\"]\n",
    "sin_dec = np.sin(ev_dec)\n",
    "\n",
    "fig, (axl, axr) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "_ = axl.hist(sin_dec, bins=200, normed=False)\n",
    "_ = axr.hist(np.rad2deg(ev_dec), bins=500, normed=False)\n",
    "\n",
    "axr.set_xlabel(\"sinDec\")\n",
    "axr.set_xlabel(\"Dec in °\")\n",
    "\n",
    "axl.arrow(x=-1, y=1000, dx=0, dy=-100, head_length=20, head_width=0.03, lw=2,\n",
    "          fc=\"C1\", ec=\"C1\")\n",
    "axr.arrow(x=-90, y=300, dx=0, dy=-200, head_length=20, head_width=3, lw=2,\n",
    "          fc=\"C1\", ec=\"C1\")\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Integrate Kent function in 2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The paraboloid sigma was gained from estimating a circular region, in which the 2D llh includes 39% of the total probability (Attention: chi2(1, df=2), because 2D gaussian behave differently).\n",
    "This assumes a gaussian llh function, so when using the usual gaussian in the signal pdf we can directly use this sigma estimate for the gaussian sigma.\n",
    "\n",
    "For a Kent distribtuions kappa however, it is unclear how both are connected.\n",
    "For starters, when kappa is 0, points are uniform on the sky, but fully concentrated in a gaussian and vice versa.\n",
    "We could simply assume $\\kappa = 1/\\sigma$ but this is mostly founded on the 1D Mises distribution which has this property for small sigma only.\n",
    "\n",
    "So here we try the following:\n",
    "\n",
    "- For a fixed sigma, try to find the corresponding kappa, so that when integrating the Kent PDF with this kappa in a circle with radius sigma, the probability content in that circle is 68%.\n",
    "\n",
    "The Kent PDF is given by:\n",
    "\n",
    "$$\n",
    "    f(\\vec{x}_i|\\vec{x}_S) = \\frac{\\kappa}{4\\pi\\sinh{\\kappa}}\\cdot\\exp(\\kappa(\\vec{x}_i\\cdot\\vec{x}_S))\n",
    "                           = \\frac{\\kappa}{4\\pi\\sinh{\\kappa}}\\cdot\\exp(\\kappa\\cos\\Psi)\n",
    "$$\n",
    "\n",
    "where $\\cos\\Psi$ is the angular distance of both vectors.\n",
    "\n",
    "Because we need to integrate over the surface of the unit sphere, we define the Kent PDF in terms of spherical coordinates as used in equatorial coordinates.\n",
    "\n",
    "$$\n",
    "    \\vec{x} = \\begin{pmatrix}\n",
    "                \\cos\\theta\\cos{\\varphi} \\\\ \\cos\\theta\\sin{\\varphi} \\\\ \\sin\\theta\n",
    "              \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "In the integration we must not forget to multiply with the functional determinant (surface element) $\\mathrm{d}A=-r^2\\cos\\varphi\\mathrm{d}\\theta\\mathrm{d}\\varphi$ for the convention used here.\n",
    "\n",
    "Without loss of generality we can assume that the mean vector (=src position) is pointing along the z-axis $(0,0,1)$.\n",
    "This way we have easy integration boundaries, the radius of the circle is then directly given by the polar angle $\\theta$:\n",
    "\n",
    "$$\n",
    "    \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} \\cdot\n",
    "      \\begin{pmatrix} \\cos\\theta\\cos{\\varphi} \\\\ \\cos\\theta\\sin{\\varphi} \\\\ \\sin\\theta \\end{pmatrix}\n",
    "    = \\sin\\theta\n",
    "$$\n",
    "\n",
    "so our full integral assembles to:\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathrm{Prob\\ Content\\ within\\ }\\sigma = \\alpha &= \\int_{\\theta=\\pi/2}^{\\pi/2 - \\sigma}\\int_{\\varphi=0}^{2\\pi}\n",
    "        \\frac{\\kappa}{4\\pi\\sinh{\\kappa}}\\cdot\\exp(\\kappa\\sin\\theta)\\cdot(-\\cos\\theta)\n",
    "        \\mathrm{d}\\theta\\mathrm{d}\\varphi \\\\\n",
    "      &= 2\\pi\\cdot\\frac{\\kappa}{4\\pi\\sinh{\\kappa}}\\cdot \\int_{\\theta=\\pi/2}^{\\pi/2 - \\sigma}\n",
    "        -\\cos\\theta\\exp(\\kappa\\sin\\theta)\\mathrm{d}\\theta\\mathrm{d}\\varphi \\\\\n",
    "      &= \\frac{\\kappa}{2\\sinh{\\kappa}}\\cdot \\int_{\\theta=\\pi/2 - \\sigma}^{\\pi/2}\n",
    "        \\cos\\theta\\exp(\\kappa\\sin\\theta)\\mathrm{d}\\theta\\mathrm{d}\\varphi\n",
    "\\end{align}\n",
    "\n",
    "We can solve this analytially:\n",
    "\n",
    "\\begin{align}\n",
    "    \\alpha &= \\frac{1}{2\\sinh{\\kappa}}\\cdot \\int_{\\theta=\\pi/2 - \\sigma}^{\\pi/2}\n",
    "               \\kappa\\cos\\theta\\exp(\\kappa\\sin\\theta)\\mathrm{d}\\theta\n",
    "            = \\frac{1}{2\\sinh{\\kappa}}\\cdot \\left[\\exp(\\kappa\\sin\\theta)\\right]_{\\theta=\\pi/2 - \\sigma}^{\\pi/2} \\\\\n",
    "            &= \\frac{1}{2\\sinh{\\kappa}}\\cdot \\left[\\exp(\\kappa) - \\exp(\\kappa\\cos(\\sigma))\\right]\n",
    "\\end{align}\n",
    "\n",
    "This automatically reduces to $\\sinh\\kappa / \\sinh\\kappa = 1$ for $\\sigma=180°$ and to zero for $\\sigma=0°$ as expected.\n",
    "\n",
    "The special case $\\kappa=0$ resembles a uniform distribtuion on the sky and is given by:\n",
    "\n",
    "\\begin{align}\n",
    "    \\alpha &= \\int_{\\theta=\\pi/2 - \\sigma}^{\\pi/2}\\int_{\\varphi=0}^{2\\pi}\n",
    "               \\frac{\\cos\\theta}{4\\pi} \\mathrm{d}\\theta\\mathrm{d}\\varphi\n",
    "           =\\frac{1}{2}\\int_{\\theta=\\pi/2 - \\sigma}^{\\pi/2}\\cos\\theta\\mathrm{d}\\theta \\\\\n",
    "           &= \\frac{\\sin(\\pi/2) - \\sin(\\pi/2-\\sigma)}{2}\n",
    "           = \\frac{1 - \\cos\\sigma}{2}\n",
    "\\end{align}\n",
    "\n",
    "Below we run some tests up to which events sigma we can go with the Kent distribution.\n",
    "Also we see, if it is justified to identify $\\kappa = 1/\\sigma**2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Quickly see that the pdf itself is normalized.\n",
    "# Note: Integral is in cos(theta), so no area element needed.\n",
    "# For very large kappas this breaks because the stepsize is too coarse\n",
    "def S(cosDist, kappa):\n",
    "    return (kappa / (2. * np.pi * (1. - np.exp(-2. * kappa))) *\n",
    "                 np.exp(kappa * (cosDist - 1. )))\n",
    "\n",
    "sig = np.deg2rad(2)\n",
    "kappa = 1 / sig**2\n",
    "scint.quad(S, a=3*sig, b=1, args=(kappa))[0] * 2 * np.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# First let's get the 1 sigma gaussian probability content to compare against\n",
    "one_sig = scs.norm.cdf(1) - scs.norm.cdf(-1)\n",
    "\n",
    "def alpha_in_kent_stable(kappa, sigma):\n",
    "    \"\"\"\n",
    "    moxe is a numeric god :+1:\n",
    "    \"\"\"\n",
    "    sigma = np.atleast_1d(sigma)\n",
    "    if np.any(sigma > np.pi):\n",
    "        raise ValueError(\"Angular error greater than pi.\")\n",
    "    if kappa == 0:\n",
    "        return 0.5 * (1. - np.cos(sigma))\n",
    "\n",
    "    a = 1. / np.tanh(kappa) + 1.\n",
    "    b = 0.5 * (1. - np.exp(kappa * (np.cos(sigma) - 1.)))\n",
    "    return a * b\n",
    "\n",
    "def alpha_in_kent(kappa, sigma):\n",
    "    sigma = np.atleast_1d(sigma)\n",
    "    if np.any(sigma > np.pi):\n",
    "        raise ValueError(\"Angular error greater than pi.\")\n",
    "    if kappa == 0:\n",
    "        return 0.5 * (1. - np.cos(sigma))\n",
    "    \n",
    "    return 0.5 * (np.exp(kappa) - np.exp(kappa * np.cos(sigma))) / np.sinh(kappa)\n",
    "\n",
    "def kappa_from_sigma(sigma, alpha=0.39):\n",
    "    # Wrapper because we need the point alpha(k) - alpha = 0\n",
    "    def fun(kappa):\n",
    "        return alpha_in_kent_stable(kappa, sigma) - alpha\n",
    "    \n",
    "    x0 = float(1. / sigma**2)\n",
    "    return sco.newton(fun, x0=x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Up to ~40° kappa = 1/sigma**2 is OK to use. Then the norm kicks in\n",
    "sigmas = np.linspace(0.1, 150, 1000)\n",
    "alpha = scs.chi2.cdf(1**2, df=2)\n",
    "\n",
    "kappas = np.zeros_like(sigmas)\n",
    "for i, sig in enumerate(sigmas):\n",
    "    kappas[i] = kappa_from_sigma(np.deg2rad(sig), alpha=alpha)\n",
    "    \n",
    "# Simple subtitution as in mrichmans thesis\n",
    "subst = 1. / np.deg2rad(sigmas)**2\n",
    "\n",
    "plt.plot(sigmas, kappas, label=r\"Exact\")\n",
    "plt.plot(sigmas, subst, label=r\"$\\kappa = 1/\\sigma^2$\")\n",
    "\n",
    "plt.xlabel(\"sigma in °\")\n",
    "plt.ylabel(r\"$\\kappa$\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "\n",
    "# plt.savefig(\"data/figs/kent_kappa.png\", dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Different view of the plot above, now in terms of different kappas\n",
    "sigmas = np.linspace(0, 20, 360 + 1)\n",
    "sigmas_rad = np.deg2rad(sigmas)\n",
    "kappas = np.arange(10, 200, 20)\n",
    "\n",
    "alpha = scs.chi2.cdf(1**2, df=2)  # ca. 39%, 2D gaussian\n",
    "plt.axhline(1, 0, 1, color=dg, ls=\"--\")\n",
    "plt.axhline(alpha, 0, 1, color=dg, ls=\"--\")\n",
    "for k in kappas:\n",
    "    a = alpha_in_kent(k, sigmas_rad)\n",
    "    plt.plot(sigmas, a, label=r\"$\\kappa = {:.1f}$\".format(k))\n",
    "\n",
    "# Note: if evt sigma is larger than ~80°, we can't describe the correct\n",
    "#       probability content with a Kent distribution. An artifact from the\n",
    "#       LLH beeing not gaussian enough.\n",
    "plt.xlabel(\"Evt sigma in °\")\n",
    "plt.ylabel(\"Prob. in sigma\")\n",
    "plt.xlim(0, 20)\n",
    "plt.ylim(0, 1.05)\n",
    "plt.legend()\n",
    "plt.title(\"CDF of symmetric Kent distribution\")\n",
    "\n",
    "# plt.savefig(\"data/figs/kent_cdf.png\", dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Now directly compare Kent to 2D gaussian (provided by mrichman :+1:)\n",
    "# For sigmas near the turnover ~75°, we can see, that the Kent line crosses\n",
    "# the gaussian. It smoothes out more to get exactly the 39% at the sigma level\n",
    "mth = 90\n",
    "theta = np.linspace (0, np.deg2rad(mth), 1000)\n",
    "sig_deg = [3, 5, 10, 20, 30, 75]\n",
    "\n",
    "cmap = plt.cm.get_cmap(\"plasma\")\n",
    "\n",
    "for sd in sig_deg:\n",
    "    sr = np.radians(sd)\n",
    "    # kappa = 1 / sr**2\n",
    "    kappa = kappa_from_sigma(sigma=sr, alpha=scs.chi2.cdf(1**2, df=2))\n",
    "    G = np.exp(-theta**2 / (2 * sr**2)) / (2. * np.pi * sr**2)\n",
    "    K = kappa / (4. * np.pi * np.sinh(kappa)) * np.exp(kappa * np.cos(theta))\n",
    "    color = cmap((sd - min(sig_deg)) / max(sig_deg))\n",
    "    plt.plot(np.rad2deg(theta), K, color=color,\n",
    "             label=r'$\\sigma={}^\\circ$'.format (sd))\n",
    "    plt.plot(np.rad2deg(theta), G, ls='--', color=color)\n",
    "\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel (r'$\\Delta\\Psi~[^\\circ]$')\n",
    "plt.ylabel (r'PDF')\n",
    "plt.xlim (0, mth)\n",
    "plt.ylim (1e-3, 1e3)\n",
    "plt.legend (loc='best', ncol=2, prop={\"size\": 'small'}, title=\"Kent\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig(\"data/figs/kent_vs_gaus.png\", dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Tests for the KDE integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Marginalize KDE by integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Instead of sampling and reducing to 2D histograms, we can try to truly integrate one dimension of the KDE to be able to plot also the tails of the distribution, where events usually end up only in large samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Assign model from CV, which has already evaluated adaptive kernels\n",
    "kde_inj = json2kde(\"data/awKDE_CV/CV10_glob_bw_alpha_EXP_IC86I_\" +\n",
    "                   \"CUT_sig.ll.20_PARS_diag_True_pass2.json\")\n",
    "\n",
    "# Generate some BG samples to compare to the original data hist\n",
    "nsamples_kde = int(1e7)\n",
    "bg_samples = kde_inj.sample(nsamples_kde)\n",
    "\n",
    "# Plot in degrees and in sinDec\n",
    "bg_samples = np.vstack((bg_samples[:, 0],\n",
    "                        np.sin(bg_samples[:, 1]),\n",
    "                        np.rad2deg(bg_samples[:, 2]))).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# NOTE: The scaled version are super old and used a fixed width sklearn KDE\n",
    "#       with mahual scaling of the input space. These are the scale factors:\n",
    "fac_logE = 1.5\n",
    "fac_dec = 2.5\n",
    "fac_sigma = 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# KDE CV is running on cluster and pickles the GridSearchCV\n",
    "fname = \"./data/kde_cv/KDE_model_selector_20_exp_IC86_I_followup_2nd_pass.pickle\"\n",
    "with open(fname, \"rb\") as f:\n",
    "    model_selector = pickle.load(f)\n",
    "\n",
    "kde = model_selector.best_estimator_\n",
    "bw = model_selector.best_params_[\"bandwidth\"]\n",
    "print(\"Best bandwidth : {:.3f}\".format(bw))\n",
    "\n",
    "# We maybe just want to stick with the slightly overfitting kernel to\n",
    "# be as close as possible to data\n",
    "OVERFIT = True\n",
    "if OVERFIT:\n",
    "    bw = 0.075\n",
    "    kde = skn.KernelDensity(bandwidth=bw, kernel=\"gaussian\", rtol=1e-8)\n",
    "print(\"Used bandwidth : {:.3f}\".format(bw))\n",
    "\n",
    "# KDE sample must be cut in sigma before fitting, similar to range in hist\n",
    "_exp = exp[exp[\"sigma\"] <= np.deg2rad(5)]\n",
    "\n",
    "fac_logE = 1.5\n",
    "fac_dec = 2.5\n",
    "fac_sigma = 2.\n",
    "\n",
    "_logE = fac_logE * _exp[\"logE\"]\n",
    "_sigma = fac_sigma * np.rad2deg(_exp[\"sigma\"])\n",
    "_dec = fac_dec * _exp[\"dec\"]\n",
    "\n",
    "kde_sample = np.vstack((_logE, _dec, _sigma)).T\n",
    "\n",
    "# Fit KDE best model to sample\n",
    "kde.fit(kde_sample)\n",
    "\n",
    "# Make some samples\n",
    "nsamples_kde = int(1e7)\n",
    "bg_samples = kde.sample(n_samples=nsamples_kde)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 1D case\n",
    "\n",
    "Integrate out 2 axis with a double integral to show a 1D margin distribution.\n",
    "This take super long, 5 Minutes per point.\n",
    "But it can be parallelized pretty simple if needed.\n",
    "Code to create the values is on phobos.\n",
    "\n",
    "**Resumee:** Sampling many values and simply bin in 1D is much better, needs less time and is probably more accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Compare to data hist\n",
    "_ = plt.hist(exp[\"logE\"], bins=100, normed=True, label=\"data\")\n",
    "\n",
    "# Compare 'true' integration with\n",
    "h, b = np.histogram(bg_samples[:, 0], bins=200, range=[2, 6], normed=True)\n",
    "_ = plt.plot(get_binmids(b), h, label=\"sample\")\n",
    "\n",
    "bins_and_vals = np.load(\"data/2d_integrate_kde_SCALED/bins_and_vals.npy\")\n",
    "x = bins_and_vals[0] / fac_logE\n",
    "vals = bins_and_vals[1]\n",
    "_ = plt.plot(x, vals, label=\"integrated\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 2D PDF\n",
    "\n",
    "Integrate out only one axis (here sigma) to show the 2D marginalized PDF.\n",
    "Executed on phobos, takes a little while on 50x50 grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Get precalculated integral data\n",
    "bins = list(np.load(\"data/1d_integrate_kde_SCALED/logE_sinDec_bins_50x50.npy\"))\n",
    "vals = np.load(\"data/1d_integrate_kde_SCALED/logE_sinDec_int_50x50.npy\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Compare to data hist\n",
    "_ = ax[0].hist2d(exp[\"logE\"], np.sin(exp[\"dec\"]), bins=bins, normed=True)\n",
    "\n",
    "# Compare 'true' integration\n",
    "mids = get_binmids(bins)\n",
    "xx, yy  = map(np.ravel, np.meshgrid(mids[0], mids[1]))\n",
    "_ = ax[1].hist2d(xx, yy, bins=bins, weights=vals)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Justify the sigma cut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Only few higher energy events from the sothern sky are excluded (see cut=10).\n",
    "But really bad reconstructed events tend to have higher energies (see cut=90).\n",
    "Still it should be OK to remove those > 10 because they have not so much spatial information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Show the leftover event s after a sigma cut\n",
    "sig_cut = 10\n",
    "m = exp[\"sigma\"] > np.deg2rad(sig_cut)\n",
    "\n",
    "_ = plt.hist2d(exp[\"logE\"][m], np.rad2deg(exp[\"dec\"][m]),\n",
    "               bins=30, cmap=\"inferno\")\n",
    "plt.colorbar()\n",
    "plt.title(\"Total Evts w sigma > {:d}°: {:d} ({:.3f}%)\".format(\n",
    "        sig_cut, np.sum(m), np.sum(m) / len(exp) * 100))\n",
    "plt.xlabel(\"logE\")\n",
    "plt.ylabel(\"dec in °\")\n",
    "plt.show()\n",
    "\n",
    "# Show the skewed sigma distribution with the cut applied and mean vs median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Test the marginalize_hist method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "It should be equivalent to use one of the following methods to create a 1D histogram from the original 3D data pdf in logE, dec and sigma:\n",
    "\n",
    "1. Simply use the original 1D data in any variable, e.g. simply histogram logE\n",
    "2. Create the complete 3D histogram and marginalize by summing over remaining dimensions.\n",
    "\n",
    "When using unnormalized hists, 2. is simply summing up all other counts.\n",
    "\n",
    "When using normalized hists, we need to sum with respect to the binwidths in the current dimension to keep the normalization intact.\n",
    "This is only useful, when only the histogram is available and not the original sample.\n",
    "\n",
    "We want to compare if both methods are equivalent\n",
    "As we can see, all ratios are one, so methods are equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def make_hist_ratio(h1, h2):\n",
    "    \"\"\"Return the ratio h1 / h2. Return 0 where h2 is 0.\"\"\"\n",
    "    m = (h2 > 0)\n",
    "    ratio = np.zeros_like(h1)\n",
    "    ratio[m] = h1[m] / h2[m]\n",
    "    return ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Unnormalized\n",
    "First the unnormalized version. Simply sum over the other axes of the 3D hist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Plot each variable in a single plot and the ratios seperately\n",
    "fig, [[axtl, axtr], [axbl, axbr]] = plt.subplots(2, 2, figsize=(10, 8))\n",
    "\n",
    "# We also make a cut < 10° in sigma, because there are some outliers\n",
    "m = exp[\"sigma\"] <= np.deg2rad(10)\n",
    "sigma = np.rad2deg(exp[\"sigma\"][m])\n",
    "logE = exp[\"logE\"][m]\n",
    "dec = np.sin(exp[\"dec\"][m])\n",
    "\n",
    "logE_nbins = 50\n",
    "dec_nbins = 40\n",
    "sigma_nbins = 30\n",
    "\n",
    "# Make the 3D hist\n",
    "sample = np.vstack((logE, dec, sigma)).T\n",
    "nbins = [logE_nbins, dec_nbins, sigma_nbins]\n",
    "h, b = np.histogramdd(sample, bins=nbins,)\n",
    "\n",
    "# Get binmids for plotting\n",
    "m = get_binmids(b)\n",
    "\n",
    "# Common hist settings\n",
    "h1 = {\"lw\": 2, \"color\": \"k\", \"histtype\": \"step\"}\n",
    "h2 = {\"lw\": 2, \"color\": \"r\", \"histtype\": \"step\", \"alpha\": 0.5}\n",
    "\n",
    "# logE\n",
    "logE_h, logE_b, _ = axtl.hist(logE, bins=logE_nbins, **h1)\n",
    "logE_hm = np.sum(h, axis=(1, 2))\n",
    "_ = axtl.hist(m[0], bins=b[0], weights=logE_hm, **h2)\n",
    "# Ratio plot below\n",
    "axtl_sec = split_axis(axtl, \"bottom\", \"20%\", cbar=False)\n",
    "axtl_sec.hist(m[0], b[0], weights=make_hist_ratio(logE_h, logE_hm), **h2)\n",
    "axtl_sec.axhline(1, 0, 1, color=\"k\")\n",
    "axtl_sec.set_ylim(0, 2)\n",
    "\n",
    "# dec\n",
    "dec_h, dec_b, _ = axbl.hist(dec, bins=dec_nbins, **h1)\n",
    "dec_hm = np.sum(h, axis=(0, 2))\n",
    "_ = axbl.hist(m[1], bins=b[1], weights=dec_hm, **h2)\n",
    "\n",
    "axbl_sec = split_axis(axbl, \"bottom\", \"20%\", cbar=None)\n",
    "axbl_sec.hist(m[1], b[1], weights=make_hist_ratio(dec_h, dec_hm), **h2)\n",
    "axbl_sec.axhline(1, 0, 1, color=\"k\")\n",
    "axbl_sec.set_ylim(0, 2)\n",
    "\n",
    "# sigma\n",
    "sigma_h, sigma_b, _ = axtr.hist(sigma, bins=sigma_nbins, **h1)\n",
    "sigma_hm = np.sum(h, axis=(0, 1))\n",
    "_ = axtr.hist(m[2], bins=b[2], weights=sigma_hm, **h2)\n",
    "\n",
    "axtr_sec = split_axis(axtr, \"bottom\", \"20%\", cbar=None)\n",
    "axtr_sec.hist(m[2], b[2], weights=make_hist_ratio(sigma_h, sigma_hm), **h2)\n",
    "axtr_sec.axhline(1, 0, 1, color=\"k\")\n",
    "axtr_sec.set_ylim(0, 2)\n",
    "\n",
    "axbr.set_visible(False)\n",
    "\n",
    "fig.suptitle(\"Black: 1D, Red: Margin\", fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Normalized\n",
    "Sum over the other axes of the 3D hist and multiply by bin widths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Plot each variable in a single plot and the ratios seperately\n",
    "fig, [[axtl, axtr], [axbl, axbr]] = plt.subplots(2, 2, figsize=(10, 8))\n",
    "\n",
    "# Now make it normed\n",
    "h, b = np.histogramdd(sample, bins=nbins, normed=True)\n",
    "\n",
    "# Get binmids for plotting\n",
    "m = get_binmids(b)\n",
    "\n",
    "# logE\n",
    "logE_h, logE_b, _ = axtl.hist(logE, bins=logE_nbins, normed=True, **h1)\n",
    "logE_hm = hist_marginalize(h=h, bins=b, axes=(1, 2))[0]\n",
    "_ = axtl.hist(m[0], bins=b[0], weights=logE_hm, **h2)\n",
    "# Ratio plot below\n",
    "axtl_sec = split_axis(axtl, \"bottom\", \"20%\", cbar=False)\n",
    "axtl_sec.hist(m[0], b[0], weights=make_hist_ratio(logE_h, logE_hm), **h2)\n",
    "axtl_sec.axhline(1, 0, 1, color=\"k\")\n",
    "axtl_sec.set_ylim(0, 2)\n",
    "\n",
    "# dec\n",
    "dec_h, dec_b, _ = axbl.hist(dec, bins=dec_nbins, normed=True, **h1)\n",
    "dec_hm = hist_marginalize(h=h, bins=b, axes=(0, 2))[0]\n",
    "_ = axbl.hist(m[1], bins=b[1], weights=dec_hm, **h2)\n",
    "\n",
    "axbl_sec = split_axis(axbl, \"bottom\", \"20%\", cbar=None)\n",
    "axbl_sec.hist(m[1], b[1], weights=make_hist_ratio(dec_h, dec_hm), **h2)\n",
    "axbl_sec.axhline(1, 0, 1, color=\"k\")\n",
    "axbl_sec.set_ylim(0, 2)\n",
    "\n",
    "# sigma\n",
    "sigma_h, sigma_b, _ = axtr.hist(sigma, bins=sigma_nbins, normed=True, **h1)\n",
    "sigma_hm = hist_marginalize(h=h, bins=b, axes=(0, 1))[0]\n",
    "_ = axtr.hist(m[2], bins=b[2], weights=sigma_hm, **h2)\n",
    "\n",
    "axtr_sec = split_axis(axtr, \"bottom\", \"20%\", cbar=None)\n",
    "axtr_sec.hist(m[2], b[2], weights=make_hist_ratio(sigma_h, sigma_hm), **h2)\n",
    "axtr_sec.axhline(1, 0, 1, color=\"k\")\n",
    "axtr_sec.set_ylim(0, 2)\n",
    "\n",
    "axbr.set_visible(False)\n",
    "\n",
    "fig.suptitle(\"Black: 1D, Red: Margin\", fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Mask sinDec logE ratio PDF spline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Assign each high E events for which no data but MC is present the maximum ratio from signal over background and all lowE the lowest ratio.\n",
    "\n",
    "In skylab all events which no data but MC events get the highest value, which is strange at the lowE regime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Prepare the MC data, signal weighted to astro unbroken power law\n",
    "gamma = 2.\n",
    "# No flux norm, because we normalize anyway\n",
    "mc_w = mc[\"ow\"] * mc[\"trueE\"]**(-gamma)\n",
    "\n",
    "# Make the MC hist. Use this binning for the data too\n",
    "mc_sindec = np.sin(mc[\"dec\"])\n",
    "mc_logE = mc[\"logE\"]\n",
    "bins = [50, 50]\n",
    "rnge = [[-1, 1], [1, 10]]\n",
    "mc_h, bx, by = np.histogram2d(mc_sindec, mc_logE, bins=bins, range=rnge,\n",
    "                              weights=mc_w, normed=True)\n",
    "\n",
    "# Make the data hist\n",
    "b = [bx, by]\n",
    "bg_logE = exp[\"logE\"]\n",
    "bg_sindec = np.sin(exp[\"dec\"])\n",
    "bg_h, _, _ = np.histogram2d(bg_sindec, bg_logE, bins=b,\n",
    "                                range=rnge, normed=True)\n",
    "\n",
    "\n",
    "# 4 cases:\n",
    "#   - Data & MC: Calculate the ratio\n",
    "#   - (>logEthresh) & (no data or no MC): Assign highest normal ratio\n",
    "#   - (<logEthresh) & (no data or no MC): Assign lowest normal ratio\n",
    "#   - No data and no MC): Assign any value (eg 1), these are never accessed\n",
    "# Get logE value per bin in entrie histogram\n",
    "m = get_binmids(b)\n",
    "logEs =  np.repeat(m[1], repeats=bins[1]).reshape(bins).T\n",
    "logEthresh = 3.5\n",
    "\n",
    "m1 = (bg_h > 0) & (mc_h > 0)\n",
    "m2 = (logEs > logEthresh) & ((bg_h <= 0) | (mc_h <= 0))\n",
    "m3 = (logEs <= logEthresh) & ((bg_h <= 0) | (mc_h <= 0))\n",
    "m4 = (bg_h <= 0) & (mc_h <= 0)\n",
    "\n",
    "SoB = np.ones_like(bg_h)\n",
    "SoB[m1] = mc_h[m1] / bg_h[m1]\n",
    "SoB[m2] = np.amax(SoB)\n",
    "SoB[m3] = np.amin(SoB)\n",
    "SoB[m4] = 1.\n",
    "\n",
    "# Make a 4 color mask map\n",
    "mask_map = np.ones_like(bg_h)\n",
    "mask_map[m1] = 1.\n",
    "mask_map[m2] = 2.\n",
    "mask_map[m3] = 0.\n",
    "mask_map[m4] = -1.\n",
    "\n",
    "# Plot it\n",
    "fig, ((axtl, axtr), (axbl, axbr)) = plt.subplots(2, 2, figsize=(12, 10))\n",
    "xx, yy = map(np.ravel, np.meshgrid(*m))\n",
    "\n",
    "_ = axtl.hist2d(xx, yy, bins=b, weights=bg_h.T.flatten(), cmap=\"viridis\",\n",
    "               norm=LogNorm())\n",
    "_ = axtr.hist2d(xx, yy, bins=b, weights=mc_h.T.flatten(), cmap=\"viridis\",\n",
    "               norm=LogNorm())\n",
    "\n",
    "# Plot mask\n",
    "cbins = np.linspace(-1, 2, 5)\n",
    "cticks = 0.5 * (cbins[:-1] + cbins[1:])\n",
    "cmap = matplotlib.cm.get_cmap(\"inferno\", len(cticks))\n",
    "_, _, _, img = axbr.hist2d(xx, yy, bins=b, weights=mask_map.T.flatten(),\n",
    "                          cmap=cmap)\n",
    "cax = split_axis(axbr, cbar=True)\n",
    "cbar = plt.colorbar(cax=cax, mappable=img, ticks=cticks)\n",
    "cbar.ax.set_yticklabels([\"No BG AND no MC\", \"lowE, no D OR MC\", \"BG AND MC\",\n",
    "                         \"higE, no D OR MC\"],\n",
    "                        rotation=60, va=\"center\")\n",
    "\n",
    "# Plot ratio\n",
    "cn = max(np.amin(SoB), np.amax(SoB))\n",
    "_, _, _, img = axbl.hist2d(xx, yy, bins=b, weights=SoB.T.flatten(),\n",
    "                          cmap=\"coolwarm\", vmin=1./cn, vmax=cn, norm=LogNorm())\n",
    "cax = split_axis(axbl, cbar=True)\n",
    "cbar = plt.colorbar(cax=cax, mappable=img)\n",
    "\n",
    "fig.tight_layout()\n",
    "# plt.savefig(\"./data/figs/energy_ratio_mask.png\", dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Compare sigma to x*exp(-x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "sig_deg = np.rad2deg(exp[\"sigma\"][exp[\"sigma\"] < np.deg2rad(10)])\n",
    "_, b, _ = plt.hist(sig_deg, bins=50, normed=True)\n",
    "\n",
    "# PDF\n",
    "x = np.linspace(0, 10, 500)\n",
    "a = 3\n",
    "y = a**2 * x * np.exp(-a * x)\n",
    "_ = plt.plot(x, y, lw=2)\n",
    "\n",
    "# Sampled from PDF\n",
    "# Combo from Pythia 8 and Trial&Error. How to derive from LambertW function?\n",
    "#   http://home.thep.lu.se/~torbjorn/doxygen/Basics_8h_source.html\n",
    "u1, u2 = np.random.uniform(size=(2, 10000))\n",
    "sam = -np.log(u1 * u2) / a\n",
    "plt.hist(sam, bins=b, normed=True, histtype=\"step\", lw=2)\n",
    "\n",
    "plt.xlim(0, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Old Scaled KDE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To save the old non-adaptive and kind-of guessed KDE for comparison, we put the former code in here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 3D histogram of BG data\n",
    "First we make a 3D histogram to better compare to mrichmann and to get an overview over the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# HANDTUNED scale parameter to \"fit\" KDE expectation to data...\n",
    "# TODO: Use Adaptive kernel width and asymmetric gaus kernels\n",
    "#       For sigma it might make sense to a take a restricted kernel [0, inf]\n",
    "fac_logE = 1.5\n",
    "fac_dec = 2.5\n",
    "fac_sigma = 2.\n",
    "\n",
    "logE = fac_logE * exp[\"logE\"]\n",
    "sigma = fac_sigma * np.rad2deg(exp[\"sigma\"])\n",
    "# np.cos(np.pi / 2. + exp[\"dec\"]); dec is for {sin(dec), dec, cos(zen)}\n",
    "dec = fac_dec * exp[\"dec\"]\n",
    "\n",
    "# Binning is rather arbitrary because we don't calc stuff with the hist\n",
    "bins = [50, 50, 50]\n",
    "# Range for sigma is picked by looking at the 1D distribution and cutting of\n",
    "# the tail. This will be covered by the KDE tail anyway. Rest is default\n",
    "r = [[np.amin(logE), np.amax(logE)],\n",
    "     [np.amin(dec), np.amax(dec)],\n",
    "     [0., fac_sigma * 5.]]\n",
    "\n",
    "sample = np.vstack((logE, dec, sigma)).T\n",
    "h, bins = np.histogramdd(sample=sample, bins=bins, range=r, normed=False)\n",
    "\n",
    "# Make bin mids for later use\n",
    "mids = []\n",
    "for b in bins:\n",
    "    mids.append(0.5 * (b[:-1] + b[1:]))\n",
    "\n",
    "# Make a nice corner plot\n",
    "fig, ax = hlp.corner_hist(h, bins=bins,\n",
    "                          label=[\"scaled logE\", \"scaled dec\", \"scaled sigma deg\"],\n",
    "                          hist2D_args={\"cmap\": \"Greys\"},\n",
    "                          hist_args={\"color\":\"#353132\"})\n",
    "\n",
    "# plt.savefig(\"./data/figs/bg_corner_scaled.png\", dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Kernel Density Estimation\n",
    "\n",
    "We use scikit learn's cross validation with a gaussian kernel to get the most robust bandwidth.\n",
    "Then we integrate with the same binning as above and compare to the 3D histogram.\n",
    "\n",
    "This section relies heavily on [Jake van der Plas examples for KDE](https://jakevdp.github.io/blog/2013/12/01/kernel-density-estimation/).\n",
    "More info on how KDE cross validation works can be found in [Modern Nonparametric Methods](http://www2.stat.duke.edu/~wjang/teaching/S05-293/lecture/ch6.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open(\"data/kde_cv_SCALED/KDE_model_selector_20_exp_IC86_I_\" +\n",
    "          \"followup_2nd_pass.json\", \"r\") as f:\n",
    "    kde_params = json.load(f)\n",
    "    \n",
    "kde = skn.KernelDensity(**kde_params)\n",
    "\n",
    "# KDE sample must be cut in sigma before fitting, similar to range in hist\n",
    "_exp = exp[exp[\"sigma\"] <= np.deg2rad(5)]\n",
    "\n",
    "_logE = fac_logE * _exp[\"logE\"]\n",
    "_sigma = fac_sigma * np.rad2deg(_exp[\"sigma\"])\n",
    "_dec = fac_dec * _exp[\"dec\"]\n",
    "\n",
    "kde_sample = np.vstack((_logE, _dec, _sigma)).T\n",
    "\n",
    "# Fit KDE best model to sample\n",
    "kde.fit(kde_sample)\n",
    "\n",
    "# Generate some BG samples to compare to the original data hist.\n",
    "# Use more statistics, histograms get normalized and we want the best estimate\n",
    "# for the pdf\n",
    "nsamples_kde = int(1e7)\n",
    "bg_samples = kde.sample(nsamples_kde)\n",
    "\n",
    "# Make histogram with same binning as original data\n",
    "bg_h, bg_bins = np.histogramdd(sample=bg_samples, bins=bins, range=r, normed=True)\n",
    "\n",
    "fig, ax = hlp.corner_hist(bg_h, bins=bg_bins,\n",
    "                         label=[\"scaled logE\", \"scaled sin(dec)\",\n",
    "                                \"scaled sigma deg\"],\n",
    "                         hist2D_args={\"cmap\": \"Greys\"},\n",
    "                         hist_args={\"color\":\"#353132\"})\n",
    "\n",
    "# plt.savefig(\"./data/figs/kde_corner_scaled.png\", dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Compare KDE to original data\n",
    "\n",
    "Make a ratio histogram of the KDE sample and the original data sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 2D marginalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create 2D hists, by leaving out one parameter\n",
    "xlabel = [\"scaled \" + s for s in [\"logE\", \"logE\", \"dec\"]]\n",
    "ylabel = [\"scaled \" + s for s in [\"dec\", \"sigma in °\", \"sigma in °\"]]\n",
    "\n",
    "for i, axes in enumerate([[0, 1], [0, 2], [1, 2]]):\n",
    "    _b = np.array(bins)\n",
    "    h_exp, b_exp = np.histogramdd(sample[:, axes],\n",
    "                                  bins=_b[axes], normed=True)\n",
    "    h_kde, b_kde = np.histogramdd(bg_samples[:, axes],\n",
    "                                  bins=_b[axes], normed=True)\n",
    "    \n",
    "    # KDE is expectation, but sampled with much more events.\n",
    "    # Weights would simply scale the total number of KDE events to match the\n",
    "    # number of original events. That would be the mean for the poisson\n",
    "    # distribution in each bin. So to get OK KDE expectation sqrt(n) errors\n",
    "    # in each bin, we divide not by the number of drawn KDE but by the number\n",
    "    # of original events.   \n",
    "    # Again shapes of meshgrid and hist are transposed\n",
    "    diffXX, _ = np.meshgrid(np.diff(_b[0]), np.diff(_b[1]))\n",
    "    norm_kde = len(exp) * diffXX.T\n",
    "    sigma_kde = np.sqrt(h_kde / norm_kde)\n",
    "\n",
    "    # Make 3 different diff/ratio hists to estimate KDE quality in\n",
    "    # 1D marginalization.\n",
    "    m = (h_exp > 0.)\n",
    "    ratio_h = np.zeros_like(h_exp)\n",
    "    ratio_h[m] = h_kde[m] / h_exp[m]\n",
    "\n",
    "    diff_h = h_kde - h_exp\n",
    "\n",
    "    m = (sigma_kde > 0.)\n",
    "    sigma_ratio_h = np.zeros_like(h_exp)\n",
    "    sigma_ratio_h[m] = (h_exp[m] - h_kde[m]) / sigma_kde[m]\n",
    "\n",
    "    # Bin mids and hist grid\n",
    "    _b = b_exp\n",
    "    m = get_binmids(_b)\n",
    "    xx, yy = map(np.ravel, np.meshgrid(m[0], m[1]))\n",
    "    \n",
    "    \n",
    "    # Big plot on the left and three right\n",
    "    fig = plt.figure(figsize=(10, 6))\n",
    "    gs = gridspec.GridSpec(3, 3)\n",
    "    axl = fig.add_subplot(gs[:, :2])\n",
    "    axrt = fig.add_subplot(gs[0, 2])\n",
    "    axrc = fig.add_subplot(gs[1, 2])\n",
    "    axrb = fig.add_subplot(gs[2, 2])\n",
    "    \n",
    "    # Steal space for colorbars\n",
    "    caxl = split_axis(axl, \"right\")\n",
    "    caxrt = split_axis(axrt, \"left\")\n",
    "    caxrc = split_axis(axrc, \"left\")\n",
    "    caxrb = split_axis(axrb, \"left\")\n",
    "\n",
    "    # Unset top and center xticklabels as they are shared with the bottom plot\n",
    "    axrt.set_xticklabels([])\n",
    "    axrc.set_xticklabels([])\n",
    "        \n",
    "    # Left: Difference over KDE sigma\n",
    "    # cbar_extr = max(np.amax(sigma_ratio_h),  # Center colormap to min/max\n",
    "    #                         abs(np.amin(sigma_ratio_h)))\n",
    "    _, _, _, imgl = axl.hist2d(xx, yy, bins=_b, weights=sigma_ratio_h.T.ravel(),\n",
    "                               cmap=\"seismic\", vmax=5, vmin=-5)\n",
    "    cbarl = plt.colorbar(cax=caxl, mappable=imgl)\n",
    "    axl.set_xlabel(xlabel[i])\n",
    "    axl.set_ylabel(ylabel[i])\n",
    "    axl.set_title(\"(exp - kde) / sigma_kde\")\n",
    "    \n",
    "    # Right top: Ratio\n",
    "    _, _, _, imgrt = axrt.hist2d(xx, yy, bins=_b, weights=ratio_h.T.ravel(),\n",
    "                                 cmap=\"seismic\", vmax=2, vmin=0);\n",
    "    cbarrt = plt.colorbar(cax=caxrt, mappable=imgrt)\n",
    "    axrt.set_title(\"kde / exp\")\n",
    "\n",
    "    # Right center: Data hist\n",
    "    _, _, _, imgrc = axrc.hist2d(xx, yy, bins=_b, weights=h_exp.T.ravel(),\n",
    "                                 cmap=\"Greys\", norm=LogNorm());\n",
    "    cbarrc = plt.colorbar(cax=caxrc, mappable=imgrc)\n",
    "    axrc.set_title(\"exp logscale\")\n",
    "\n",
    "    # Right bottom: KDE hist, same colorbar scale as on data\n",
    "    _, _, _, imgrb = axrb.hist2d(xx, yy, bins=_b, weights=h_kde.T.ravel(),\n",
    "                                 cmap=\"Greys\", norm=LogNorm());\n",
    "    # Set with same colormap as on data\n",
    "    imgrb.set_clim(cbarrc.get_clim())\n",
    "    cbarrb = plt.colorbar(cax=caxrb, mappable=imgrb)\n",
    "    axrb.set_title(\"kde logscale\")\n",
    "    \n",
    "    # Set tick and label positions\n",
    "    for ax in [caxrt, caxrc, caxrb]:\n",
    "        ax.yaxis.set_label_position(\"right\")\n",
    "        ax.yaxis.tick_left()\n",
    "    \n",
    "    fig.tight_layout()\n",
    "#     plt.savefig(\"./data/figs/kde_data_2d_{}_{}.png\".format(\n",
    "#                     xlabel[i], ylabel[i]), dpi=200)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 1D marginalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Pseudo smooth marginalization is done by sampling many point from KDE an\n",
    "# using a finely binned 1D histogram, so it looks smooth\n",
    "xlabel = [\"scaled \" + s for s in [\"logE\", \"dec\", \"sigma °\"]]\n",
    "\n",
    "for i, axes in enumerate([0, 1, 2]):\n",
    "    _b = np.array(bins)\n",
    "    h_exp, b_exp = np.histogram(sample[:, axes],\n",
    "                                bins=_b[axes], normed=True)\n",
    "    h_kde, b_kde = np.histogram(bg_samples[:, axes],\n",
    "                                bins=_b[axes], normed=True)\n",
    "    \n",
    "#     h_exp, b_exp = hist_marginalize(h, bins, axes=axes)\n",
    "#     h_kde, b_kde = hist_marginalize(bg_h, bg_bins, axes=axes)\n",
    "      \n",
    "    # KDE errorbars as in 2D case\n",
    "    norm_kde = len(exp) * np.diff(b_kde)\n",
    "    sigma_kde = np.sqrt(h_kde / norm_kde)\n",
    "\n",
    "    # Make 3 different diff/ratio hists to estimate KDE quality in\n",
    "    # 1D marginalization.\n",
    "    m = (h_exp > 0.)\n",
    "    ratio_h = np.zeros_like(h_exp)\n",
    "    ratio_h[m] = h_kde[m] / h_exp[m]\n",
    "\n",
    "    diff_h = h_kde - h_exp\n",
    "\n",
    "    m = (sigma_kde > 0.)\n",
    "    sigma_ratio_h = np.zeros_like(h_exp)\n",
    "    sigma_ratio_h[m] = (h_exp[m] - h_kde[m]) / sigma_kde[m]\n",
    "\n",
    "    # Bin mids\n",
    "    _b = b_exp\n",
    "    m = get_binmids([_b])[0]\n",
    "    \n",
    "    # Plot both and the ration normed. Big plot on the left and three right\n",
    "    fig = plt.figure(figsize=(10, 6))\n",
    "    gs = gridspec.GridSpec(3, 3)\n",
    "    axl = fig.add_subplot(gs[:, :2])\n",
    "    axrt = fig.add_subplot(gs[0, 2])\n",
    "    axrc = fig.add_subplot(gs[1, 2])\n",
    "    axrb = fig.add_subplot(gs[2, 2])\n",
    "\n",
    "    axrt.set_xticklabels([])\n",
    "    axrc.set_xticklabels([])\n",
    "\n",
    "    # Set ticks and labels right\n",
    "    for ax in [axrt, axrc, axrb]:\n",
    "        ax.yaxis.set_label_position(\"right\")\n",
    "        ax.yaxis.tick_right()\n",
    "\n",
    "    # Limits\n",
    "    for ax in [axl, axrt, axrc, axrb]:\n",
    "        ax.set_xlim(_b[0], _b[-1])\n",
    "        \n",
    "    # Main plot:\n",
    "    # Plot more dense to mimic a smooth curve\n",
    "    __h, __b = np.histogram(bg_samples[:, i], bins=500,\n",
    "                            range=[_b[0], _b[-1]], density=True)\n",
    "    __m = get_binmids([__b])[0]\n",
    "    axl.plot(__m, __h, lw=3, alpha=0.5)\n",
    "    \n",
    "    _ = axl.hist(m, bins=_b, weights=h_exp, label=\"exp\", histtype=\"step\",\n",
    "                 lw=2, color=\"k\")\n",
    "    _ = axl.errorbar(m, h_kde, yerr=sigma_kde, fmt=\",\", color=\"r\")\n",
    "    _ = axl.hist(m, bins=_b, weights=h_kde, label=\"kde\", histtype=\"step\",\n",
    "                 lw=2, color=\"r\")    \n",
    "    \n",
    "    axl.set_xlabel(xlabel[i])\n",
    "    axl.legend(loc=\"upper right\")\n",
    "\n",
    "    # Top right: Difference\n",
    "    _ = axrt.axhline(0, 0, 1, color=\"k\", ls=\"-\")\n",
    "    _ = axrt.hlines([-.02, -.01, .01, .02], _b[0], _b[-1],\n",
    "                    colors='#353132', linestyles='dashed')\n",
    "    _ = axrt.hist(m, bins=_b, weights=diff_h, histtype=\"step\", lw=2, color=\"r\")\n",
    "    axrt.set_ylim(-.05, +.05)\n",
    "    axrt.set_ylabel(\"kde - exp\")\n",
    "\n",
    "    # Center right: Ratio\n",
    "    _ = axrc.axhline(1, 0, 1, color=\"k\", ls=\"-\")\n",
    "    _ = axrc.hlines([0.8, 0.9, 1.1, 1.2], _b[0], _b[-1],\n",
    "                    colors='#353132', linestyles='dashed')\n",
    "    _ = axrc.hist(m, bins=_b, weights=ratio_h, histtype=\"step\", lw=2, color=\"r\")\n",
    "    axrc.set_ylim(.5, 1.5)\n",
    "    axrc.set_ylabel(\"kde / exp\")\n",
    "\n",
    "    # Bottom right: Ratio of diff to sigma of expectation\n",
    "    _ = axrb.axhline(0, 0, 1, color=\"k\", ls=\"-\")\n",
    "    _ = axrb.hlines([-2, -1, 1, 2], _b[0], _b[-1],\n",
    "                    colors='#353132', linestyles='dashed')\n",
    "    _ = axrb.hist(m, bins=_b, weights=sigma_ratio_h, histtype=\"step\", lw=2, color=\"r\")\n",
    "    axrb.set_ylim(-3, +3)\n",
    "    axrb.set_ylabel(\"(exp-kde)/sigma_kde\")\n",
    "    \n",
    "#     plt.savefig(\"./data/figs/kde_data_1d_{}.png\".format(\n",
    "#             xlabel[i]), dpi=200)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Show scores of awKDE CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open(\"data/awKDE_CV/CV10_glob_bw_alpha_EXP_IC86I_CUT_sig.ll.20_PARS\" +\n",
    "          \"_diag_True_pass2_cv_res.json\", \"r\") as f:\n",
    "    cv_results = json.load(f)\n",
    "    \n",
    "glob_bws = cv_results[\"param_glob_bw\"]\n",
    "mean_score = cv_results[\"mean_test_score\"]\n",
    "std_score = cv_results[\"std_test_score\"]\n",
    "\n",
    "# Mark best\n",
    "bf_idx = np.argmax(mean_score)\n",
    "_ = plt.axvline(glob_bws[bf_idx], 0, 1, ls=\"--\", color=\"C2\", zorder=-1)\n",
    "_ = plt.axhline(mean_score[bf_idx], 0, 1, ls=\"--\", color=\"C2\", zorder=-1)\n",
    "# Each horizontal line is one alpha tested for that glob_bw\n",
    "bf_idx = np.argmax(mean_score)\n",
    "_ = plt.errorbar(glob_bws, mean_score, yerr=std_score,fmt=\"_\")\n",
    "\n",
    "plt.xlabel(\"global bandwidth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Old BG Rate Sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Tried to sample everything at once (trials, src, times).\n",
    "Very clumsy because for each src and trial, a different number of evt is sampled, so we can't use arrays.\n",
    "\n",
    "Also we can fit a single LLH at a time only, so no need to get all samples in advance.\n",
    "Might be faster, but consumes also much more memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Sample number of events in frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def filter_runs(run):\n",
    "    \"\"\"\n",
    "    Filter runs as stated in jfeintzig's doc.\n",
    "    \"\"\"\n",
    "    exclude_runs = [120028, 120029, 120030, 120087, 120156, 120157]\n",
    "    if ((run[\"good_i3\"] == True) & (run[\"good_it\"] == True) &\n",
    "        (run[\"run\"] not in exclude_runs)):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "goodrun_dict, _livetime = hlp.create_goodrun_dict(\n",
    "    runlist=\"data/runlists/ic86-i-goodrunlist.json\", filter_runs=filter_runs)\n",
    "\n",
    "h = hlp._create_runtime_bins(exp[\"timeMJD\"], goodrun_dict=goodrun_dict,\n",
    "                             remove_zero_runs=True)\n",
    "\n",
    "def f(x, pars):\n",
    "    \"\"\"\n",
    "    Returns the rate at a given time in MJD.\n",
    "    \"\"\"\n",
    "    a, b, c, d = pars\n",
    "    return a * np.sin(b * (x - c)) + d\n",
    "\n",
    "def lstsq(pars, *args):\n",
    "    \"\"\"\n",
    "    Weighted leastsquares min sum((wi * (yi - fi))**2)\n",
    "    \"\"\"\n",
    "    # data x,y-values and weights are fixed\n",
    "    x, y, w = args\n",
    "    _f = f(x, pars)\n",
    "    return np.sum((w * (y - _f))**2)\n",
    "\n",
    "# Seed values from consideration above.\n",
    "# a0 = -0.0005\n",
    "# b0 = 2. * np.pi / 365.  # We could restrict the period to one yr exact.\n",
    "# c0 = np.amin(start_mjd)\n",
    "# d0 = np.average(h, weights=yerr**2)\n",
    "\n",
    "rate = h[\"rate\"]\n",
    "rate_std = h[\"rate_std\"]\n",
    "X = exp[\"timeMJD\"]\n",
    "binmids = 0.5 * (h[\"start_mjd\"] + h[\"stop_mjd\"])\n",
    "\n",
    "a0 = 0.5 * (np.amax(rate) - np.amin(rate))\n",
    "b0 = 2. * np.pi / 365.\n",
    "c0 = np.amin(X)\n",
    "d0 = np.average(rate, weights=rate_std**2)\n",
    "\n",
    "x0 = [a0, b0, c0, d0]\n",
    "# Bounds as explained above\n",
    "bounds = [[None, None], [0.5 * b0, 1.5 * b0], [c0 - b0, c0 + b0, ], [0, 0.01]]\n",
    "# x, y values, weights\n",
    "args = (binmids, rate, 1. / rate_std)\n",
    "\n",
    "res = sco.minimize(fun=lstsq, x0=x0, args=args, bounds=bounds)\n",
    "bf_pars = res.x\n",
    "\n",
    "print(\"Amplitude   : {: 13.5f} in Hz\".format(res.x[0]))\n",
    "print(\"Period (d)  : {: 13.5f} in days\".format(2 * np.pi / res.x[1]))\n",
    "print(\"Offset (MJD): {: 13.5f} in MJD\".format(res.x[2]))\n",
    "print(\"Avg. rate   : {: 13.5f} in Hz\".format(res.x[3]))\n",
    "\n",
    "# Define the rate function:\n",
    "def rate_fun(t):\n",
    "    \"\"\"\n",
    "    Returns the rate at a given time in MJD.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    t : array-like\n",
    "        Time in MJD.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    rate : array-like\n",
    "        The rate of background events in Hz.\n",
    "    \"\"\"\n",
    "    return f(t, res.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def _prep_times(t, trange):\n",
    "    \"\"\"\n",
    "    Little wrapper to not DRY.\n",
    "    \"\"\"\n",
    "    t = np.atleast_1d(t)\n",
    "    trange = np.array(trange)\n",
    "    nsrc = len(t)\n",
    "    \n",
    "    # Make shape (nsources, 1) for the times\n",
    "    t = t.reshape(nsrc, 1)\n",
    "    \n",
    "    # If range is 1D (one for all) reshape it to (nsources, 2)\n",
    "    if len(trange.shape) == 1:\n",
    "        trange = np.repeat(trange.reshape(1, 2), repeats=nsrc, axis=0)\n",
    "        \n",
    "    # Prepare time window in MJD\n",
    "    trange = t + trange / secinday\n",
    "    \n",
    "    return t, trange\n",
    "\n",
    "def get_num_of_bg_events(t, trange, ntrials, pars):\n",
    "    \"\"\"\n",
    "    Draw number of background events per trial from a poisson distribution\n",
    "    with the mean of the fitted rate function.\n",
    "    Then draw nevents times via rejection sampling for the time dpeendent rate\n",
    "    function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    t : array-like\n",
    "        Times of the occurance of each source event in MJD.\n",
    "    trange : [float, float] or array_like, shape (len(t), 2)\n",
    "        Time window(s) in seconds relativ to the given time(s) t.\n",
    "        - If [float, float], the same window [lower, upper] is used for every\n",
    "          source.\n",
    "        - If array-like, lower [i, 0] and upper [i, 1] bounds of the time\n",
    "          window per source.\n",
    "    ntrials : int\n",
    "        Number of background trials we need the number of how many events to\n",
    "        inject for.\n",
    "    pars : array-like\n",
    "        Best fit parameters from the fit function used in its integral.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    nevents : array-like, shape (len(t), ntrials)\n",
    "        The number of events to inject for each trial for each source.\n",
    "    \"\"\"\n",
    "    # Integrate rate function analytially in desired interval\n",
    "    def rate_integral(trange, pars):\n",
    "        \"\"\"\n",
    "        Match with factor [secinday] = 24 * 60 * 60 s / MJD = 86400/(Hz*MJD)\n",
    "        in the last step.\n",
    "            [a], [d] = Hz, [b], [c], [ti] = MJD\n",
    "            [a / b] = Hz * MJD, [d * (t1 - t0)] = HZ * MJD\n",
    "        \"\"\"\n",
    "        a, b, c, d = pars\n",
    "        \n",
    "        t0 = np.atleast_2d(trange[:, 0]).reshape(len(trange), 1)\n",
    "        t1 = np.atleast_2d(trange[:, 1]).reshape(len(trange), 1)\n",
    "        \n",
    "        per = a / b * (np.cos(b * (t0 - c)) - np.cos(b * (t1 - c)))\n",
    "        lin = d * (t1 - t0)\n",
    "\n",
    "        return (per + lin) * secinday\n",
    "    \n",
    "    t, trange = _prep_times(t, trange)\n",
    "    \n",
    "    # Expectation is the integral in the time frame\n",
    "    expect = rate_integral(trange, pars)\n",
    "        \n",
    "    # Sample from poisson\n",
    "    nevts = np.random.poisson(lam=expect, size=(len(t), ntrials))\n",
    "      \n",
    "    return nevts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "start, stop = h[\"start_mjd\"], h[\"stop_mjd\"]\n",
    "\n",
    "nsrc = 5\n",
    "t = start[100:100 + nsrc]\n",
    "# Make different time windows to verify that more events are sampled in more time\n",
    "trange = np.array([[-10 * 5 * i, 20 * 5*i] for i in range(1, nsrc + 1)])\n",
    "print(\"Time windows:\\n\", trange)\n",
    "\n",
    "ntrials = 10\n",
    "nevts = get_num_of_bg_events(t, trange, ntrials, res.x)\n",
    "# print(\"All evts, per src and trial:\\n\", nevts)\n",
    "\n",
    "# Total evts in all trials per src\n",
    "print(\"Total sampled evts per src:\\n\", np.sum(nevts, axis=1)[:, np.newaxis])\n",
    "\n",
    "# Compare to expectation\n",
    "print(\"Expected per trial per src:\\n\", np.diff(trange, axis=1) * res.x[3])\n",
    "print(\"Sampled per trial per src:\\n\", np.sum(nevts, axis=1)[:, np.newaxis] / ntrials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Now the sampling of random times in the time frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "First we want to see, that all BG injected events stay in the correct time frame and make a uniform distribution for small time frames.\n",
    "\n",
    "Then we make the time window really big and the events should follow the rate function PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_times_in_frame(t, trange, nsamples):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    t : float\n",
    "        Time of the occurance of the source event in MJD.\n",
    "    trange : [float, float]\n",
    "        Time window in seconds relativ to the given time t.\n",
    "    nsamples : array-like, type int, shape (len(t))\n",
    "        Number of events to inject per trial. Number of trials is given by\n",
    "        the length of nsamples.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    times : list, length len(t)\n",
    "        List of samples times in MJD of background events per source.\n",
    "        For each source i nsamples[i] times are drawn from the rate function.\n",
    "    \"\"\"\n",
    "    _pdf = rate_fun\n",
    "    \n",
    "    t, trange = _prep_times(t, trange)\n",
    "    \n",
    "    sample = []\n",
    "    nsamples = np.atleast_1d(nsamples)\n",
    "    \n",
    "    for i, ni in enumerate(nsamples):\n",
    "        sam, _ = rejection_sampling(_pdf, bounds=trange, n=ni)\n",
    "        sample.append(sam)\n",
    "        \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# First the small time frame\n",
    "# Arbitrary start date from data\n",
    "t0 = start[100]\n",
    "t0_sec = t0 * secinday\n",
    "\n",
    "# dt from t0 in seconds, clip at 4 sigma\n",
    "dt = 200\n",
    "nsig = 4.\n",
    "\n",
    "# Make t values for plotting in MJD around t0\n",
    "clip = np.clip(dt, 2, 30) * nsig\n",
    "plt_rng = [-clip, dt + clip]\n",
    "trange = plt_rng\n",
    "ntrials = 10000\n",
    "\n",
    "# Sample times\n",
    "nevts = get_num_of_bg_events(t=t0, trange=trange, ntrials=ntrials,\n",
    "                             pars=res.x)[0]\n",
    "times = get_times_in_frame(t0, trange, nevts)\n",
    "\n",
    "# Plot them in together with the PDFs\n",
    "def time_bg_pdf(t, t0, a, b):\n",
    "    # Normalize relative to t0 in seconds (first multiply avoids rounding?)\n",
    "    _t = t * secinday - t0 * secinday\n",
    "  \n",
    "    pdf = np.zeros_like(_t, dtype=np.float)\n",
    "    uni = (_t >= a) & (_t <= b)\n",
    "    pdf[uni] = 1. / (b - a)\n",
    "    return pdf\n",
    "\n",
    "def time_sig_pdf(t, t0, dt, nsig=4):\n",
    "    if dt < 0:\n",
    "        raise ValueError(\"dt must not be negative.\")\n",
    "\n",
    "    # Normalize relative to t0 in seconds (first multiply avoids rounding?)\n",
    "    _t = t * secinday - t0 * secinday\n",
    "    \n",
    "    # Constrain sig_t to [2, 30]s regardless of uniform time window\n",
    "    sig_t = np.clip(dt, 2, 30)\n",
    "    sig_t_clip = nsig * sig_t\n",
    "    gaus_norm = (np.sqrt(2 * np.pi) * sig_t)\n",
    "    \n",
    "    # Split in def regions gaus rising, uniform, gaus falling and zero\n",
    "    gr = (_t < 0) & (_t >= -sig_t_clip)\n",
    "    gf = (_t > dt) & (_t <= dt + sig_t_clip)\n",
    "    uni = (_t >= 0) & (_t <= dt)\n",
    "    \n",
    "    pdf = np.zeros_like(t, dtype=np.float)\n",
    "    pdf[gr] = scs.norm.pdf(_t[gr], loc=0, scale=sig_t)\n",
    "    pdf[gf] = scs.norm.pdf(_t[gf], loc=dt, scale=sig_t)\n",
    "    # Connect smoothly with the gaussians\n",
    "    pdf[uni] = 1. / gaus_norm\n",
    "    \n",
    "    # Normalize whole distribtuion\n",
    "    dcdf = (scs.norm.cdf(dt + sig_t_clip, loc=dt, scale=sig_t) -\n",
    "            scs.norm.cdf(-sig_t_clip, loc=0., scale=sig_t))\n",
    "    norm = dcdf + dt / gaus_norm\n",
    "    \n",
    "    return pdf / norm\n",
    "\n",
    "\n",
    "# Plot the pdfs\n",
    "t = np.linspace(t0_sec + plt_rng[0], t0_sec + plt_rng[1], 200) / secinday\n",
    "bg_pdf = time_bg_pdf(t, t0, -clip, dt + clip)\n",
    "sig_pdf = time_sig_pdf(t, t0, dt, nsig)\n",
    "\n",
    "# Plot in normalized time\n",
    "_t = t * secinday - t0 * secinday\n",
    "plt.plot(_t, bg_pdf, \"C0-\")\n",
    "plt.plot(_t, sig_pdf, \"C1-\")\n",
    "plt.axvline(dt, 0, 1, color=\"C7\", ls=\"--\")\n",
    "plt.axvline(0, 0, 1, color=\"C1\", ls=\"--\")\n",
    "\n",
    "# Plot injected events from all trials\n",
    "T = np.array([])\n",
    "for ti in times:\n",
    "    T = np.append(T, ti)  \n",
    "T = (T - t0) * secinday\n",
    "\n",
    "_ = plt.hist(T, bins=50, normed=True, color=dg, alpha=.25)\n",
    "\n",
    "plt.xlabel(\"Time relative to t0 in sec\")\n",
    "plt.ylim(0, None);\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig(\"./data/figs/bg_events_time_sampled_narrow.png\", dpi=200)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Now the really large time frame, over the whole time range\n",
    "t0 = start[0]\n",
    "t0_sec = t0 * secinday\n",
    "\n",
    "# dt from t0 in seconds, clip at 4 sigma\n",
    "dt = (stop[-1] - start[0]) * secinday\n",
    "nsig = 4.\n",
    "\n",
    "# Make t values for plotting in MJD around t0\n",
    "clip = np.clip(dt, 2, 30) * nsig\n",
    "plt_rng = [-clip, dt + clip]\n",
    "trange = plt_rng\n",
    "ntrials = 1\n",
    "\n",
    "# Sample times\n",
    "nevts = get_num_of_bg_events(t=t0, trange=trange, ntrials=ntrials,\n",
    "                             pars=res.x)[0]\n",
    "times = get_times_in_frame(t0, trange, nevts)\n",
    "\n",
    "# Plot injected events from all trials\n",
    "T = np.array([])\n",
    "for ti in times:\n",
    "    T = np.append(T, ti)  \n",
    "\n",
    "h, b = np.histogram(T, bins=1081)\n",
    "m = get_binmids([b])[0]\n",
    "scale = np.diff(b) * secinday * ntrials\n",
    "yerr = np.sqrt(h) / scale\n",
    "h = h / scale\n",
    "\n",
    "plt.errorbar(m, h, yerr=yerr, fmt=\",\")\n",
    "\n",
    "# Plot normalized rate function to compare\n",
    "t = np.linspace(start[0], stop[-1], 100)\n",
    "r = rate_fun(t=t)\n",
    "plt.plot(t, r, lw=2, zorder=5)\n",
    "plt.axhline(res.x[3], 0, 1, color=\"C1\", ls=\"--\", label=\"\", zorder=5)\n",
    "\n",
    "plt.xlim(start[0], stop[-1])\n",
    "plt.ylim(0, 0.009)\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig(\"./data/figs/bg_events_time_sampled_wide.png\", dpi=200)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Testing for overlapping time windows snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Make overlapping time windows and raise error when we detect them\n",
    "# Here window 4 overlaps with 3 and 5, which explains the matrix entries\n",
    "dt0 = np.arange(1, 5 + 1) * 10\n",
    "# Make this bigger or smaller to create overlap\n",
    "overl = 10\n",
    "dt1 = np.arange(1, 5 + 1) * 10 * overl\n",
    "t = np.linspace(100, 1000, 5)\n",
    "\n",
    "# Plot the windows\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "for i, (ti, dti0, dti1) in enumerate(zip(t, dt0, dt1)):\n",
    "    plt.axvspan(ti + dti0, ti + dti1, label=\"{:d}\".format(i),\n",
    "                color=\"C{}\".format(i), alpha=.2)\n",
    "    plt.axvline(ti + dti0, 0, 1, color=\"C{}\".format(i), ls=\"--\")\n",
    "    plt.axvline(ti + dti1, 0, 1, color=\"C{}\".format(i), ls=\"--\")\n",
    "plt.show()\n",
    "\n",
    "# Test mutual exclusiveness of the windows\n",
    "# larger edge must be y than lower edge and vice versa to be exclusive\n",
    "exclusive = (((t + dt0)[:, None] >= t + dt1) |\n",
    "             ((t + dt1)[:, None] <= t + dt0))\n",
    "# Fix manually that time windows overlap with themselves of course\n",
    "np.fill_diagonal(exclusive, True)\n",
    "\n",
    "# If any entry is False, we have an overlapping window case\n",
    "# Here it is (2, 3), (3, 2), (3, 4) and (4, 3)  as expected per construction\n",
    "print(exclusive)\n",
    "\n",
    "# Now raise errors\n",
    "if np.any(exclusive == False):\n",
    "    window_ids = [[x, y] for x, y in zip(*np.where(~exclusive))]\n",
    "    raise ValueError(\"Overlapping time windows: {}.\".format(\", \".join(\n",
    "        [\"[{:d}, {:d}]\".format(*c) for c in window_ids])) +\n",
    "                     \"\\nThis is not supported yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Utils -- rotator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Compare to mhuber raw_flux and event selection \n",
    "\n",
    "**We need to change the kernel to Python 2 here, to load skylab**\n",
    "\n",
    "No modifications have been done to skylab code.\n",
    "\n",
    "The conversion from flux to fluence is simply the lack of livetime in the weights.\n",
    "So we obtain skylab fluence by just repeating the steps from the code, but dropping the multiplication with the livetime.\n",
    "\n",
    "The event selection is not affected by the livetime, so we don't have to modify the code itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nsrc = 5\n",
    "src_ra = np.linspace(0, 2. * np.pi, nsrc)\n",
    "src_dec = np.deg2rad([-60., -30., 0, 30., 60.])\n",
    "src_t = np.linspace(50000, 50300, nsrc)\n",
    "dt0 = np.zeros(nsrc, dtype=np.float)\n",
    "dt1 = np.zeros(nsrc, dtype=np.float) + 200.\n",
    "w_theo = np.ones(nsrc)\n",
    "src_names = [\"ra\", \"dec\", \"t\", \"dt0\", \"dt1\", \"w_theo\"]\n",
    "srcs = np.core.records.fromarrays(np.vstack((src_ra, src_dec,src_t,\n",
    "                                             dt0, dt1, w_theo)),\n",
    "                                  names=src_names)\n",
    "\n",
    "# Only set the src params we need here manually\n",
    "sig_inj = SigInj.SignalInjector(gamma=2., mode=\"band\",\n",
    "                                inj_width=np.arcsin(0.1))\n",
    "\n",
    "# Simulate skylab's band selection (debug flag only)\n",
    "sig_inj._skylab_band = True\n",
    "\n",
    "# Correct ow to ow_per_type\n",
    "mc[\"ow\"] = mc[\"ow\"] / 0.5\n",
    "\n",
    "# Different sample sizes\n",
    "mc_dict = {0: mc, 1: mc[::10]}\n",
    "livetime = {0: 340., 1: 34.}\n",
    "sig_inj.fit(srcs, mc_dict, exp.dtype.names)\n",
    "\n",
    "print(\"\\nRaw Flux: {}\".format(sig_inj._raw_flux))\n",
    "\n",
    "# Undo ow to ow_per_type\n",
    "mc[\"ow\"] = mc[\"ow\"] * 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(sig_inj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, \"/Users/tmenne/icecube/software/skylab-mhuber\")\n",
    "\n",
    "from skylab.ps_injector import StackingPointSourceInjector as psinj\n",
    "inj = psinj(gamma=2.)\n",
    "inj.fill(src_dec, mc_dict, livetime)\n",
    "\n",
    "ow = np.empty(0, dtype=np.float)\n",
    "omega = (inj._omega / inj.w_theo)[inj.mc_arr[\"src_idx\"]]\n",
    "print(\"\")\n",
    "for enum in mc_dict.keys():\n",
    "    idx = inj.mc_arr[inj.mc_arr[\"enum\"] == enum][\"idx\"]\n",
    "    _ow = (inj.mc[enum][\"ow\"] * inj.mc[enum][\"trueE\"]**(-inj.gamma))[idx]\n",
    "    _ow /= omega[inj.mc_arr[\"enum\"] == enum][idx]\n",
    "    ow = np.append(ow, _ow)\n",
    "    print(\"Raw Fluence at {} : {}\".format(enum, np.sum(_ow)))\n",
    "\n",
    "print(\"\\nRaw Flux: {}\".format(np.sum(ow)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Are both raw_fluences and number of events equal?\n",
    "print(\"Selected events equal : {}\".format(len(sig_inj.mc_arr) ==\n",
    "                                          len(inj.mc_arr)))\n",
    "print(\"Raw fluences equal    : {}\".format(np.allclose(sig_inj._raw_flux,\n",
    "                                                      np.sum(ow))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Percentile with nzeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def _percentile_nzeros(vals, nzeros, q):\n",
    "    \"\"\"\n",
    "    Returns the percentile q for a dataset with `vals` > 0 and `nzeros`\n",
    "    entries that are zero.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    vals : array-like\n",
    "        Non-zero values.\n",
    "    nzeros : int\n",
    "        Number of zero trials.\n",
    "    q : float\n",
    "        Percentile in [0, 100].\n",
    "    \"\"\"\n",
    "    q /= 100.\n",
    "    nonzero = len(vals)\n",
    "    ntot = nonzero + nzeros\n",
    "    idx = int(q * ntot) - nzeros - 1\n",
    "    vals = np.sort(vals)\n",
    "        \n",
    "    if idx < 0:\n",
    "        return 0.\n",
    "    else:\n",
    "        return vals[idx]\n",
    "    \n",
    "if True:  # Test it\n",
    "    nzeros = 100\n",
    "    a = np.arange(10) + 1\n",
    "    b = np.concatenate((np.zeros(nzeros), a))\n",
    "\n",
    "    qs = np.arange(99) + 1\n",
    "\n",
    "    for q in qs:\n",
    "        print(\"- q   : \", q)\n",
    "        print(\"  np  : \", np.percentile(b, q, interpolation=\"lower\"))\n",
    "        print(\"  own : \", _percentile_nzeros(a, nzeros, q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Interpolator options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "skylab is linearly interpolating in logspcae.\n",
    "Here we see what this mean (and also test the other settings for 1D).\n",
    "\n",
    "The effect basically that we smooth the lower values and get more conservative to higher values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rndgen = np.random.RandomState(28756)\n",
    "\n",
    "x = np.arange(10)\n",
    "y = rndgen.randint(1, 1000, len(x))\n",
    "\n",
    "x_ = np.linspace(x[0], x[-1], 1000)\n",
    "\n",
    "# Linear interpol in logspace\n",
    "interpol = sci.interp1d(x, np.log(y), kind=\"linear\")\n",
    "y_log = np.exp(interpol(x_))\n",
    "\n",
    "# Normal space interpol\n",
    "interpol = sci.interp1d(x, y, kind=\"linear\")\n",
    "y_lin = interpol(x_)\n",
    "interpol = sci.interp1d(x, y, kind=\"quadratic\")\n",
    "y_quad = interpol(x_)\n",
    "interpol = sci.interp1d(x, y, kind=\"cubic\")\n",
    "y_cube = interpol(x_)\n",
    "interpol = sci.interp1d(x, y, kind=\"nearest\")\n",
    "y_near = interpol(x_)\n",
    "\n",
    "plt.plot(x, y, color=\"k\")\n",
    "plt.plot(x_, y_log, ls=\"--\", label=\"log-lin\")\n",
    "plt.plot(x_, y_lin, ls=\"--\", label=\"lin\")\n",
    "plt.plot(x_, y_quad, ls=\"--\", label=\"quad\")\n",
    "plt.plot(x_, y_cube, ls=\"--\", label=\"cube\")\n",
    "plt.plot(x_, y_near, ls=\"--\", label=\"near\")\n",
    "\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "# plt.savefig(\"./data/figs/interpol_test_lin_log.png\", dpi=200)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Do the same thing but now look at the interpolations in log space.\n",
    "The log space linear interpolation is now working as the linear interpolation in normal space before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rndgen = np.random.RandomState(28756)\n",
    "\n",
    "x = np.arange(10)\n",
    "y = rndgen.randint(1, 1000, len(x))\n",
    "\n",
    "x_ = np.linspace(x[0], x[-1], 1000)\n",
    "\n",
    "# Linear interpol in logspace\n",
    "interpol = sci.interp1d(x, np.log(y), kind=\"linear\")\n",
    "y_log = interpol(x_)\n",
    "\n",
    "# Normal space interpol\n",
    "interpol = sci.interp1d(x, y, kind=\"linear\")\n",
    "y_lin = np.log(interpol(x_))\n",
    "interpol = sci.interp1d(x, y, kind=\"quadratic\")\n",
    "y_quad = np.log(interpol(x_))\n",
    "interpol = sci.interp1d(x, y, kind=\"cubic\")\n",
    "y_cube = np.log(interpol(x_))\n",
    "interpol = sci.interp1d(x, y, kind=\"nearest\")\n",
    "y_near = np.log(interpol(x_))\n",
    "\n",
    "plt.plot(x, np.log(y), color=\"k\")\n",
    "plt.plot(x_, y_log, ls=\"--\", label=\"log-lin\")\n",
    "plt.plot(x_, y_lin, ls=\"--\", label=\"lin\")\n",
    "plt.plot(x_, y_quad, ls=\"--\", label=\"quad\")\n",
    "plt.plot(x_, y_cube, ls=\"--\", label=\"cube\")\n",
    "plt.plot(x_, y_near, ls=\"--\", label=\"near\")\n",
    "\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"ln(y)\")\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "# plt.savefig(\"./data/figs/interpol_test_lin_log_in_logspace.png\", dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Time between run starts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here we look at the time between run stop time and the start time of the next run to justify the handling of flaot errors in runtime binning.\n",
    "Few events are outside any run but very close to a start or stop time, so they should be included in that run.\n",
    "To account for that, we check if the events are within the bin edges +- eps and choose eps to be 1 second which is 1.15e-05 in MJD times.\n",
    "\n",
    "Here we check if runs are usually further apart than 1 second so we introduce no double counting with the eps expansion.\n",
    "\n",
    "**Result:** Normal runs seem to be started with no time loss. So we should not do the eps thing.\n",
    "But then it's strange where those stray events come from, that are almost a second outside the nearest run..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def create_goodrun_dict(runlist):\n",
    "    if \"runs\" not in runlist.keys():\n",
    "        raise ValueError(\"Runlist misses key 'runs' on top level\")\n",
    "\n",
    "    # This is a list of dicts (one dict per run)\n",
    "    goodrun_list = runlist[\"runs\"]\n",
    "\n",
    "    # Convert the run list of dicts to a dict of arrays for easier handling\n",
    "    goodrun_dict = dict(zip(goodrun_list[0].keys(),\n",
    "                            zip(*[r.values() for r in goodrun_list])))\n",
    "    for k in goodrun_dict.keys():\n",
    "        goodrun_dict[k] = np.array(goodrun_dict[k])\n",
    "\n",
    "    # Add times to MJD floats\n",
    "    goodrun_dict[\"good_start_mjd\"] = astrotime(\n",
    "        goodrun_dict[\"good_tstart\"], format=\"iso\").mjd\n",
    "    goodrun_dict[\"good_stop_mjd\"] = astrotime(\n",
    "        goodrun_dict[\"good_tstop\"], format=\"iso\").mjd\n",
    "\n",
    "    # Add runtimes in MJD days\n",
    "    goodrun_dict[\"runtime_days\"] = (goodrun_dict[\"good_stop_mjd\"] -\n",
    "                                    goodrun_dict[\"good_start_mjd\"])\n",
    "\n",
    "    return goodrun_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open(\"data/runlists/ic86-i-goodrunlist.json\", \"r\") as f:\n",
    "    runlist = json.load(f)\n",
    "    \n",
    "rundict = create_goodrun_dict(runlist)\n",
    "\n",
    "tstart = np.sort(rundict[\"good_start_mjd\"])\n",
    "tstop = np.sort(rundict[\"good_stop_mjd\"])\n",
    "\n",
    "fraction = []\n",
    "dt = np.logspace(-2, 3, num=100)\n",
    "for dti in dt:\n",
    "    mask = []\n",
    "    for t0, t1 in zip(tstart[1:], tstop[:-1]):\n",
    "        mask.append(np.isclose(t0, t1, rtol=0., atol=dti / secinday))\n",
    "    fraction.append(np.sum(mask) / (len(tstart) - 1.))\n",
    "\n",
    "plt.plot(np.log10(dt), fraction, ls=\"\", marker=\"o\")\n",
    "plt.xlabel(\"dt between runs in log10(sec)\")\n",
    "plt.ylabel(\"fraction of runs with |t1-t0| <= dt\")\n",
    "plt.ylim(0, 1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# delta chi2 RV, implementing scipy.stats.rv_continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Compare vanilla chi2 with delta chi2 where `1-eta` is the zero fraction.\n",
    "\n",
    "### Note\n",
    "\n",
    "The survival function is `1-cdf`, but for the delta chi2 we define it to be 1 at zero because no events are lost.\n",
    "If we stick to `1-cdf` it would be `eta` at zero, wrongly indicating, that events are lost.\n",
    "On the other hand, the cdf is nonzero at zero, indicating the number of zero trials which are included in the PDF definition range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class delta_chi2_gen(rv_continuous):\n",
    "    \"\"\"\n",
    "    Class for a probability denstiy function modelled by using a:math`\\chi^2`\n",
    "    distribution for :math:`x > 0` and a constant fraction :math:`1 - \\eta`\n",
    "    of zero trials for :math`x = 0` (like a delta peak at 0).\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The probability density function for `delta_chi2` is:\n",
    "\n",
    "    .. math::\n",
    "\n",
    "      \\text{PDF}(x|\\text{df}, \\eta) =\n",
    "          \\begin{cases}\n",
    "              (1-\\eta)                &\\text{for } x=0 \\\\\n",
    "              \\eta\\chi^2_\\text{df}(x) &\\text{for } x>0 \\\\\n",
    "          \\end{cases}\n",
    "\n",
    "    `delta_chi2` takes ``df`` and ``eta``as a shape parameter, where ``df`` is\n",
    "    the standard :math:`\\chi^2_\\text{df}` degrees of freedom parameter and\n",
    "    ``1-eta`` is the fraction of the contribution of the delta function at zero.\n",
    "    \"\"\"\n",
    "    def _rvs(self, df, eta):\n",
    "        # Determine fraction of zeros by drawing from binomial with p=eta\n",
    "        s = self._size if not len(self._size) == 0 else 1\n",
    "        nzeros = self._random_state.binomial(n=s, p=1. - eta, size=None)\n",
    "        # If None, len of size is 0 and single scalar rvs requested\n",
    "        if len(self._size) == 0:\n",
    "            if nzeros == 1:\n",
    "                return 0.\n",
    "            else:\n",
    "                return self._random_state.chisquare(df, size=None)\n",
    "        # If no zeros or no chi2 is drawn for this trial, only draw one type\n",
    "        if nzeros == self._size:\n",
    "            return np.zeros(nzeros, dtype=np.float)\n",
    "        if nzeros == 0:\n",
    "            return self._random_state.chisquare(df, size=self._size)\n",
    "        # All other cases: Draw, concatenate and shuffle to simulate a per\n",
    "        # random number Bernoulli process with p=eta\n",
    "        out = np.r_[np.zeros(nzeros, dtype=np.float),\n",
    "                    self._random_state.chisquare(df,\n",
    "                                                 size=(self._size - nzeros))]\n",
    "        self._random_state.shuffle(out)\n",
    "        return out\n",
    "\n",
    "    def _pdf(self, x, df, eta):\n",
    "        x = np.asarray(x)\n",
    "        return np.where(x > 0., eta * scs.chi2.pdf(x, df=df), 1. - eta)\n",
    "\n",
    "    def _logpdf(self, x, df, eta):\n",
    "        x = np.asarray(x)\n",
    "        return np.where(x > 0., np.log(eta) + scs.chi2.logpdf(x, df=df),\n",
    "                        np.log(1. - eta))\n",
    "\n",
    "    def _cdf(self, x, df, eta):\n",
    "        x = np.asarray(x)\n",
    "        return np.where(x > 0., (1. - eta) + eta * scs.chi2.cdf(x, df=df),\n",
    "                        (1. - eta))\n",
    "\n",
    "    def _logcdf(self, x, df, eta):\n",
    "        x = np.asarray(x)\n",
    "        return np.where(x > 0., np.log(1 - eta + eta * scs.chi2.cdf(x, df)),\n",
    "                        np.log(1. - eta))\n",
    "\n",
    "    def _sf(self, x, df, eta):\n",
    "        # Note: Define sf(0)=0 and not sf(0)=(1-eta) because 0 is inclusive\n",
    "        x = np.asarray(x)\n",
    "        return np.where(x > 0., eta * scs.chi2.sf(x, df), 1.)\n",
    "\n",
    "    def _logsf(self, x, df, eta):\n",
    "        # Note: Define sf(0)=0 and not sf(0)=(1-eta) because 0 is inclusive\n",
    "        x = np.asarray(x)\n",
    "        return np.where(x > 0., np.log(eta) + scs.chi2.logsf(x, df), 0.)\n",
    "\n",
    "    def _ppf(self, p, df, eta):\n",
    "        p = np.asarray(p)\n",
    "        return np.where(p > (1. - eta), scs.chi2.ppf(1 + (p - 1) / eta, df), 0.)\n",
    "\n",
    "    def _isf(self, p, df, eta):\n",
    "        return np.where(p < eta, scs.chi2.isf(p / eta, df), 0.)\n",
    "\n",
    "\n",
    "delta_chi2 = delta_chi2_gen(name=\"delta_chi2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x = np.linspace(0, 2, 200)\n",
    "p = np.linspace(0, 1, 200)\n",
    "eta = 0.7\n",
    "df = 1\n",
    "\n",
    "chi2 = scs.chi2(df=df)\n",
    "dchi2 = delta_chi2(df=df, eta=eta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**PDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, (axl, axr) = plt.subplots(1, 2, figsize=(14, 6));\n",
    "\n",
    "axl.plot(x, chi2.pdf(x), lw=3)\n",
    "axl.set_title(r\"Vanilla $\\chi^2_{}$\".format(df))\n",
    "\n",
    "axr.plot(x, dchi2.pdf(x), lw=3, c=\"C3\")\n",
    "axr.set_title(r\"$(1-{:.1f}) + {:.1f}\\times \\chi^2_{}$\".format(eta, eta, df))\n",
    "\n",
    "for axi in (axl, axr):\n",
    "    axi.axhline(1. - eta, 0, 1, ls=\"--\", color=\"k\", label=r\"$1-\\eta$\")\n",
    "    axi.set_xlabel(\"x\")\n",
    "    axi.set_ylabel(\"PDF\")\n",
    "    axi.set_xlim(x[0], x[-1])\n",
    "    axi.set_ylim(0, 4)\n",
    "    axi.grid(ls=\"--\")\n",
    "    axi.legend(loc=\"upper right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**ln(PDF)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Test if exp(logPDF) == PDF\n",
    "print(\"exp(logPDF) is PDF: \", np.allclose(np.exp(dchi2.logpdf(x)),\n",
    "                                          dchi2.pdf(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, (axl, axr) = plt.subplots(1, 2, figsize=(14, 6));\n",
    "\n",
    "axl.plot(x, chi2.logpdf(x), lw=3)\n",
    "axl.set_title(r\"Vanilla $\\chi^2_{}$\".format(df))\n",
    "\n",
    "axr.plot(x, dchi2.logpdf(x), lw=3, c=\"C3\")\n",
    "axr.set_title(r\"$(1-{:.1f}) + {:.1f}\\times \\chi^2_{}$\".format(eta, eta, df))\n",
    "\n",
    "for axi in (axl, axr):\n",
    "    axi.axhline(np.log(1. - eta), 0, 1, ls=\"--\", color=\"k\", label=r\"$1-\\eta$\")\n",
    "    axi.set_xlabel(\"x\")\n",
    "    axi.set_ylabel(\"ln(PDF)\")\n",
    "    axi.set_xlim(x[0], x[-1])\n",
    "    axi.set_ylim(-2, 2)\n",
    "    axi.grid(ls=\"--\")\n",
    "    axi.legend(loc=\"upper right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**CDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, (axl, axr) = plt.subplots(1, 2, figsize=(14, 6));\n",
    "\n",
    "axl.plot(x, chi2.cdf(x), lw=3)\n",
    "axl.set_title(r\"Vanilla $\\chi^2_{}$\".format(df))\n",
    "\n",
    "axr.plot(x, dchi2.cdf(x), lw=3, c=\"C3\")\n",
    "axr.set_title(r\"$(1-{:.1f}) + {:.1f}\\times \\chi^2_{}$\".format(eta, eta, df))\n",
    "\n",
    "for axi in (axl, axr):\n",
    "    axi.axhline(1. - eta, 0, 1, ls=\"--\", color=\"k\", label=r\"$1-\\eta$\")\n",
    "    axi.set_xlabel(\"x\")\n",
    "    axi.set_ylabel(\"CDF\")\n",
    "    axi.set_xlim(x[0], x[-1])\n",
    "    axi.set_ylim(0, 1)\n",
    "    axi.grid(ls=\"--\")\n",
    "    axi.legend(loc=\"upper right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**ln(cdf)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Test if exp(logCDF) == CDF\n",
    "print(\"exp(logCDF) is CDF: \", np.allclose(np.exp(dchi2.logcdf(x)),\n",
    "                                          dchi2.cdf(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, (axl, axr) = plt.subplots(1, 2, figsize=(14, 6));\n",
    "\n",
    "axl.plot(x, chi2.logcdf(x), lw=3)\n",
    "axl.set_title(r\"Vanilla $\\chi^2_{}$\".format(df))\n",
    "\n",
    "axr.plot(x, dchi2.logcdf(x), lw=3, c=\"C3\")\n",
    "axr.set_title(r\"$(1-{:.1f}) + {:.1f}\\times \\chi^2_{}$\".format(eta, eta, df))\n",
    "\n",
    "for axi in (axl, axr):\n",
    "    axi.axhline(np.log(1. - eta), 0, 1, ls=\"--\", color=\"k\", label=r\"$1-\\eta$\")\n",
    "    axi.set_xlabel(\"x\")\n",
    "    axi.set_ylabel(\"ln(CDF)\")\n",
    "    axi.set_xlim(x[0], x[-1])\n",
    "    axi.set_ylim(-2, 0)\n",
    "    axi.grid(ls=\"--\")\n",
    "    axi.legend(loc=\"upper right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**PPF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, (axl, axr) = plt.subplots(1, 2, figsize=(14, 6));\n",
    "\n",
    "axl.plot(p, chi2.ppf(p), lw=3)\n",
    "axl.set_title(r\"Vanilla $\\chi^2_{}$\".format(df))\n",
    "\n",
    "axr.plot(p, dchi2.ppf(p), lw=3, c=\"C3\")\n",
    "axr.set_title(r\"$(1-{:.1f}) + {:.1f}\\times \\chi^2_{}$\".format(eta, eta, df))\n",
    "\n",
    "# Overlay rotated CDF, should be the same function (graphical inversion)\n",
    "axr.plot(dchi2.cdf(x), x, lw=2, ls=\"--\", c=\"k\", label=\"inverted CDF\")\n",
    "\n",
    "for axi in (axl, axr):\n",
    "    axi.axvline(1. - eta, 0, 1, ls=\"--\", color=\"k\", label=r\"$1-\\eta$\")\n",
    "    axi.set_xlabel(\"p\")\n",
    "    axi.set_ylabel(\"PPF\")\n",
    "    axi.set_xlim(p[0], p[-1])\n",
    "    axi.set_ylim(x[0], x[-1])\n",
    "    axi.grid(ls=\"--\")\n",
    "    axi.legend(loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Survival function, SF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, (axl, axr) = plt.subplots(1, 2, figsize=(14, 6));\n",
    "\n",
    "axl.plot(x, chi2.sf(x), lw=3)\n",
    "axl.set_title(r\"Vanilla $\\chi^2_{}$\".format(df))\n",
    "\n",
    "axr.plot(x, dchi2.sf(x), lw=3, c=\"C3\")\n",
    "axr.set_title(r\"$(1-{:.1f}) + {:.1f}\\times \\chi^2_{}$\".format(eta, eta, df))\n",
    "\n",
    "# Overlay 1-CDF, should be the same function, except for x=0\n",
    "axr.plot(x, 1. - dchi2.cdf(x), lw=2, ls=\"--\", c=\"k\", label=\"1-CDF\")\n",
    "\n",
    "for axi in (axl, axr):\n",
    "    axi.set_xlabel(\"x\")\n",
    "    axi.set_ylabel(\"CDF\")\n",
    "    axi.set_xlim(x[0], x[-1])\n",
    "    axi.set_ylim(0, 1)\n",
    "    axi.grid(ls=\"--\")\n",
    "axr.legend(loc=\"upper right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**ln(SF)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Test if exp(logCDF) == CDF\n",
    "print(\"exp(logSF) is SF: \", np.allclose(np.exp(dchi2.logsf(x)),\n",
    "                                          dchi2.sf(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, (axl, axr) = plt.subplots(1, 2, figsize=(14, 6));\n",
    "\n",
    "axl.plot(x, chi2.logsf(x), lw=3)\n",
    "axl.set_title(r\"Vanilla $\\chi^2_{}$\".format(df))\n",
    "\n",
    "axr.plot(x, dchi2.logsf(x), lw=3, c=\"C3\")\n",
    "axr.set_title(r\"$(1-{:.1f}) + {:.1f}\\times \\chi^2_{}$\".format(eta, eta, df))\n",
    "\n",
    "for axi in (axl, axr):\n",
    "    axi.set_xlabel(\"x\")\n",
    "    axi.set_ylabel(\"ln(CDF)\")\n",
    "    axi.set_xlim(x[0], x[-1])\n",
    "    axi.set_ylim(-2, 0)\n",
    "    axi.grid(ls=\"--\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**inverse SF, ISF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, (axl, axr) = plt.subplots(1, 2, figsize=(14, 6));\n",
    "\n",
    "axl.plot(p, chi2.isf(p), lw=3)\n",
    "axl.set_title(r\"Vanilla $\\chi^2_{}$\".format(df))\n",
    "\n",
    "axr.plot(p, dchi2.isf(p), lw=3, c=\"C3\")\n",
    "axr.set_title(r\"$(1-{:.1f}) + {:.1f}\\times \\chi^2_{}$\".format(eta, eta, df))\n",
    "\n",
    "# Overlay rotated SF, should be the same function (graphical inversion)\n",
    "axr.plot(dchi2.sf(x), x, lw=2, ls=\"--\", c=\"k\", label=\"inverted SF\")\n",
    "\n",
    "for axi in (axl, axr):\n",
    "    axi.set_xlabel(\"p\")\n",
    "    axi.set_ylabel(\"PPF\")\n",
    "    axi.set_xlim(p[0], p[-1])\n",
    "    axi.set_ylim(x[0], x[-1])\n",
    "    axi.grid(ls=\"--\")\n",
    "axr.legend(loc=\"upper right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Random numbers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, (axl, axr) = plt.subplots(1, 2, figsize=(14, 6));\n",
    "\n",
    "# Draw RVS, seperate almost pure zero bin\n",
    "bins = np.arange(x[0], x[-1], 0.05)\n",
    "nrnd = 10000\n",
    "chi2rvs = chi2.rvs(size=nrnd)\n",
    "dchi2rvs = dchi2.rvs(size=nrnd)\n",
    "\n",
    "axl.hist(chi2rvs, bins=bins, normed=True, color=\"C7\", log=True)\n",
    "axl.plot(x, chi2.pdf(x), lw=3)\n",
    "axl.set_title(r\"Vanilla $\\chi^2_{}$\".format(df))\n",
    "\n",
    "axr.hist(dchi2rvs, bins=bins, normed=True, color=\"C7\", log=True)\n",
    "axr.plot(x, dchi2.pdf(x), lw=3, c=\"C3\")\n",
    "axr.set_title(r\"$(1-{:.1f}) + {:.1f}\\times \\chi^2_{}$\".format(eta, eta, df))\n",
    "\n",
    "for axi in (axl, axr):\n",
    "    axi.set_xlabel(\"x\")\n",
    "    axi.set_ylabel(\"ln(PDF)\")\n",
    "    axi.set_xlim(x[0], x[-1])\n",
    "    axi.grid(ls=\"--\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**The old comparison, also showing that skylabs logCDF is wrong**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = 1\n",
    "eta = 0.7\n",
    "x = np.linspace(0, 10, 200)\n",
    "\n",
    "# lin vanilla\n",
    "cdf = scs.chi2.cdf(x, df)\n",
    "sf = scs.chi2.sf(x, df)\n",
    "sf_ = 1 - cdf\n",
    "# delta chi2\n",
    "cdf_eta = np.where(x > 0, 1 - eta + eta * scs.chi2.cdf(x, df), 1 - eta)\n",
    "sf_eta = np.where(x > 0, eta * scs.chi2.sf(x, df), 1.)\n",
    "sf_eta_ = 1 - cdf_eta  # Incorrect here, sf(0) = 1\n",
    "\n",
    "#log vanilla\n",
    "logcdf = scs.chi2.logcdf(x, df)\n",
    "logsf = scs.chi2.logsf(x, df)\n",
    "logsf_ = np.log(1 - cdf)\n",
    "# delta chi2\n",
    "logcdf_eta = np.where(x > 0., np.log(1 - eta + eta * scs.chi2.cdf(x, df)),\n",
    "                      np.log(1. - eta))\n",
    "logcdf_eta_2 = np.log(cdf_eta)\n",
    "logsf_eta =  np.where(x > 0, np.log(eta) + scs.chi2.logsf(x, df), 0.)\n",
    "logsf_eta_ = np.log(1 - cdf_eta)  # Incorrect here, sf(0) = 1\n",
    "\n",
    "#skylab\n",
    "cdf_eta_skylab = (1. - eta) + eta * scs.chi2.cdf(x, df)\n",
    "logcdf_eta_skylab = np.log(1. - eta) + np.log(eta) + scs.chi2.logcdf(x, df)\n",
    "\n",
    "# Check exp(log) = lin\n",
    "print(\"CDF: \", np.allclose(np.exp(logcdf_eta), cdf_eta))\n",
    "print(\"SF: \", np.allclose(np.exp(logsf_eta), sf_eta))\n",
    "print(\"CDF skylab: \", np.allclose(np.exp(logcdf_eta_skylab), cdf_eta_skylab))\n",
    "\n",
    "# lin\n",
    "plt.plot(x, cdf)\n",
    "plt.plot(x, sf)\n",
    "plt.plot(x, sf_, ls=\"--\")\n",
    "plt.axhline(1,0,1,ls=\"--\", c=\"k\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(x, cdf_eta)\n",
    "plt.plot(x, sf_eta)\n",
    "plt.plot(x, sf_eta_, ls=\"--\")\n",
    "plt.plot(x, cdf_eta_skylab, ls=\"--\")\n",
    "plt.axhline(1,0,1,ls=\"--\", c=\"k\")\n",
    "plt.show()\n",
    "\n",
    "# log\n",
    "plt.plot(x, logcdf)\n",
    "plt.plot(x, logsf)\n",
    "plt.plot(x, logsf_, ls=\"--\")\n",
    "plt.axhline(0,0,1,ls=\"--\", c=\"k\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(x, logcdf_eta)\n",
    "plt.plot(x, logcdf_eta_2, c=\"k\", ls=\"--\")\n",
    "plt.plot(x, logsf_eta)\n",
    "plt.plot(x, logsf_eta_, ls=\"--\")\n",
    "plt.plot(x, logcdf_eta_skylab, ls=\"--\")\n",
    "plt.axhline(0,0,1,ls=\"--\", c=\"k\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Placeholder for new test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Initialisation Cell",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2 (OSX)",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

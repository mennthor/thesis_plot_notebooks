{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some more side tests to clarify / justify details, that would clutter the main test notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import helper as hlp\n",
    "\n",
    "import numpy as np\n",
    "import numpy.lib.recfunctions\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mpldates\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.colors import LogNorm, Normalize\n",
    "%matplotlib inline\n",
    "\n",
    "import scipy.interpolate as sci\n",
    "import scipy.optimize as sco\n",
    "import scipy.integrate as scint\n",
    "import scipy.stats as scs\n",
    "import scipy.signal as scsignal\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import json\n",
    "import datetime\n",
    "import pickle\n",
    "from astropy.time import Time as astrotime\n",
    "from corner import corner\n",
    "\n",
    "import sklearn.neighbors as skn\n",
    "import sklearn.model_selection as skms  # Newer version of grid_search\n",
    "\n",
    "from corner_hist import corner_hist\n",
    "from anapymods3.plots.general import (split_axis, get_binmids,\n",
    "                                      hist_marginalize, dg)\n",
    "from anapymods3.stats.sampling import rejection_sampling\n",
    "from anapymods3.general.misc import (fill_dict_defaults,\n",
    "                                     flatten_list_of_1darrays)\n",
    "\n",
    "import tdepps.bg_injector as BGInj\n",
    "import tdepps.bg_rate_injector as BGRateInj\n",
    "import tdepps.rate_function as RateFunc\n",
    "import tdepps.llh as LLH\n",
    "import tdepps.analysis as Analysis\n",
    "\n",
    "secinday = 24. * 60. * 60."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Load IC86 data from epinat, which should be the usual IC86-I (2011) PS sample, but pull corrected and OneWeights corrected by number of events generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "exp = np.load(\"data/IC86_I_data.npy\")\n",
    "mc = np.load(\"data/IC86_I_mc.npy\")\n",
    "# Use the officially stated livetime, not the ones from below\n",
    "livetime = 332.61"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Data livetime comparison to v1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's compare to the v1.4 list, as used by jfeintzig.\n",
    "Oddly we have 0.2 days less livetime as he had.\n",
    "The number of runs is correct though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# New livetime from iclive\n",
    "run_list = hlp.get_run_list()\n",
    "run_dict = hlp.get_run_dict(run_list)\n",
    "inc_run_arr, ic_livetime = hlp.get_good_runs(run_dict)\n",
    "\n",
    "print(\"Total runs from iclive     : \", len(inc_run_arr))\n",
    "print(\"IC86-I livetime from iclive: \", ic_livetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# For comparison, also parse the v1.4 list\n",
    "# Should be: 1081 runs, with a total livetime of 332.61 days.\n",
    "with open(\"data/Prelim_IC86-I_v1.4a.txt\",'r') as f:\n",
    "    data = []\n",
    "    for line in f.readlines():\n",
    "        data.append(line.replace('\\n',''))\n",
    "        \n",
    "# Skip to beginning of run info\n",
    "data = data[73:]\n",
    "\n",
    "# Split at white space\n",
    "data = [d.split() for d in data]\n",
    "\n",
    "dtype = [(\"runID\", np.int), (\"duration\", np.float), (\"IT\", \"|S2\"),\n",
    "         (\"CONF\", \"|S7\"), (\"FLAG\", \"|S6\")]\n",
    "runlist = np.empty((len(data),), dtype=dtype)\n",
    "\n",
    "runlist[\"runID\"] = np.array([int(d[0]) for d in data])\n",
    "runlist[\"duration\"] = np.array([float(d[3]) for d in data])\n",
    "runlist[\"IT\"] = np.array([d[5] for d in data])\n",
    "runlist[\"CONF\"] = np.array([d[6] for d in data])\n",
    "runlist[\"FLAG\"] = np.array([d[7] for d in data])\n",
    "\n",
    "# Now filter: Include IT=it, CONF=full, FLAG=GOOD, exclude strange rate runs\n",
    "exclude_rate = [120028, 120029, 120030, 120087, 120156, 120157]\n",
    "itgood = runlist[\"IT\"] == b\"IT\"  # Somehow only bitwise comparison is non-empty\n",
    "confgood = runlist[\"CONF\"] == b\"full\"\n",
    "flaggood = runlist[\"FLAG\"] == b\"GOOD\"\n",
    "ratebad = np.in1d(runlist[\"runID\"], exclude_rate)\n",
    "\n",
    "include = itgood & confgood & flaggood & ~ratebad\n",
    "runlist_inc = runlist[include]\n",
    "\n",
    "# Get the livetime of the sample in days\n",
    "hoursindays = 24.\n",
    "secinday = hoursindays * 60. * 60.\n",
    "old_livetime = np.sum(runlist_inc[\"duration\"]) / hoursindays\n",
    "\n",
    "print(\"Total runs from v1.4     : \", len(runlist_inc))\n",
    "print(\"Total livetime from v1.4 : \", old_livetime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's see, if the 120 extra runs in the new runlist make up for the difference of about 10 days in livetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "iclive_in_old = np.in1d(inc_run_arr[\"runID\"], runlist_inc[\"runID\"])\n",
    "not_in_old = inc_run_arr[~iclive_in_old]\n",
    "\n",
    "start = not_in_old[\"start_mjd\"]\n",
    "stop = not_in_old[\"stop_mjd\"]\n",
    "missing_livetime = np.sum(stop - start)\n",
    "\n",
    "print(\"\\nOfficial IC86-I PS livetime: \", livetime)\n",
    "print(\"Total livetime from v1.4   : \", old_livetime)\n",
    "print(\"IC86-I livetime from iclive: \", ic_livetime)\n",
    "\n",
    "print(\"\\nMissing runs in old: \", len(not_in_old))\n",
    "print(\"Livetime icliv - old :\", ic_livetime - old_livetime)\n",
    "\n",
    "print(\"\\nDiff from summing missing runs           : \", missing_livetime)\n",
    "print(\"New iclive livetime with same runs as old: \",\n",
    "      ic_livetime - missing_livetime)\n",
    "\n",
    "print(\"\\nTotal rate [Hz] over total livetime: \",\n",
    "      len(exp) / (livetime * secinday))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "All runs from the new run list that zero events, make up for the missing runs in the old runlist, so this is consisting.\n",
    "\n",
    "Dont't know though, where the missing 0,2 days come from. Probably some runtimes have shifted a little making some extra livetime in the new list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Store events in bins with run borders\n",
    "exp_times = exp[\"timeMJD\"]\n",
    "start_mjd = inc_run_arr[\"start_mjd\"]\n",
    "stop_mjd = inc_run_arr[\"stop_mjd\"]\n",
    "\n",
    "tot = 0\n",
    "evts_in_run = {}\n",
    "for start, stop , runid in zip(start_mjd, stop_mjd, inc_run_arr[\"runID\"]):\n",
    "    mask = (exp_times >= start) & (exp_times < stop)\n",
    "    evts_in_run[runid] = exp[mask]\n",
    "    tot += np.sum(mask)\n",
    "    \n",
    "# Crosscheck, if we got all events and counted nothing double\n",
    "print(\"Do we have all events? \", tot == len(exp))\n",
    "print(\"  Events selected : \", tot)\n",
    "print(\"  Events in exp   : \", len(exp))\n",
    "\n",
    "# Create binmids and histogram values in each bin\n",
    "binmids = 0.5 * (start_mjd + stop_mjd)\n",
    "h = np.zeros(len(binmids), dtype=np.float)\n",
    "\n",
    "for i, evts in enumerate(evts_in_run.values()):\n",
    "    h[i] = len(evts)\n",
    "    \n",
    "m = (h > 0)\n",
    "print(\"Runs with 0 events :\", np.sum(~m))\n",
    "print(\"Runtime in those runs: \", np.sum(inc_run_arr[\"stop_mjd\"][~m] -\n",
    "                                        inc_run_arr[\"start_mjd\"][~m]))\n",
    "\n",
    "# Remove all zero event runs (artifacts from new run list) and calc the rate\n",
    "stop_mjd, start_mjd = stop_mjd[m], start_mjd[m]\n",
    "h = h[m] / ((stop_mjd - start_mjd) * secinday)\n",
    "binmids = binmids[m]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Time dependent rate function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Note: I think it is unnecessary to use a time and declination dependent rate. The spatial part is injected from the data BG from KDE anyways. So we just need to have the rate to determine how much events we inject allsky.**\n",
    "\n",
    "Rate ist time dependent because of seasonal variation.\n",
    "We take this varariation into account by fitting a priodic function to the time resolved rate.\n",
    "\n",
    "The data is built by calculating the rate in each run as seen before.\n",
    "This rate is correctly normalized and smoothes local fluctuations.\n",
    "\n",
    "### Peridoc function with a weighted least squares fit\n",
    "\n",
    "See side_test for comparison to spline fits.\n",
    "The function is a simple sinus scalable by 4 parameters to fit the shape of the rates:\n",
    "\n",
    "$$\n",
    "    f(x) = a\\cdot \\sin(b\\cdot(x - c)) + d\n",
    "$$\n",
    "\n",
    "The least squares loss function is\n",
    "\n",
    "$$\n",
    "    R = \\sum_i (w_i(y_i - f(x_i)))^2\n",
    "$$\n",
    "\n",
    "Weights are standard deviations from poisson histogram error.\n",
    "\n",
    "$$\n",
    "    w_i = \\frac{1}{\\sigma_i}\n",
    "$$\n",
    "\n",
    "Seed values are estimated from plot rate vs time.\n",
    "\n",
    "- Period should be 365 days (MJD) because we have one year of data so we choose $b0 = 2\\pi/365$.\n",
    "- Amplitude is about $a_0=-0.0005$, because sinus seems to start with negative values.\n",
    "- The x-offset is choose as the first start date, to get the right order of magnitude.\n",
    "- The y-axis intersection $d$ schould be close to the weighted average, so we take this as a seed.\n",
    "\n",
    "The bounds are motivated as follows (and if we don't hit them, it's OK to use them).\n",
    "\n",
    "- Amplitude $a$ should be positive, this also resolves a degenracy between a-axis offset.\n",
    "- The period $b$ should scatter around one year, a period larger than +-1 half a year is unphysical.\n",
    "- The x-offset $c$ cannot be greater than the initial +- the period because we have a periodic function.\n",
    "- The y-axis offset $d$ is arbitrarily constrained, but as seen from the plot it should not exceed 0.1. \n",
    "\n",
    "## Proposed was something like this\n",
    "\n",
    "Rate ist time dependent because of seasonal variation and delination dependent because the detector acceptance is declination dependent.\n",
    "A correletation should not exist or be very small.\n",
    "\n",
    "So we express the rate in depence of time and decliantion as\n",
    "\n",
    "$$\n",
    "    R(t,\\delta) = R_T(t)\\cdot R_\\delta(t)\n",
    "$$\n",
    "\n",
    "with independent parts in declination and time each.\n",
    "\n",
    "The time parts is constructed by fitting a periodic function.\n",
    "Then we seperatly fit a spline to the total $\\sin(\\delta)$ distribtuion and normalizes it over the declination range of the sample.\n",
    "For a given time and declination the smooth function $R(t, \\delta)$ gives the correct detector rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Splinefit to sinDec\n",
    "\n",
    "First we fit a spline to the sinDec distribtuion for all events.\n",
    "This is equivalent to what is done in skylab to estimate the per signal background PDF from data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# This is equivalent to skylab's baclground PDF construction\n",
    "sinDec_bins = 25\n",
    "sinDec_range= [-1, 1]\n",
    "sinDec_hist, sinDec_bins = np.histogram(exp[\"sinDec\"], density=True,\n",
    "                                        bins=sinDec_bins, range=sinDec_range)\n",
    "\n",
    "m = get_binmids([sinDec_bins])[0]\n",
    "\n",
    "if np.any(sinDec_hist <= 0.):\n",
    "    raise ValueError((\"Declination hist bins empty, this must not happen. \"\n",
    "                      +\"Empty bin idx: {}\".format(\n",
    "                          np.arange(len(m))[sinDec_hist <= 0.])))\n",
    "\n",
    "# Fit to logarithm, to avoid ringing. Raise err if evaluated outside range\n",
    "sinDec_spline = sci.InterpolatedUnivariateSpline(m, np.log(sinDec_hist),\n",
    "                                                 k=3, ext=\"extrapolate\")\n",
    "\n",
    "# Normalize to area on whole sky = 1, so norm = 2pi * integral(exp(spl))\n",
    "def sinDec_pdf_(x):\n",
    "    return np.exp(sinDec_spline(x))\n",
    "\n",
    "norm = scint.quad(sinDec_pdf_, -1, 1)[0] * 2. * np.pi\n",
    "\n",
    "def sinDec_pdf(x):\n",
    "    return (np.exp(sinDec_spline(x))) / norm\n",
    "\n",
    "print(\"SinDec pdf has area on 4pi = \", scint.quad(\n",
    "    sinDec_pdf, -1, 1)[0] * 2 * np.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x = np.linspace(sinDec_bins[0], sinDec_bins[-1], 100)\n",
    "y = sinDec_pdf(x)\n",
    "\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Try to hist dec dependence of single run\n",
    "# As expected we have not enough statistic to see anything\n",
    "run = 50\n",
    "start = start_mjd[run]\n",
    "stop = stop_mjd[run]\n",
    "mask = (exp_times >= start) & (exp_times < stop)\n",
    "_sinDec_run = np.sin(exp[\"dec\"][mask])\n",
    "\n",
    "plt.hist(_sinDec_run, range=[-1, 1], bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Spline to rate distribution\n",
    "\n",
    "Choosing spline weights according to scipy.interpolate.UnivariateSpline manual:\n",
    "\n",
    "    If None (default), s = len(w) which should be a good value if 1/w[i] is an estimate of\n",
    "    the standard deviation of y[i].\n",
    "\n",
    "Internally it is doing a weighted least squares fit with $\\sum_i(w_i(y_i-\\text{spl}(x_i)))^2 \\leq s$.\n",
    "We leave $s$ as the default because we have an estimate for the stddevs: $\\sigma_i = \\sqrt{h_i}$.\n",
    "To match the definition of the weights we use:\n",
    "\n",
    "$$\n",
    "    \\frac{1}{w_i} \\stackrel{!}{=} \\sigma_i = \\sqrt{h_i} \\Leftrightarrow w_i \n",
    "                               = \\frac{1}{\\sqrt{h_i}}\n",
    "$$\n",
    "\n",
    "Because we scaled $h_i$ to get the rate in events per s, we need to scale the errors too:\n",
    "\n",
    "$$\n",
    "    \\tilde{h}_i = \\frac{h_i}{s_i} \\Rightarrow \\tilde{\\sigma}_i = \\frac{\\sqrt{h_i}}{s_i} \n",
    "                = \\frac{\\sqrt{s_i\\tilde{h}_i}}{s} \n",
    "                = \\sqrt{\\frac{\\tilde{h}_i}{s_i}} = \\frac{1}{w_i}\n",
    "$$\n",
    "\n",
    "The smoothing condition `s` chooses the support knots based on the weights.\n",
    "Because we have some oscilating pattern due to to seasonal variations (periode ~1yr) a quadratic spline function is not enough.\n",
    "So we choose the next higher order, a cubic spline, which is able to oscilate up and down exatcly once.\n",
    "\n",
    "**Note:** If a weight is zero, the corresponding point doesn't contribute at all.\n",
    "So we might consider using $w_i = \\sigma_i$ instead.\n",
    "Then point woth high poisson statsitics are preferred over low statistic bins.\n",
    "It doesn't seem to make a huge difference though.\n",
    "\n",
    "Below we try both weights and the unweighted case.\n",
    "For the 'correctly' weighted case with $w_i = 1. / \\sigma_i$ the spline oscillates strongly.\n",
    "So we better try a true perdiodic function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# h is already scaled, so we need to scale the errors too\n",
    "yerr = np.sqrt(h) / np.sqrt((stop_mjd - start_mjd) * secinday)\n",
    "w = 1. / yerr\n",
    "rate_spline = sci.UnivariateSpline(binmids, h, k=3, w=w,\n",
    "                                   s=None, ext=\"extrapolate\")\n",
    "\n",
    "rate_spline_inv = sci.UnivariateSpline(binmids, h, k=3, w=1. / w,\n",
    "                                       s=None, ext=\"extrapolate\")\n",
    "\n",
    "rate_spline_unw = sci.UnivariateSpline(binmids, h, k=3, w=None,\n",
    "                                       s=None, ext=\"extrapolate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Plot runs\n",
    "xerr = 0.5 * (stop_mjd - start_mjd)\n",
    "plt.errorbar(binmids, h, xerr=0, yerr=yerr, fmt=\",\")\n",
    "plt.ylim(0, None);\n",
    "\n",
    "# Plot spline\n",
    "x = np.linspace(start_mjd[0], stop_mjd[-1], 200)\n",
    "y = rate_spline(x)\n",
    "plt.plot(x, y, zorder=5, lw=2, color=\"k\", label=\"w=1/std\")\n",
    "\n",
    "# Plot weighted average. Weights are variance to resemble stddev weighted\n",
    "# least squares fit\n",
    "avg = np.average(h, weights=yerr**2)\n",
    "plt.axhline(avg, 0, 1, color=\"k\", ls=\"--\", zorder=5)\n",
    "\n",
    "# Plot unweighted mean and spline\n",
    "y = rate_spline_unw(x)\n",
    "plt.plot(x, y, zorder=5, lw=2, color=\"r\")\n",
    "\n",
    "avg = np.mean(h)\n",
    "plt.axhline(avg, 0, 1, color=\"r\", ls=\"--\", zorder=5, label=\"w=1\")\n",
    "\n",
    "# Plot with inverse weights\n",
    "y = rate_spline_inv(x)\n",
    "plt.plot(x, y, zorder=5, lw=2, color=\"g\", label=\"w=std\")\n",
    "avg = np.average(h, weights=1. / yerr**2)\n",
    "plt.axhline(avg, 0, 1, color=\"g\", ls=\"--\", zorder=5)\n",
    "\n",
    "plt.xlim(start_mjd[0], stop_mjd[-1])\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./data/figs/time_rate_splines.png\", dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Periodic function fit\n",
    "\n",
    "Try a peridoc function with a weighted least squares fit.\n",
    "\n",
    "$$\n",
    "    f(x) = a\\cdot \\sin(b\\cdot(x - c)) + d\n",
    "$$\n",
    "\n",
    "The least squares loss function is\n",
    "\n",
    "$$\n",
    "    R = \\sum_i (w_i(y_i - f(x_i)))^2\n",
    "$$\n",
    "\n",
    "Weights are standard deviations from poisson histogram error.\n",
    "\n",
    "$$\n",
    "    w_i = \\frac{1}{\\sigma_i}\n",
    "$$\n",
    "\n",
    "Seed values are estimated from plot rate vs time.\n",
    "Period should be 365 days (MJD) because we have one year of data so we choose $b0 = 2\\pi/365$.\n",
    "Amplitude is about $a_0=-0.0005$, because sinus seems to start with negative values.\n",
    "The x-offset is choose as the first start date, to get the right order of magnitude.\n",
    "The y-axis intersection $d$ schould be close to the weighted average, so we take this as a seed.\n",
    "\n",
    "The bounds are motivated as follows (and if we don't hit them, it's OK to use them).\n",
    "Amplitude $a$ should be positive, this also resolves a degenracy between a-axis offset.\n",
    "The period $b$ should scatter around one year, a period larger than +-1 half a year is unphysical.\n",
    "The x-offset $c$ cannot be greater than the initial +- the period because we have a periodic function.\n",
    "The y-axis offset $d$ is arbitrarily constrained, but as seen from the plot it should not exceed 0.1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def f(x, args):\n",
    "    a, b, c, d = args\n",
    "    return a * np.sin(b * (x - c)) + d\n",
    "\n",
    "def lstsq(pars, *args):\n",
    "    \"\"\"\n",
    "    Weighted leastsquares min sum((wi * (yi - fi))**2)\n",
    "    \"\"\"\n",
    "    # data x,y-values and weights are fixed\n",
    "    x, y, w = args[0], args[1], args[2]\n",
    "    # Params get fitted\n",
    "    a, b, c, d = pars[0], pars[1], pars[2], pars[3]\n",
    "    # Target function\n",
    "    f = a * np.sin(b * (x - c)) + d\n",
    "    # Least squares loss\n",
    "    return np.sum((w * (y - f))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Seed values from consideration above.\n",
    "a0 = -0.0005\n",
    "b0 = 2. * np.pi / 365.\n",
    "c0 = np.amin(start_mjd)\n",
    "d0 = np.average(h, weights=yerr**2)\n",
    "\n",
    "x0 = [a0, b0, c0, d0]\n",
    "# Bounds as explained above\n",
    "bounds = [[None, None], [0.5 * b0, 1.5 * b0], [c0 - b0, c0 + b0, ], [0, 0.01]]\n",
    "# x, y values, weights\n",
    "args = (binmids, h, 1. / yerr)\n",
    "\n",
    "res = sco.minimize(fun=lstsq, x0=x0, args=args, bounds=bounds)\n",
    "\n",
    "for i, name in enumerate([\"Amplitude a\", \"Period b\", \"x-Shift c\", \"y-axis d\"]):\n",
    "    print(name, \" : \", res.x[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Plot runs\n",
    "xerr = 0.5 * (stop_mjd - start_mjd)\n",
    "plt.errorbar(binmids, h, xerr=0, yerr=yerr, fmt=\",\")\n",
    "plt.ylim(0, None);\n",
    "\n",
    "# Plot fit\n",
    "pars = res.x\n",
    "x = np.linspace(start_mjd[0], stop_mjd[-1], 1000)\n",
    "y = f(x, pars)\n",
    "plt.plot(x, y, zorder=5)\n",
    "\n",
    "# Plot y shift dashed to see baseline or years average\n",
    "plt.axhline(res.x[3], 0, 1, color=\"C1\", ls=\"--\")\n",
    "\n",
    "plt.xlim(start_mjd[0], stop_mjd[-1])\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"./data/figs/time_rate_sinus.png\", dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Combine both to make a time-dec rate function\n",
    "\n",
    "Multiply the rate function of time with the pdf in sinDec.\n",
    "This gives the rate per solid angle.\n",
    "Integrated over the whole sphere, we recover the total rate that time.\n",
    "Integrating further over the whole time range, regarding the deadtimes of the detector, we recover the number of total events in all runs in this sample.\n",
    "We can approximate this by using the fitted y-axis offset, which is approximatly the mean and multiply with the livetitme.\n",
    "We recover the number of total events to good approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(\"Number of events from approx : \", res.x[3] * livetime * secinday)\n",
    "print(\"True number of events        : \", len(exp))\n",
    "\n",
    "# Simply integrating doesn't respect the downtimes\n",
    "wrong = scint.quad(f, start_mjd[0], stop_mjd[-1], args=res.x)[0] * secinday\n",
    "print(\"Integrating over whole year  : \", wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Function of time, sinDec and right-ascension to get the rate at that point.\n",
    "def time_sinDec_rate(sinDec, t):\n",
    "    return sinDec_pdf(sinDec) * f(t, res.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# This should yield ~1. The ratio of the fitted average d and the integral\n",
    "# of the rate function over the whole sky at a time approximately at rate=d\n",
    "_i = 2. * np.pi * scint.quad(time_sinDec_rate, -1, 1, args=55700)[0] / res.x[3]\n",
    "print(\"1D and mukltiply by 2pi : \",_i)\n",
    "\n",
    "# We can also use a 2D integrator to integrate RA as well (same result)\n",
    "def fullsky_rate(ra, sinDec, t):\n",
    "    return sinDec_pdf(sinDec) * f(t, res.x)\n",
    "_i = scint.dblquad(fullsky_rate, -1, 1, lambda x: 0, lambda x: 2.*np.pi,\n",
    "                   args=(55700,))[0] / res.x[3]\n",
    "print(\"2D over dec and ra      : \", _i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Sampling the number of BG events to inject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Testing if the integration of the rate function gives the correct number of events to inject.\n",
    "For small time windows it should be the same as just taking the rate times the window size (rectangular approximation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def rate_integral(trange, pars):\n",
    "        \"\"\"\n",
    "        Match with factor [secinday] = 24 * 60 * 60 s / MJD = 86400/(Hz*MJD)\n",
    "        in the last step.\n",
    "            [a], [d] = Hz, [b], [c], [ti] = MJD\n",
    "            [a / b] = Hz * MJD, [d * (t1 - t0)] = HZ * MJD\n",
    "        \"\"\"\n",
    "        a, b, c, d = pars\n",
    "        \n",
    "        t0 = np.atleast_2d(trange[:, 0]).reshape(len(trange), 1)\n",
    "        t1 = np.atleast_2d(trange[:, 1]).reshape(len(trange), 1)\n",
    "        \n",
    "        per = a / b * (np.cos(b * (t0 - c)) - np.cos(b * (t1 - c)))\n",
    "        lin = d * (t1 - t0)\n",
    "\n",
    "        return (per + lin) * secinday\n",
    "\n",
    "\n",
    "def get_num_of_bg_events(t, trange, ntrials, pars):\n",
    "    \"\"\"\n",
    "    Draw number of background events per trial from a poisson distribution\n",
    "    with the mean of the fitted rate function.\n",
    "    Then draw nevents times via rejection sampling for the time dpeendent rate\n",
    "    function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    t : array-like\n",
    "        Times of the occurance of each source event in MJD.\n",
    "    trange : [float, float] or array_like, shape (len(t), 2)\n",
    "        Time window(s) in seconds relativ to the given time(s) t.\n",
    "        - If [float, float], the same window [lower, upper] is used for every\n",
    "          source.\n",
    "        - If array-like, lower [i, 0] and upper [i, 1] bounds of the time\n",
    "          window per source.\n",
    "    ntrials : int\n",
    "        Number of background trials we need the number of how many events to\n",
    "        inject for.\n",
    "    pars : array-like\n",
    "        Best fit parameters from the fit function used in its integral.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    sample : list, length len(t)\n",
    "        Contains a dict with keys \"times\" and \"nevents\"\n",
    "    times : \n",
    "    nevents : array-like, shape (len(t), ntrials)\n",
    "        The number of events to inject for each trial for each source.\n",
    "    \"\"\"\n",
    "    t = np.array(t)\n",
    "    trange = np.array(trange)\n",
    "    nsrc = len(t)\n",
    "    \n",
    "    # Make shape (nsources, 1) for the times\n",
    "    t = t.reshape(nsrc, 1)\n",
    "    \n",
    "    # If range is 1D (one for all) reshape it to (nsources, 2)\n",
    "    if len(trange.shape) == 1:\n",
    "        print(\"Using the same time window for all sources.\")\n",
    "        trange = np.repeat(trange.reshape(1, 2), repeats=nsrc, axis=0)\n",
    "        \n",
    "    # Prepare time window in MJD\n",
    "    trange = t + trange / secinday\n",
    "    \n",
    "    # Expectation is the integral in the time frame\n",
    "    expect = rate_integral(trange, pars)\n",
    "        \n",
    "    # Sample from poisson\n",
    "    nevts = np.random.poisson(lam=expect, size=(nsrc, ntrials))\n",
    "    \n",
    "    return nevts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "t = start_mjd[100:104]\n",
    "trange = np.array([-120, 220])\n",
    "ntrials = 10\n",
    "\n",
    "nevts = get_num_of_bg_events(t=t, trange=trange, ntrials=ntrials, pars=res.x)\n",
    "print(\"Events to inject:\\n\", nevts)\n",
    "\n",
    "# Test if integral and simple approximation is the same for small time frames\n",
    "t = np.array(t)\n",
    "trange = np.array(trange)\n",
    "t = t.reshape(len(t), 1)\n",
    "trange = np.repeat(trange.reshape(1, 2), repeats=len(t), axis=0)\n",
    "trange = t + trange / secinday\n",
    "rate = f(t, res.x)\n",
    "nevts = rate_integral(trange, res.x)\n",
    "print(\"\\nFor small windows, rate * dt should be equal to integral\")\n",
    "print(\"Time window dt in seconds:\\n\", np.diff(trange))\n",
    "print(\"Rate * dt:\\n\", rate * np.diff(trange, axis=1) * secinday)\n",
    "print(\"Integral:\\n\", nevts)\n",
    "print(np.allclose(rate * np.diff(trange, axis=1) * secinday, nevts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Time PDF ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Background in uniformly distributed in the time window.\n",
    "Signal distribtution is falling off gaussian-like at both edges so normalization is different.\n",
    "So the ratio S/B is simply the the signal pdf divided by the uniform normalization $1 / (t_1 - t_0)$ in the time frame.\n",
    "\n",
    "To get finite support we truncate the gaussian edges at n sigma.\n",
    "Though arbitrarliy introducet to smoothly run to zero, the concrete cutoff of the doesn't really matter (so say 4, 5, 6 sigma, etc).\n",
    "This is because in the LLH we get the product of $\\langle b_B \\rangle B_i$.\n",
    "A larger cutoff make the normalization of the BG pdf larger, but in the same time makes the number of expected BG event get higher in the same linear fashion.\n",
    "So as long as we choose a cutoff which ensures that $S \\approx 0$ outside, we're good to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "secinday = 24. * 60. * 60.\n",
    "\n",
    "def time_bg_pdf(t, t0, a, b):\n",
    "    \"\"\"\n",
    "    BG is uniform for t in [t0 + a, t0 + b] and 0 outside.\n",
    "    \n",
    "    Times t and t0 are given in MJD, the range is given relative to t0\n",
    "    in seconds. t are the times we return pdf values for, t0 is the time of\n",
    "    the source event around which the time frame is defined.\n",
    "    \n",
    "    The PDF is normed to time in seconds!\n",
    "    \"\"\"\n",
    "    # Normalize relative to t0 in seconds (first multiply avoids rounding?)\n",
    "    _t = t * secinday - t0 * secinday\n",
    "  \n",
    "    pdf = np.zeros_like(_t, dtype=np.float)\n",
    "    uni = (_t >= a) & (_t <= b)\n",
    "    pdf[uni] = 1. / (b - a)\n",
    "    return pdf\n",
    "\n",
    "def time_sig_pdf(t, t0, dt, nsig=4):\n",
    "    \"\"\"\n",
    "    Signal falls of with gaussian with sigma = dt outside uniform range dt.\n",
    "    \n",
    "    Times t, t0 are in MJD, dt is in seconds.\n",
    "    t are the times we return pdf values for, t0 is the time of the source\n",
    "    event around which the time frame is defined.\n",
    "    dt is the time window starting from t0 in which signal is uniform.\n",
    "    \n",
    "    The PDF is normed to time in seconds!\n",
    "    \"\"\"\n",
    "    if dt < 0:\n",
    "        raise ValueError(\"dt must not be negative.\")\n",
    "\n",
    "    # Normalize relative to t0 in seconds (first multiply avoids rounding?)\n",
    "    _t = t * secinday - t0 * secinday\n",
    "    \n",
    "    # Constrain sig_t to [2, 30]s regardless of uniform time window\n",
    "    sig_t = np.clip(dt, 2, 30)\n",
    "    sig_t_clip = nsig * sig_t\n",
    "    gaus_norm = (np.sqrt(2 * np.pi) * sig_t)\n",
    "    \n",
    "    # Split in def regions gaus rising, uniform, gaus falling and zero\n",
    "    gr = (_t < 0) & (_t >= -sig_t_clip)\n",
    "    gf = (_t > dt) & (_t <= dt + sig_t_clip)\n",
    "    uni = (_t >= 0) & (_t <= dt)\n",
    "    \n",
    "    pdf = np.zeros_like(t, dtype=np.float)\n",
    "    pdf[gr] = scs.norm.pdf(_t[gr], loc=0, scale=sig_t)\n",
    "    pdf[gf] = scs.norm.pdf(_t[gf], loc=dt, scale=sig_t)\n",
    "    # Connect smoothly with the gaussians\n",
    "    pdf[uni] = 1. / gaus_norm\n",
    "    \n",
    "    # Normalize whole distribtuion\n",
    "    dcdf = (scs.norm.cdf(dt + sig_t_clip, loc=dt, scale=sig_t) -\n",
    "            scs.norm.cdf(-sig_t_clip, loc=0., scale=sig_t))\n",
    "    norm = dcdf + dt / gaus_norm\n",
    "    \n",
    "    return pdf / norm\n",
    "\n",
    "def time_soverb(t, t0, dt, nsig):\n",
    "    \"\"\"\n",
    "    Time signal over background PDF.\n",
    "    \n",
    "    Signal and background PDFs are each normalized over seconds.\n",
    "    Signal PDF has gaussian edges to smoothly let it fall of to zero, the\n",
    "    stddev is dt when dt is in [2, 30]s, otherwise the nearest edge.\n",
    "\n",
    "    To ensure finite support, the edges are truncated after nsig * dt.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    t : array-like\n",
    "        Times given in MJD for which we want to evaluate the ratio.\n",
    "    t0 : float\n",
    "        Time of the source event.\n",
    "    dt : float\n",
    "        Time window in seconds starting from t0 in which the signal pdf is\n",
    "        assumed to be uniform. Must not be negative.\n",
    "    nsig : float\n",
    "        Clip the gaussian edges at nsig * dt\n",
    "    \"\"\"\n",
    "    if dt < 0:\n",
    "        raise ValueError(\"dt must not be negative.\")\n",
    "\n",
    "    secinday = 24. * 60. * 60.\n",
    "\n",
    "    # Normalize relative to t0 in seconds (first multiply avoids rounding?)\n",
    "    _t = t * secinday - t0 * secinday\n",
    "   \n",
    "    # Create signal PDF\n",
    "    # Constrain sig_t to [2, 30]s regardless of uniform time window\n",
    "    sig_t = np.clip(dt, 2, 30)\n",
    "    sig_t_clip = nsig * sig_t\n",
    "    gaus_norm = (np.sqrt(2 * np.pi) * sig_t)\n",
    "    \n",
    "    # Split in def regions gaus rising, uniform, gaus falling\n",
    "    gr = (_t < 0) & (_t >= -sig_t_clip)\n",
    "    gf = (_t > dt) & (_t <= dt + sig_t_clip)\n",
    "    uni = (_t >= 0) & (_t <= dt)\n",
    "    \n",
    "    pdf = np.zeros_like(t, dtype=np.float)\n",
    "    pdf[gr] = scs.norm.pdf(_t[gr], loc=0, scale=sig_t)\n",
    "    pdf[gf] = scs.norm.pdf(_t[gf], loc=dt, scale=sig_t)\n",
    "    # Connect smoothly with the gaussians\n",
    "    pdf[uni] = 1. / gaus_norm\n",
    "    \n",
    "    # Normalize signal distribtuion\n",
    "    dcdf = (scs.norm.cdf(dt + sig_t_clip, loc=dt, scale=sig_t) -\n",
    "            scs.norm.cdf(-sig_t_clip, loc=0., scale=sig_t))\n",
    "    norm = dcdf + dt / gaus_norm\n",
    "    pdf /= norm\n",
    "    \n",
    "    # Calculate the ratio\n",
    "    bg_pdf = 1. / (dt + 2 * sig_t_clip)\n",
    "    ratio = pdf / bg_pdf\n",
    "    return ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Plot a the signal and BG PDFs for a single case\n",
    "# Arbitrary start date from data\n",
    "t0 = start_mjd[100]\n",
    "t0_sec = t0 * secinday\n",
    "\n",
    "# dt from t0 in seconds, clip at 4 sigma\n",
    "dt = 10\n",
    "nsig = 4.\n",
    "\n",
    "# Make t values for plotting in MJD around t0\n",
    "clip = np.clip(dt, 2, 30) * nsig\n",
    "plt_rng = [-clip, dt + clip]\n",
    "t = np.linspace(t0_sec + plt_rng[0], t0_sec + plt_rng[1], 200) / secinday\n",
    "\n",
    "bg_pdf = time_bg_pdf(t, t0, -clip, dt + clip)\n",
    "sig_pdf = time_sig_pdf(t, t0, dt, nsig)\n",
    "\n",
    "# Plot in normalized time\n",
    "_t = t * secinday - t0 * secinday\n",
    "plt.plot(_t, bg_pdf, \"C0-\")\n",
    "plt.plot(_t, sig_pdf, \"C1-\")\n",
    "plt.axvline(dt, 0, 1, color=\"C7\", ls=\"--\")\n",
    "plt.axvline(0, 0, 1, color=\"C1\", ls=\"--\")\n",
    "\n",
    "plt.xlabel(\"Time relative to t0 in sec\")\n",
    "plt.ylim(0, None);\n",
    "plt.show()\n",
    "\n",
    "# Integrate both pdf over time range to show they are correctly normalized\n",
    "# Note that PDFs are defined in second so we multiply by secinday \n",
    "bg_int = scint.quad(time_bg_pdf, t[0], t[-1],\n",
    "                    args=(t0, -clip, dt + clip))[0] * secinday\n",
    "sig_int = scint.quad(time_sig_pdf, t[0], t[-1],\n",
    "                    args=(t0, dt, nsig))[0] * secinday\n",
    "\n",
    "print(\"BG integral     : \", bg_int)\n",
    "print(\"Signal integral : \", sig_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Make a plot with ratios for different time windows as in the paper\n",
    "# Arbitrary start date from data\n",
    "t0 = start_mjd[100]\n",
    "t0_sec = t0 * secinday\n",
    "\n",
    "# dt from t0 in seconds, clip at 4 sigma\n",
    "dts = [5, 50, 200]\n",
    "nsig = 4\n",
    "\n",
    "# Make t values for plotting in MJD around t0, fitting all in one plot\n",
    "max_dt = np.amax(dts)\n",
    "clip = np.clip(max_dt, 2, 30) * nsig\n",
    "plt_rng = np.array([-clip, max_dt + clip])\n",
    "t = np.linspace(t0_sec + 1.2 *plt_rng[0],\n",
    "                t0_sec + 1.2 * plt_rng[1], 1000) / secinday\n",
    "_t = t * secinday - t0 * secinday\n",
    "\n",
    "# Mark event time\n",
    "plt.axvline(0, 0, 1, c=\"#353132\", ls=\"--\", lw=2)\n",
    "\n",
    "colors = [\"C0\", \"C3\", \"C2\"]\n",
    "for i, dt in enumerate(dts):\n",
    "    # Plot ratio S/B\n",
    "    SoB = time_soverb(t, t0, dt, nsig)\n",
    "    plt.plot(_t, SoB, lw=2, c=colors[i],\n",
    "             label=r\"$T_\\mathrm{{uni}}$: {:>3d}s\".format(dt))\n",
    "    # Fill uniform part, might look nicely\n",
    "    # fbtw = (_t > 0) & (_t < dt)\n",
    "    # plt.fill_between(_t[fbtw], 0, SoB[fbtw], color=\"C7\", alpha=0.1)\n",
    "\n",
    "# Make it look like the paper plot, but with slightly extended borders, to\n",
    "# nothing breaks outside the total time frame\n",
    "plt.xlim(1.2 * plt_rng)\n",
    "plt.ylim(0, 3)\n",
    "plt.xlabel(\"t - t0 in sec\")\n",
    "plt.ylabel(\"S / B\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Declination bump in data a south"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "At declination -pi/2 there are a lot of events, that show up as a large spike in sin(dec).\n",
    "Where does this come from?\n",
    "Those events come directly from above (southern sky, zenith=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "bins = 20\n",
    "ev_dec = exp[\"dec\"]\n",
    "sin_dec = np.sin(ev_dec)\n",
    "\n",
    "fig, (axl, axr) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "_ = axl.hist(sin_dec, bins=200, normed=False)\n",
    "_ = axr.hist(np.rad2deg(ev_dec), bins=500, normed=False)\n",
    "\n",
    "axr.set_xlabel(\"sinDec\")\n",
    "axr.set_xlabel(\"Dec in °\")\n",
    "\n",
    "axl.arrow(x=-1, y=1000, dx=0, dy=-100, head_length=20, head_width=0.03, lw=2,\n",
    "          fc=\"C1\", ec=\"C1\")\n",
    "axr.arrow(x=-90, y=300, dx=0, dy=-200, head_length=20, head_width=3, lw=2,\n",
    "          fc=\"C1\", ec=\"C1\")\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Integrate Kent function in 2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The paraboloid sigma was gained from estimating a circular region, in which the 2D llh includes 39% of the total probability (Attention: chi2(1, df=2), because 2D gaussian behave differently).\n",
    "This assumes a gaussian llh function, so when using the usual gaussian in the signal pdf we can directly use this sigma estimate for the gaussian sigma.\n",
    "\n",
    "For a Kent distribtuions kappa however, it is unclear how both are connected.\n",
    "For starters, when kappa is 0, points are uniform on the sky, but fully concentrated in a gaussian and vice versa.\n",
    "We could simply assume $\\kappa = 1/\\sigma$ but this is mostly founded on the 1D Mises distribution which has this property for small sigma only.\n",
    "\n",
    "So here we try the following:\n",
    "\n",
    "- For a fixed sigma, try to find the corresponding kappa, so that when integrating the Kent PDF with this kappa in a circle with radius sigma, the probability content in that circle is 68%.\n",
    "\n",
    "The Kent PDF is given by:\n",
    "\n",
    "$$\n",
    "    f(\\vec{x}_i|\\vec{x}_S) = \\frac{\\kappa}{4\\pi\\sinh{\\kappa}}\\cdot\\exp(\\kappa(\\vec{x}_i\\cdot\\vec{x}_S))\n",
    "                           = \\frac{\\kappa}{4\\pi\\sinh{\\kappa}}\\cdot\\exp(\\kappa\\cos\\Psi)\n",
    "$$\n",
    "\n",
    "where $\\cos\\Psi$ is the angular distance of both vectors.\n",
    "\n",
    "Because we need to integrate over the surface of the unit sphere, we define the Kent PDF in terms of spherical coordinates as used in equatorial coordinates.\n",
    "\n",
    "$$\n",
    "    \\vec{x} = \\begin{pmatrix}\n",
    "                \\cos\\theta\\cos{\\varphi} \\\\ \\cos\\theta\\sin{\\varphi} \\\\ \\sin\\theta\n",
    "              \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "In the integration we must not forget to multiply with the functional determinant (surface element) $\\mathrm{d}A=-r^2\\cos\\varphi\\mathrm{d}\\theta\\mathrm{d}\\varphi$ for the convention used here.\n",
    "\n",
    "Without loss of generality we can assume that the mean vector (=src position) is pointing along the z-axis $(0,0,1)$.\n",
    "This way we have easy integration boundaries, the radius of the circle is then directly given by the polar angle $\\theta$:\n",
    "\n",
    "$$\n",
    "    \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} \\cdot\n",
    "      \\begin{pmatrix} \\cos\\theta\\cos{\\varphi} \\\\ \\cos\\theta\\sin{\\varphi} \\\\ \\sin\\theta \\end{pmatrix}\n",
    "    = \\sin\\theta\n",
    "$$\n",
    "\n",
    "so our full integral assembles to:\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathrm{Prob\\ Content\\ within\\ }\\sigma = \\alpha &= \\int_{\\theta=\\pi/2}^{\\pi/2 - \\sigma}\\int_{\\varphi=0}^{2\\pi}\n",
    "        \\frac{\\kappa}{4\\pi\\sinh{\\kappa}}\\cdot\\exp(\\kappa\\sin\\theta)\\cdot(-\\cos\\theta)\n",
    "        \\mathrm{d}\\theta\\mathrm{d}\\varphi \\\\\n",
    "      &= 2\\pi\\cdot\\frac{\\kappa}{4\\pi\\sinh{\\kappa}}\\cdot \\int_{\\theta=\\pi/2}^{\\pi/2 - \\sigma}\n",
    "        -\\cos\\theta\\exp(\\kappa\\sin\\theta)\\mathrm{d}\\theta\\mathrm{d}\\varphi \\\\\n",
    "      &= \\frac{\\kappa}{2\\sinh{\\kappa}}\\cdot \\int_{\\theta=\\pi/2 - \\sigma}^{\\pi/2}\n",
    "        \\cos\\theta\\exp(\\kappa\\sin\\theta)\\mathrm{d}\\theta\\mathrm{d}\\varphi\n",
    "\\end{align}\n",
    "\n",
    "We can solve this analytially:\n",
    "\n",
    "\\begin{align}\n",
    "    \\alpha &= \\frac{1}{2\\sinh{\\kappa}}\\cdot \\int_{\\theta=\\pi/2 - \\sigma}^{\\pi/2}\n",
    "               \\kappa\\cos\\theta\\exp(\\kappa\\sin\\theta)\\mathrm{d}\\theta\n",
    "            = \\frac{1}{2\\sinh{\\kappa}}\\cdot \\left[\\exp(\\kappa\\sin\\theta)\\right]_{\\theta=\\pi/2 - \\sigma}^{\\pi/2} \\\\\n",
    "            &= \\frac{1}{2\\sinh{\\kappa}}\\cdot \\left[\\exp(\\kappa) - \\exp(\\kappa\\cos(\\sigma))\\right]\n",
    "\\end{align}\n",
    "\n",
    "This automatically reduces to $\\sinh\\kappa / \\sinh\\kappa = 1$ for $\\sigma=180°$ and to zero for $\\sigma=0°$ as expected.\n",
    "\n",
    "The special case $\\kappa=0$ resembles a uniform distribtuion on the sky and is given by:\n",
    "\n",
    "\\begin{align}\n",
    "    \\alpha &= \\int_{\\theta=\\pi/2 - \\sigma}^{\\pi/2}\\int_{\\varphi=0}^{2\\pi}\n",
    "               \\frac{\\cos\\theta}{4\\pi} \\mathrm{d}\\theta\\mathrm{d}\\varphi\n",
    "           =\\frac{1}{2}\\int_{\\theta=\\pi/2 - \\sigma}^{\\pi/2}\\cos\\theta\\mathrm{d}\\theta \\\\\n",
    "           &= \\frac{\\sin(\\pi/2) - \\sin(\\pi/2-\\sigma)}{2}\n",
    "           = \\frac{1 - \\cos\\sigma}{2}\n",
    "\\end{align}\n",
    "\n",
    "Below we run some tests up to which events sigma we can go with the Kent distribution.\n",
    "Also we see, if it is justified to identify $\\kappa = 1/\\sigma**2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Quickly see that the pdf itself is normalized.\n",
    "# Note: Integral is in cos(theta), so no area element needed.\n",
    "# For very large kappas this breaks because the stepsize is too coarse\n",
    "def S(cosDist, kappa):\n",
    "    return (kappa / (2. * np.pi * (1. - np.exp(-2. * kappa))) *\n",
    "                 np.exp(kappa * (cosDist - 1. )))\n",
    "\n",
    "sig = np.deg2rad(2)\n",
    "kappa = 1 / sig**2\n",
    "scint.quad(S, a=3*sig, b=1, args=(kappa))[0] * 2 * np.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# First let's get the 1 sigma gaussian probability content to compare against\n",
    "one_sig = scs.norm.cdf(1) - scs.norm.cdf(-1)\n",
    "\n",
    "def alpha_in_kent_stable(kappa, sigma):\n",
    "    \"\"\"\n",
    "    moxe is a numeric god :+1:\n",
    "    \"\"\"\n",
    "    sigma = np.atleast_1d(sigma)\n",
    "    if np.any(sigma > np.pi):\n",
    "        raise ValueError(\"Angular error greater than pi.\")\n",
    "    if kappa == 0:\n",
    "        return 0.5 * (1. - np.cos(sigma))\n",
    "\n",
    "    a = 1. / np.tanh(kappa) + 1.\n",
    "    b = 0.5 * (1. - np.exp(kappa * (np.cos(sigma) - 1.)))\n",
    "    return a * b\n",
    "\n",
    "def alpha_in_kent(kappa, sigma):\n",
    "    sigma = np.atleast_1d(sigma)\n",
    "    if np.any(sigma > np.pi):\n",
    "        raise ValueError(\"Angular error greater than pi.\")\n",
    "    if kappa == 0:\n",
    "        return 0.5 * (1. - np.cos(sigma))\n",
    "    \n",
    "    return 0.5 * (np.exp(kappa) - np.exp(kappa * np.cos(sigma))) / np.sinh(kappa)\n",
    "\n",
    "def kappa_from_sigma(sigma, alpha=0.39):\n",
    "    # Wrapper because we need the point alpha(k) - alpha = 0\n",
    "    def fun(kappa):\n",
    "        return alpha_in_kent_stable(kappa, sigma) - alpha\n",
    "    \n",
    "    x0 = float(1. / sigma**2)\n",
    "    return sco.newton(fun, x0=x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Up to ~40° kappa = 1/sigma**2 is OK to use. Then the norm kicks in\n",
    "sigmas = np.linspace(0.1, 150, 1000)\n",
    "alpha = scs.chi2.cdf(1**2, df=2)\n",
    "\n",
    "kappas = np.zeros_like(sigmas)\n",
    "for i, sig in enumerate(sigmas):\n",
    "    kappas[i] = kappa_from_sigma(np.deg2rad(sig), alpha=alpha)\n",
    "    \n",
    "# Simple subtitution as in mrichmans thesis\n",
    "subst = 1. / np.deg2rad(sigmas)**2\n",
    "\n",
    "plt.plot(sigmas, kappas, label=r\"Exact\")\n",
    "plt.plot(sigmas, subst, label=r\"$\\kappa = 1/\\sigma^2$\")\n",
    "\n",
    "plt.xlabel(\"sigma in °\")\n",
    "plt.ylabel(r\"$\\kappa$\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "\n",
    "# plt.savefig(\"data/figs/kent_kappa.png\", dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Different view of the plot above, now in terms of different kappas\n",
    "sigmas = np.linspace(0, 20, 360 + 1)\n",
    "sigmas_rad = np.deg2rad(sigmas)\n",
    "kappas = np.arange(10, 200, 20)\n",
    "\n",
    "alpha = scs.chi2.cdf(1**2, df=2)  # ca. 39%, 2D gaussian\n",
    "plt.axhline(1, 0, 1, color=dg, ls=\"--\")\n",
    "plt.axhline(alpha, 0, 1, color=dg, ls=\"--\")\n",
    "for k in kappas:\n",
    "    a = alpha_in_kent(k, sigmas_rad)\n",
    "    plt.plot(sigmas, a, label=r\"$\\kappa = {:.1f}$\".format(k))\n",
    "\n",
    "# Note: if evt sigma is larger than ~80°, we can't describe the correct\n",
    "#       probability content with a Kent distribution. An artifact from the\n",
    "#       LLH beeing not gaussian enough.\n",
    "plt.xlabel(\"Evt sigma in °\")\n",
    "plt.ylabel(\"Prob. in sigma\")\n",
    "plt.xlim(0, 20)\n",
    "plt.ylim(0, 1.05)\n",
    "plt.legend()\n",
    "plt.title(\"CDF of symmetric Kent distribution\")\n",
    "\n",
    "# plt.savefig(\"data/figs/kent_cdf.png\", dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Now directly compare Kent to 2D gaussian (provided by mrichman :+1:)\n",
    "# For sigmas near the turnover ~75°, we can see, that the Kent line crosses\n",
    "# the gaussian. It smoothes out more to get exactly the 39% at the sigma level\n",
    "mth = 90\n",
    "theta = np.linspace (0, np.deg2rad(mth), 1000)\n",
    "sig_deg = [3, 5, 10, 20, 30, 75]\n",
    "\n",
    "cmap = plt.cm.get_cmap(\"plasma\")\n",
    "\n",
    "for sd in sig_deg:\n",
    "    sr = np.radians(sd)\n",
    "    # kappa = 1 / sr**2\n",
    "    kappa = kappa_from_sigma(sigma=sr, alpha=scs.chi2.cdf(1**2, df=2))\n",
    "    G = np.exp(-theta**2 / (2 * sr**2)) / (2. * np.pi * sr**2)\n",
    "    K = kappa / (4. * np.pi * np.sinh(kappa)) * np.exp(kappa * np.cos(theta))\n",
    "    color = cmap((sd - min(sig_deg)) / max(sig_deg))\n",
    "    plt.plot(np.rad2deg(theta), K, color=color,\n",
    "             label=r'$\\sigma={}^\\circ$'.format (sd))\n",
    "    plt.plot(np.rad2deg(theta), G, ls='--', color=color)\n",
    "\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel (r'$\\Delta\\Psi~[^\\circ]$')\n",
    "plt.ylabel (r'PDF')\n",
    "plt.xlim (0, mth)\n",
    "plt.ylim (1e-3, 1e3)\n",
    "plt.legend (loc='best', ncol=2, prop={\"size\": 'small'}, title=\"Kent\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig(\"data/figs/kent_vs_gaus.png\", dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Tests for the KDE integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Marginalize KDE by integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Instead of sampling and reducing to 2D histograms, we can try to truly integrate one dimension of the KDE to be able to plot also the tails of the distribution, where events usually end up only in large samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# KDE CV is running on cluster and pickles the GridSearchCV\n",
    "fname = \"./data/kde_cv/KDE_model_selector_20_exp_IC86_I_followup_2nd_pass.pickle\"\n",
    "with open(fname, \"rb\") as f:\n",
    "    model_selector = pickle.load(f)\n",
    "\n",
    "kde = model_selector.best_estimator_\n",
    "bw = model_selector.best_params_[\"bandwidth\"]\n",
    "print(\"Best bandwidth : {:.3f}\".format(bw))\n",
    "\n",
    "# We maybe just want to stick with the slightly overfitting kernel to\n",
    "# be as close as possible to data\n",
    "OVERFIT = True\n",
    "if OVERFIT:\n",
    "    bw = 0.075\n",
    "    kde = skn.KernelDensity(bandwidth=bw, kernel=\"gaussian\", rtol=1e-8)\n",
    "print(\"Used bandwidth : {:.3f}\".format(bw))\n",
    "\n",
    "# KDE sample must be cut in sigma before fitting, similar to range in hist\n",
    "_exp = exp[exp[\"sigma\"] <= np.deg2rad(5)]\n",
    "\n",
    "fac_logE = 1.5\n",
    "fac_dec = 2.5\n",
    "fac_sigma = 2.\n",
    "\n",
    "_logE = fac_logE * _exp[\"logE\"]\n",
    "_sigma = fac_sigma * np.rad2deg(_exp[\"sigma\"])\n",
    "_dec = fac_dec * _exp[\"dec\"]\n",
    "\n",
    "kde_sample = np.vstack((_logE, _dec, _sigma)).T\n",
    "\n",
    "# Fit KDE best model to sample\n",
    "kde.fit(kde_sample)\n",
    "\n",
    "# Make some samples\n",
    "nsamples_kde = int(1e7)\n",
    "bg_samples = kde.sample(n_samples=nsamples_kde)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 1D case\n",
    "\n",
    "Integrate out 2 axis with a double integral to show a 1D margin distribution.\n",
    "This take super long, 5 Minutes per point.\n",
    "But it can be parallelized pretty simple if needed.\n",
    "Code to create the values is on phobos.\n",
    "\n",
    "**Resumee:** Sampling many values and simply bin in 1D is much better, needs less time and is probably more accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Compare to data hist\n",
    "_ = plt.hist(exp[\"logE\"] * fac_logE, bins=100, normed=True, label=\"data\")\n",
    "\n",
    "# Compare 'true' integration with\n",
    "h, b = np.histogram(bg_samples[:, 0], bins=200, range=[2, 10], normed=True)\n",
    "m = 0.5 * (b[:-1] + b[1:])\n",
    "_ = plt.plot(m, h, label=\"sample\")\n",
    "\n",
    "bins_and_vals = np.load(\"data/2d_integrate_kde/bins_and_vals.npy\")\n",
    "x = bins_and_vals[0]\n",
    "vals = bins_and_vals[1]\n",
    "_ = plt.plot(x, vals, label=\"integrated\")\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig(\"./kde.png\", dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 2D PDF\n",
    "\n",
    "Integrate out only one axis (here sigma) to show the 2D marginalized PDF.\n",
    "Executed on phobos, takes a little while on 50x50 grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Get precalculated integral data\n",
    "bins = np.load(\"data/1d_integrate_kde/logE_sinDec_bins_50x50.npy\")\n",
    "vals = np.load(\"data/1d_integrate_kde/logE_sinDec_int_50x50.npy\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Compare to data hist\n",
    "_ = ax[0].hist2d(exp[\"logE\"], np.sin(exp[\"dec\"]), bins=bins, normed=True)\n",
    "\n",
    "# Compare 'true' integration\n",
    "mids = get_binmids(bins)\n",
    "xx, yy  = map(np.ravel, np.meshgrid(mids[0], mids[1]))\n",
    "_ = ax[1].hist2d(xx, yy, bins=bins, weights=vals)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Justify the sigma cut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Only few higher energy events from the sothern sky are excluded (see cut=10).\n",
    "But really bad reconstructed events tend to have higher energies (see cut=90).\n",
    "Still it should be OK to remove those > 10 because they have not so much spatial information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Show the leftover event s after a sigma cut\n",
    "sig_cut = 10\n",
    "m = exp[\"sigma\"] > np.deg2rad(sig_cut)\n",
    "\n",
    "_ = plt.hist2d(exp[\"logE\"][m], np.rad2deg(exp[\"dec\"][m]),\n",
    "               bins=30, cmap=\"inferno\")\n",
    "plt.colorbar()\n",
    "plt.title(\"Total Evts w sigma > {:d}°: {:d} ({:.3f}%)\".format(\n",
    "        sig_cut, np.sum(m), np.sum(m) / len(exp) * 100))\n",
    "plt.xlabel(\"logE\")\n",
    "plt.ylabel(\"dec in °\")\n",
    "plt.show()\n",
    "\n",
    "# Show the skewed sigma distribution with the cut applied and mean vs median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Test the marginalize_hist method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "It should be equivalent to use one of the following methods to create a 1D histogram from the original 3D data pdf in logE, dec and sigma:\n",
    "\n",
    "1. Simply use the original 1D data in any variable, e.g. simply histogram logE\n",
    "2. Create the complete 3D histogram and marginalize by summing over remaining dimensions.\n",
    "\n",
    "When using unnormalized hists, 2. is simply summing up all other counts.\n",
    "\n",
    "When using normalized hists, we need to sum with respect to the binwidths in the current dimension to keep the normalization intact.\n",
    "This is only useful, when only the histogram is available and not the original sample.\n",
    "\n",
    "We want to compare if both methods are equivalent\n",
    "As we can see, all ratios are one, so methods are equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def make_hist_ratio(h1, h2):\n",
    "    \"\"\"Return the ratio h1 / h2. Return 0 where h2 is 0.\"\"\"\n",
    "    m = (h2 > 0)\n",
    "    ratio = np.zeros_like(h1)\n",
    "    ratio[m] = h1[m] / h2[m]\n",
    "    return ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Unnormalized\n",
    "First the unnormalized version. Simply sum over the other axes of the 3D hist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Plot each variable in a single plot and the ratios seperately\n",
    "fig, [[axtl, axtr], [axbl, axbr]] = plt.subplots(2, 2, figsize=(10, 8))\n",
    "\n",
    "# We also make a cut < 10° in sigma, because there are some outliers\n",
    "m = exp[\"sigma\"] <= np.deg2rad(10)\n",
    "sigma = np.rad2deg(exp[\"sigma\"][m])\n",
    "logE = exp[\"logE\"][m]\n",
    "dec = np.sin(exp[\"dec\"][m])\n",
    "\n",
    "logE_nbins = 50\n",
    "dec_nbins = 40\n",
    "sigma_nbins = 30\n",
    "\n",
    "# Make the 3D hist\n",
    "sample = np.vstack((logE, dec, sigma)).T\n",
    "nbins = [logE_nbins, dec_nbins, sigma_nbins]\n",
    "h, b = np.histogramdd(sample, bins=nbins,)\n",
    "\n",
    "# Get binmids for plotting\n",
    "m = get_binmids(b)\n",
    "\n",
    "# Common hist settings\n",
    "h1 = {\"lw\": 2, \"color\": \"k\", \"histtype\": \"step\"}\n",
    "h2 = {\"lw\": 2, \"color\": \"r\", \"histtype\": \"step\", \"alpha\": 0.5}\n",
    "\n",
    "# logE\n",
    "logE_h, logE_b, _ = axtl.hist(logE, bins=logE_nbins, **h1)\n",
    "logE_hm = np.sum(h, axis=(1, 2))\n",
    "_ = axtl.hist(m[0], bins=b[0], weights=logE_hm, **h2)\n",
    "# Ratio plot below\n",
    "axtl_sec = split_axis(axtl, \"bottom\", \"20%\", cbar=False)\n",
    "axtl_sec.hist(m[0], b[0], weights=make_hist_ratio(logE_h, logE_hm), **h2)\n",
    "axtl_sec.axhline(1, 0, 1, color=\"k\")\n",
    "axtl_sec.set_ylim(0, 2)\n",
    "\n",
    "# dec\n",
    "dec_h, dec_b, _ = axbl.hist(dec, bins=dec_nbins, **h1)\n",
    "dec_hm = np.sum(h, axis=(0, 2))\n",
    "_ = axbl.hist(m[1], bins=b[1], weights=dec_hm, **h2)\n",
    "\n",
    "axbl_sec = split_axis(axbl, \"bottom\", \"20%\", cbar=None)\n",
    "axbl_sec.hist(m[1], b[1], weights=make_hist_ratio(dec_h, dec_hm), **h2)\n",
    "axbl_sec.axhline(1, 0, 1, color=\"k\")\n",
    "axbl_sec.set_ylim(0, 2)\n",
    "\n",
    "# sigma\n",
    "sigma_h, sigma_b, _ = axtr.hist(sigma, bins=sigma_nbins, **h1)\n",
    "sigma_hm = np.sum(h, axis=(0, 1))\n",
    "_ = axtr.hist(m[2], bins=b[2], weights=sigma_hm, **h2)\n",
    "\n",
    "axtr_sec = split_axis(axtr, \"bottom\", \"20%\", cbar=None)\n",
    "axtr_sec.hist(m[2], b[2], weights=make_hist_ratio(sigma_h, sigma_hm), **h2)\n",
    "axtr_sec.axhline(1, 0, 1, color=\"k\")\n",
    "axtr_sec.set_ylim(0, 2)\n",
    "\n",
    "axbr.set_visible(False)\n",
    "\n",
    "fig.suptitle(\"Black: 1D, Red: Margin\", fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Normalized\n",
    "Sum over the other axes of the 3D hist and multiply by bin widths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Plot each variable in a single plot and the ratios seperately\n",
    "fig, [[axtl, axtr], [axbl, axbr]] = plt.subplots(2, 2, figsize=(10, 8))\n",
    "\n",
    "# Now make it normed\n",
    "h, b = np.histogramdd(sample, bins=nbins, normed=True)\n",
    "\n",
    "# Get binmids for plotting\n",
    "m = get_binmids(b)\n",
    "\n",
    "# logE\n",
    "logE_h, logE_b, _ = axtl.hist(logE, bins=logE_nbins, normed=True, **h1)\n",
    "logE_hm = hist_marginalize(h=h, bins=b, axes=(1, 2))[0]\n",
    "_ = axtl.hist(m[0], bins=b[0], weights=logE_hm, **h2)\n",
    "# Ratio plot below\n",
    "axtl_sec = split_axis(axtl, \"bottom\", \"20%\", cbar=False)\n",
    "axtl_sec.hist(m[0], b[0], weights=make_hist_ratio(logE_h, logE_hm), **h2)\n",
    "axtl_sec.axhline(1, 0, 1, color=\"k\")\n",
    "axtl_sec.set_ylim(0, 2)\n",
    "\n",
    "# dec\n",
    "dec_h, dec_b, _ = axbl.hist(dec, bins=dec_nbins, normed=True, **h1)\n",
    "dec_hm = hist_marginalize(h=h, bins=b, axes=(0, 2))[0]\n",
    "_ = axbl.hist(m[1], bins=b[1], weights=dec_hm, **h2)\n",
    "\n",
    "axbl_sec = split_axis(axbl, \"bottom\", \"20%\", cbar=None)\n",
    "axbl_sec.hist(m[1], b[1], weights=make_hist_ratio(dec_h, dec_hm), **h2)\n",
    "axbl_sec.axhline(1, 0, 1, color=\"k\")\n",
    "axbl_sec.set_ylim(0, 2)\n",
    "\n",
    "# sigma\n",
    "sigma_h, sigma_b, _ = axtr.hist(sigma, bins=sigma_nbins, normed=True, **h1)\n",
    "sigma_hm = hist_marginalize(h=h, bins=b, axes=(0, 1))[0]\n",
    "_ = axtr.hist(m[2], bins=b[2], weights=sigma_hm, **h2)\n",
    "\n",
    "axtr_sec = split_axis(axtr, \"bottom\", \"20%\", cbar=None)\n",
    "axtr_sec.hist(m[2], b[2], weights=make_hist_ratio(sigma_h, sigma_hm), **h2)\n",
    "axtr_sec.axhline(1, 0, 1, color=\"k\")\n",
    "axtr_sec.set_ylim(0, 2)\n",
    "\n",
    "axbr.set_visible(False)\n",
    "\n",
    "fig.suptitle(\"Black: 1D, Red: Margin\", fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Mask sinDec logE ratio PDF spline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Assign each high E events for which no data but MC is present the maximum ratio from signal over background and all lowE the lowest ratio.\n",
    "\n",
    "In skylab all events which no data but MC events get the highest value, which is strange at the lowE regime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Prepare the MC data, signal weighted to astro unbroken power law\n",
    "gamma = 2.\n",
    "# No flux norm, because we normalize anyway\n",
    "mc_w = mc[\"ow\"] * mc[\"trueE\"]**(-gamma)\n",
    "\n",
    "# Make the MC hist. Use this binning for the data too\n",
    "mc_sindec = np.sin(mc[\"dec\"])\n",
    "mc_logE = mc[\"logE\"]\n",
    "bins = [50, 50]\n",
    "range = [[-1, 1], [1, 10]]\n",
    "mc_h, bx, by = np.histogram2d(mc_sindec, mc_logE, bins=bins, range=range,\n",
    "                              weights=mc_w, normed=True)\n",
    "\n",
    "# Make the data hist\n",
    "b = [bx, by]\n",
    "bg_logE = exp[\"logE\"]\n",
    "bg_sindec = np.sin(exp[\"dec\"])\n",
    "bg_h, _, _ = np.histogram2d(bg_sindec, bg_logE, bins=b,\n",
    "                                range=range, normed=True)\n",
    "\n",
    "\n",
    "# 4 cases:\n",
    "#   - Data & MC: Calculate the ratio\n",
    "#   - (>logEthresh) & (no data or no MC): Assign highest normal ratio\n",
    "#   - (<logEthresh) & (no data or no MC): Assign lowest normal ratio\n",
    "#   - No data and no MC): Assign any value (eg 1), these are never accessed\n",
    "# Get logE value per bin in entrie histogram\n",
    "m = get_binmids(b)\n",
    "logEs =  np.repeat(m[1], repeats=bins[1]).reshape(bins).T\n",
    "logEthresh = 3.5\n",
    "\n",
    "m1 = (bg_h > 0) & (mc_h > 0)\n",
    "m2 = (logEs > logEthresh) & ((bg_h <= 0) | (mc_h <= 0))\n",
    "m3 = (logEs <= logEthresh) & ((bg_h <= 0) | (mc_h <= 0))\n",
    "m4 = (bg_h <= 0) & (mc_h <= 0)\n",
    "\n",
    "SoB = np.ones_like(bg_h)\n",
    "SoB[m1] = mc_h[m1] / bg_h[m1]\n",
    "SoB[m2] = np.amax(SoB)\n",
    "SoB[m3] = np.amin(SoB)\n",
    "SoB[m4] = 1.\n",
    "\n",
    "# Make a 4 color mask map\n",
    "mask_map = np.ones_like(bg_h)\n",
    "mask_map[m1] = 1.\n",
    "mask_map[m2] = 2.\n",
    "mask_map[m3] = 0.\n",
    "mask_map[m4] = -1.\n",
    "\n",
    "# Plot it\n",
    "fig, ((axtl, axtr), (axbl, axbr)) = plt.subplots(2, 2, figsize=(12, 10))\n",
    "xx, yy = map(np.ravel, np.meshgrid(*m))\n",
    "\n",
    "_ = axtl.hist2d(xx, yy, bins=b, weights=bg_h.T.flatten(), cmap=\"viridis\",\n",
    "               norm=LogNorm())\n",
    "_ = axtr.hist2d(xx, yy, bins=b, weights=mc_h.T.flatten(), cmap=\"viridis\",\n",
    "               norm=LogNorm())\n",
    "\n",
    "# Plot mask\n",
    "cbins = np.linspace(-1, 2, 5)\n",
    "cticks = 0.5 * (cbins[:-1] + cbins[1:])\n",
    "cmap = matplotlib.cm.get_cmap(\"inferno\", len(cticks))\n",
    "_, _, _, img = axbr.hist2d(xx, yy, bins=b, weights=mask_map.T.flatten(),\n",
    "                          cmap=cmap)\n",
    "cax = split_axis(axbr, cbar=True)\n",
    "cbar = plt.colorbar(cax=cax, mappable=img, ticks=cticks)\n",
    "cbar.ax.set_yticklabels([\"No BG AND no MC\", \"lowE, no D OR MC\", \"BG AND MC\",\n",
    "                         \"higE, no D OR MC\"],\n",
    "                        rotation=60, va=\"center\")\n",
    "\n",
    "# Plot ratio\n",
    "cn = max(np.amin(SoB), np.amax(SoB))\n",
    "_, _, _, img = axbl.hist2d(xx, yy, bins=b, weights=SoB.T.flatten(),\n",
    "                          cmap=\"coolwarm\", vmin=1./cn, vmax=cn, norm=LogNorm())\n",
    "cax = split_axis(axbl, cbar=True)\n",
    "cbar = plt.colorbar(cax=cax, mappable=img)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig(\"./data/figs/energy_ratio_mask.png\", dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Compare sigma to x*exp(-x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "# sig_deg = np.rad2deg(exp[\"sigma\"][exp[\"sigma\"] < np.deg2rad(10)])\n",
    "# _, b, _ = plt.hist(sig_deg, bins=50, normed=True)\n",
    "\n",
    "# PDF\n",
    "x = np.linspace(0, 10, 500)\n",
    "a = 3\n",
    "y = a**2 * x * np.exp(-a * x)\n",
    "_ = plt.plot(x, y, lw=2)\n",
    "\n",
    "# Sampled from PDF\n",
    "# Combo from Pythia 8 and Trial&Error. How to derive from LambertW function?\n",
    "#   http://home.thep.lu.se/~torbjorn/doxygen/Basics_8h_source.html\n",
    "u1, u2 = np.random.uniform(size=(2, 10000))\n",
    "sam = -np.log(u1 * u2) / a\n",
    "plt.hist(sam, bins=b, normed=True, histtype=\"step\", lw=2)\n",
    "\n",
    "plt.xlim(0, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Old Scaled KDE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To save the old non-adaptive and kind-of guessed KDE for comparison, we put the former code in here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 3D histogram of BG data\n",
    "First we make a 3D histogram to better compare to mrichmann and to get an overview over the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# HANDTUNED scale parameter to \"fit\" KDE expectation to data...\n",
    "# TODO: Use Adaptive kernel width and asymmetric gaus kernels\n",
    "#       For sigma it might make sense to a take a restricted kernel [0, inf]\n",
    "fac_logE = 1.5\n",
    "fac_dec = 2.5\n",
    "fac_sigma = 2.\n",
    "\n",
    "logE = fac_logE * exp[\"logE\"]\n",
    "sigma = fac_sigma * np.rad2deg(exp[\"sigma\"])\n",
    "# np.cos(np.pi / 2. + exp[\"dec\"]); dec is for {sin(dec), dec, cos(zen)}\n",
    "dec = fac_dec * exp[\"dec\"]\n",
    "\n",
    "# Binning is rather arbitrary because we don't calc stuff with the hist\n",
    "bins = [50, 50, 50]\n",
    "# Range for sigma is picked by looking at the 1D distribution and cutting of\n",
    "# the tail. This will be covered by the KDE tail anyway. Rest is default\n",
    "r = [[np.amin(logE), np.amax(logE)],\n",
    "     [np.amin(dec), np.amax(dec)],\n",
    "     [0., fac_sigma * 5.]]\n",
    "\n",
    "sample = np.vstack((logE, dec, sigma)).T\n",
    "h, bins = np.histogramdd(sample=sample, bins=bins, range=r, normed=False)\n",
    "\n",
    "# Make bin mids for later use\n",
    "mids = []\n",
    "for b in bins:\n",
    "    mids.append(0.5 * (b[:-1] + b[1:]))\n",
    "\n",
    "# Make a nice corner plot\n",
    "fig, ax = corner_hist(h, bins=bins,\n",
    "                      label=[\"scaled logE\", \"scaled dec\", \"scaled sigma deg\"],\n",
    "                      hist2D_args={\"cmap\": \"Greys\"},\n",
    "                      hist_args={\"color\":\"#353132\"})\n",
    "\n",
    "# plt.savefig(\"./data/figs/bg_corner_scaled.png\", dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Kernel Density Estimation\n",
    "\n",
    "We use scikit learn's cross validation with a gaussian kernel to get the most robust bandwidth.\n",
    "Then we integrate with the same binning as above and compare to the 3D histogram.\n",
    "\n",
    "This section relies heavily on [Jake van der Plas examples for KDE](https://jakevdp.github.io/blog/2013/12/01/kernel-density-estimation/).\n",
    "More info on how KDE cross validation works can be found in [Modern Nonparametric Methods](http://www2.stat.duke.edu/~wjang/teaching/S05-293/lecture/ch6.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# KDE CV is running on cluster and pickles the GridSearchCV\n",
    "fname = \"./data/kde_cv/KDE_model_selector_20_exp_IC86_I_followup_2nd_pass.pickle\"\n",
    "with open(fname, \"rb\") as f:\n",
    "    model_selector = pickle.load(f)\n",
    "\n",
    "kde = model_selector.best_estimator_\n",
    "bw = model_selector.best_params_[\"bandwidth\"]\n",
    "print(\"Best bandwidth : {:.3f}\".format(bw))\n",
    "\n",
    "# We maybe just want to stick with the slightly overfitting kernel to\n",
    "# be as close as possible to data\n",
    "OVERFIT = True\n",
    "if OVERFIT:\n",
    "    bw = 0.075\n",
    "    kde = skn.KernelDensity(bandwidth=bw, kernel=\"gaussian\", rtol=1e-8)\n",
    "print(\"Used bandwidth : {:.3f}\".format(bw))\n",
    "\n",
    "# KDE sample must be cut in sigma before fitting, similar to range in hist\n",
    "_exp = exp[exp[\"sigma\"] <= np.deg2rad(5)]\n",
    "\n",
    "_logE = fac_logE * _exp[\"logE\"]\n",
    "_sigma = fac_sigma * np.rad2deg(_exp[\"sigma\"])\n",
    "_dec = fac_dec * _exp[\"dec\"]\n",
    "\n",
    "kde_sample = np.vstack((_logE, _dec, _sigma)).T\n",
    "\n",
    "# Fit KDE best model to sample\n",
    "kde.fit(kde_sample)\n",
    "\n",
    "# Generate some BG samples to compare to the original data hist.\n",
    "# Use more statistics, histograms get normalized and we want the best estimate\n",
    "# for the pdf\n",
    "nsamples_kde = int(1e7)\n",
    "bg_samples = kde.sample(n_samples=nsamples_kde)\n",
    "\n",
    "# Make histogram with same binning as original data\n",
    "bg_h, bg_bins = np.histogramdd(sample=bg_samples, bins=bins, range=r, normed=True)\n",
    "\n",
    "fig, ax = corner_hist(bg_h, bins=bg_bins,\n",
    "                      label=[\"scaled logE\", \"scaled sin(dec)\",\n",
    "                             \"scaled sigma deg\"],\n",
    "                      hist2D_args={\"cmap\": \"Greys\"},\n",
    "                      hist_args={\"color\":\"#353132\"})\n",
    "\n",
    "# plt.savefig(\"./data/figs/kde_corner_scaled.png\", dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Compare KDE to original data\n",
    "\n",
    "Make a ratio histogram of the KDE sample and the original data sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 2D marginalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create 2D hists, by leaving out one parameter\n",
    "xlabel = [\"scaled \" + s for s in [\"logE\", \"logE\", \"dec\"]]\n",
    "ylabel = [\"scaled \" + s for s in [\"dec\", \"sigma in °\", \"sigma in °\"]]\n",
    "\n",
    "for i, axes in enumerate([[0, 1], [0, 2], [1, 2]]):\n",
    "    _b = np.array(bins)\n",
    "    h_exp, b_exp = np.histogramdd(sample[:, axes],\n",
    "                                  bins=_b[axes], normed=True)\n",
    "    h_kde, b_kde = np.histogramdd(bg_samples[:, axes],\n",
    "                                  bins=_b[axes], normed=True)\n",
    "    \n",
    "    # KDE is expectation, but sampled with much more events.\n",
    "    # Weights would simply scale the total number of KDE events to match the\n",
    "    # number of original events. That would be the mean for the poisson\n",
    "    # distribution in each bin. So to get OK KDE expectation sqrt(n) errors\n",
    "    # in each bin, we divide not by the number of drawn KDE but by the number\n",
    "    # of original events.   \n",
    "    # Again shapes of meshgrid and hist are transposed\n",
    "    diffXX, _ = np.meshgrid(np.diff(_b[0]), np.diff(_b[1]))\n",
    "    norm_kde = len(exp) * diffXX.T\n",
    "    sigma_kde = np.sqrt(h_kde / norm_kde)\n",
    "\n",
    "    # Make 3 different diff/ratio hists to estimate KDE quality in\n",
    "    # 1D marginalization.\n",
    "    m = (h_exp > 0.)\n",
    "    ratio_h = np.zeros_like(h_exp)\n",
    "    ratio_h[m] = h_kde[m] / h_exp[m]\n",
    "\n",
    "    diff_h = h_kde - h_exp\n",
    "\n",
    "    m = (sigma_kde > 0.)\n",
    "    sigma_ratio_h = np.zeros_like(h_exp)\n",
    "    sigma_ratio_h[m] = (h_exp[m] - h_kde[m]) / sigma_kde[m]\n",
    "\n",
    "    # Bin mids and hist grid\n",
    "    _b = b_exp\n",
    "    m = get_binmids(_b)\n",
    "    xx, yy = map(np.ravel, np.meshgrid(m[0], m[1]))\n",
    "    \n",
    "    \n",
    "    # Big plot on the left and three right\n",
    "    fig = plt.figure(figsize=(10, 6))\n",
    "    gs = gridspec.GridSpec(3, 3)\n",
    "    axl = fig.add_subplot(gs[:, :2])\n",
    "    axrt = fig.add_subplot(gs[0, 2])\n",
    "    axrc = fig.add_subplot(gs[1, 2])\n",
    "    axrb = fig.add_subplot(gs[2, 2])\n",
    "    \n",
    "    # Steal space for colorbars\n",
    "    caxl = split_axis(axl, \"right\")\n",
    "    caxrt = split_axis(axrt, \"left\")\n",
    "    caxrc = split_axis(axrc, \"left\")\n",
    "    caxrb = split_axis(axrb, \"left\")\n",
    "\n",
    "    # Unset top and center xticklabels as they are shared with the bottom plot\n",
    "    axrt.set_xticklabels([])\n",
    "    axrc.set_xticklabels([])\n",
    "        \n",
    "    # Left: Difference over KDE sigma\n",
    "    # cbar_extr = max(np.amax(sigma_ratio_h),  # Center colormap to min/max\n",
    "    #                         abs(np.amin(sigma_ratio_h)))\n",
    "    _, _, _, imgl = axl.hist2d(xx, yy, bins=_b, weights=sigma_ratio_h.T.ravel(),\n",
    "                               cmap=\"seismic\", vmax=5, vmin=-5)\n",
    "    cbarl = plt.colorbar(cax=caxl, mappable=imgl)\n",
    "    axl.set_xlabel(xlabel[i])\n",
    "    axl.set_ylabel(ylabel[i])\n",
    "    axl.set_title(\"(exp - kde) / sigma_kde\")\n",
    "    \n",
    "    # Right top: Ratio\n",
    "    _, _, _, imgrt = axrt.hist2d(xx, yy, bins=_b, weights=ratio_h.T.ravel(),\n",
    "                                 cmap=\"seismic\", vmax=2, vmin=0);\n",
    "    cbarrt = plt.colorbar(cax=caxrt, mappable=imgrt)\n",
    "    axrt.set_title(\"kde / exp\")\n",
    "\n",
    "    # Right center: Data hist\n",
    "    _, _, _, imgrc = axrc.hist2d(xx, yy, bins=_b, weights=h_exp.T.ravel(),\n",
    "                                 cmap=\"Greys\", norm=LogNorm());\n",
    "    cbarrc = plt.colorbar(cax=caxrc, mappable=imgrc)\n",
    "    axrc.set_title(\"exp logscale\")\n",
    "\n",
    "    # Right bottom: KDE hist, same colorbar scale as on data\n",
    "    _, _, _, imgrb = axrb.hist2d(xx, yy, bins=_b, weights=h_kde.T.ravel(),\n",
    "                                 cmap=\"Greys\", norm=LogNorm());\n",
    "    # Set with same colormap as on data\n",
    "    imgrb.set_clim(cbarrc.get_clim())\n",
    "    cbarrb = plt.colorbar(cax=caxrb, mappable=imgrb)\n",
    "    axrb.set_title(\"kde logscale\")\n",
    "    \n",
    "    # Set tick and label positions\n",
    "    for ax in [caxrt, caxrc, caxrb]:\n",
    "        ax.yaxis.set_label_position(\"right\")\n",
    "        ax.yaxis.tick_left()\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    plt.savefig(\"./data/figs/kde_data_2d_{}_{}.png\".format(\n",
    "                    xlabel[i], ylabel[i]),\n",
    "                dpi=200)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 1D marginalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Pseudo smooth marginalization is done by sampling many point from KDE an\n",
    "# using a finely binned 1D histogram, so it looks smooth\n",
    "xlabel = [\"scaled \" + s for s in [\"logE\", \"dec\", \"sigma °\"]]\n",
    "\n",
    "for i, axes in enumerate([0, 1, 2]):\n",
    "    _b = np.array(bins)\n",
    "    h_exp, b_exp = np.histogram(sample[:, axes],\n",
    "                                bins=_b[axes], normed=True)\n",
    "    h_kde, b_kde = np.histogram(bg_samples[:, axes],\n",
    "                                bins=_b[axes], normed=True)\n",
    "    \n",
    "#     h_exp, b_exp = hist_marginalize(h, bins, axes=axes)\n",
    "#     h_kde, b_kde = hist_marginalize(bg_h, bg_bins, axes=axes)\n",
    "      \n",
    "    # KDE errorbars as in 2D case\n",
    "    norm_kde = len(exp) * np.diff(b_kde)\n",
    "    sigma_kde = np.sqrt(h_kde / norm_kde)\n",
    "\n",
    "    # Make 3 different diff/ratio hists to estimate KDE quality in\n",
    "    # 1D marginalization.\n",
    "    m = (h_exp > 0.)\n",
    "    ratio_h = np.zeros_like(h_exp)\n",
    "    ratio_h[m] = h_kde[m] / h_exp[m]\n",
    "\n",
    "    diff_h = h_kde - h_exp\n",
    "\n",
    "    m = (sigma_kde > 0.)\n",
    "    sigma_ratio_h = np.zeros_like(h_exp)\n",
    "    sigma_ratio_h[m] = (h_exp[m] - h_kde[m]) / sigma_kde[m]\n",
    "\n",
    "    # Bin mids\n",
    "    _b = b_exp\n",
    "    m = get_binmids([_b])[0]\n",
    "    \n",
    "    # Plot both and the ration normed. Big plot on the left and three right\n",
    "    fig = plt.figure(figsize=(10, 6))\n",
    "    gs = gridspec.GridSpec(3, 3)\n",
    "    axl = fig.add_subplot(gs[:, :2])\n",
    "    axrt = fig.add_subplot(gs[0, 2])\n",
    "    axrc = fig.add_subplot(gs[1, 2])\n",
    "    axrb = fig.add_subplot(gs[2, 2])\n",
    "\n",
    "    axrt.set_xticklabels([])\n",
    "    axrc.set_xticklabels([])\n",
    "\n",
    "    # Set ticks and labels right\n",
    "    for ax in [axrt, axrc, axrb]:\n",
    "        ax.yaxis.set_label_position(\"right\")\n",
    "        ax.yaxis.tick_right()\n",
    "\n",
    "    # Limits\n",
    "    for ax in [axl, axrt, axrc, axrb]:\n",
    "        ax.set_xlim(_b[0], _b[-1])\n",
    "        \n",
    "    # Main plot:\n",
    "    # Plot more dense to mimic a smooth curve\n",
    "    __h, __b = np.histogram(bg_samples[:, i], bins=500,\n",
    "                            range=[_b[0], _b[-1]], density=True)\n",
    "    __m = get_binmids([__b])[0]\n",
    "    axl.plot(__m, __h, lw=3, alpha=0.5)\n",
    "    \n",
    "    _ = axl.hist(m, bins=_b, weights=h_exp, label=\"exp\", histtype=\"step\",\n",
    "                 lw=2, color=\"k\")\n",
    "    _ = axl.errorbar(m, h_kde, yerr=sigma_kde, fmt=\",\", color=\"r\")\n",
    "    _ = axl.hist(m, bins=_b, weights=h_kde, label=\"kde\", histtype=\"step\",\n",
    "                 lw=2, color=\"r\")    \n",
    "    \n",
    "    axl.set_xlabel(xlabel[i])\n",
    "    axl.legend(loc=\"upper right\")\n",
    "\n",
    "    # Top right: Difference\n",
    "    _ = axrt.axhline(0, 0, 1, color=\"k\", ls=\"-\")\n",
    "    _ = axrt.hlines([-.02, -.01, .01, .02], _b[0], _b[-1],\n",
    "                    colors='#353132', linestyles='dashed')\n",
    "    _ = axrt.hist(m, bins=_b, weights=diff_h, histtype=\"step\", lw=2, color=\"r\")\n",
    "    axrt.set_ylim(-.05, +.05)\n",
    "    axrt.set_ylabel(\"kde - exp\")\n",
    "\n",
    "    # Center right: Ratio\n",
    "    _ = axrc.axhline(1, 0, 1, color=\"k\", ls=\"-\")\n",
    "    _ = axrc.hlines([0.8, 0.9, 1.1, 1.2], _b[0], _b[-1],\n",
    "                    colors='#353132', linestyles='dashed')\n",
    "    _ = axrc.hist(m, bins=_b, weights=ratio_h, histtype=\"step\", lw=2, color=\"r\")\n",
    "    axrc.set_ylim(.5, 1.5)\n",
    "    axrc.set_ylabel(\"kde / exp\")\n",
    "\n",
    "    # Bottom right: Ratio of diff to sigma of expectation\n",
    "    _ = axrb.axhline(0, 0, 1, color=\"k\", ls=\"-\")\n",
    "    _ = axrb.hlines([-2, -1, 1, 2], _b[0], _b[-1],\n",
    "                    colors='#353132', linestyles='dashed')\n",
    "    _ = axrb.hist(m, bins=_b, weights=sigma_ratio_h, histtype=\"step\", lw=2, color=\"r\")\n",
    "    axrb.set_ylim(-3, +3)\n",
    "    axrb.set_ylabel(\"(exp-kde)/sigma_kde\")\n",
    "    \n",
    "    plt.savefig(\"./data/figs/kde_data_1d_{}.png\".format(\n",
    "            xlabel[i]), dpi=200)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Show scores of awKDE CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open(\"data/awKDE_CV/CV10_glob_bw_alpha_EXP_IC86I_CUT_sig.ll.20_PARS_\" +\n",
    "          \"diag_True_pass2.pickle\", \"rb\") as f:\n",
    "    model_selector = pickle.load(f)\n",
    "    \n",
    "glob_bws = model_selector.cv_results_[\"param_glob_bw\"].compressed()\n",
    "mean_score = model_selector.cv_results_[\"mean_test_score\"]\n",
    "std_score = model_selector.cv_results_[\"std_test_score\"]\n",
    "\n",
    "# Mark best\n",
    "_ = plt.axvline(glob_bws[bf_idx], 0, 1, ls=\"--\", lw=2, color=\"C2\", zorder=-1)\n",
    "# Each horizontal line is one alpha tested for that glob_bw\n",
    "bf_idx = np.argmax(mean_score)\n",
    "_ = plt.errorbar(glob_bws, mean_score, yerr=std_score,fmt=\"_\")\n",
    "\n",
    "plt.xlabel(\"global bandwidth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Old BG Rate Sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Tried to sample everything at once (trials, src, times).\n",
    "Very clumsy because for each src and trial, a different number of evt is sampled, so we can't use arrays.\n",
    "\n",
    "Also we can fit a single LLH at a time only, so no need to get all samples in advance.\n",
    "Might be faster, but consumes also much more memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Sample number of events in frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def filter_runs(run):\n",
    "    \"\"\"\n",
    "    Filter runs as stated in jfeintzig's doc.\n",
    "    \"\"\"\n",
    "    exclude_runs = [120028, 120029, 120030, 120087, 120156, 120157]\n",
    "    if ((run[\"good_i3\"] == True) & (run[\"good_it\"] == True) &\n",
    "        (run[\"run\"] not in exclude_runs)):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "goodrun_dict, _livetime = hlp.create_goodrun_dict(\n",
    "    runlist=\"data/runlists/ic86-i-goodrunlist.json\", filter_runs=filter_runs)\n",
    "\n",
    "h = hlp._create_runtime_bins(exp[\"timeMJD\"], goodrun_dict=goodrun_dict,\n",
    "                             remove_zero_runs=True)\n",
    "\n",
    "def f(x, pars):\n",
    "    \"\"\"\n",
    "    Returns the rate at a given time in MJD.\n",
    "    \"\"\"\n",
    "    a, b, c, d = pars\n",
    "    return a * np.sin(b * (x - c)) + d\n",
    "\n",
    "def lstsq(pars, *args):\n",
    "    \"\"\"\n",
    "    Weighted leastsquares min sum((wi * (yi - fi))**2)\n",
    "    \"\"\"\n",
    "    # data x,y-values and weights are fixed\n",
    "    x, y, w = args\n",
    "    _f = f(x, pars)\n",
    "    return np.sum((w * (y - _f))**2)\n",
    "\n",
    "# Seed values from consideration above.\n",
    "# a0 = -0.0005\n",
    "# b0 = 2. * np.pi / 365.  # We could restrict the period to one yr exact.\n",
    "# c0 = np.amin(start_mjd)\n",
    "# d0 = np.average(h, weights=yerr**2)\n",
    "\n",
    "rate = h[\"rate\"]\n",
    "rate_std = h[\"rate_std\"]\n",
    "X = exp[\"timeMJD\"]\n",
    "binmids = 0.5 * (h[\"start_mjd\"] + h[\"stop_mjd\"])\n",
    "\n",
    "a0 = 0.5 * (np.amax(rate) - np.amin(rate))\n",
    "b0 = 2. * np.pi / 365.\n",
    "c0 = np.amin(X)\n",
    "d0 = np.average(rate, weights=rate_std**2)\n",
    "\n",
    "x0 = [a0, b0, c0, d0]\n",
    "# Bounds as explained above\n",
    "bounds = [[None, None], [0.5 * b0, 1.5 * b0], [c0 - b0, c0 + b0, ], [0, 0.01]]\n",
    "# x, y values, weights\n",
    "args = (binmids, rate, 1. / rate_std)\n",
    "\n",
    "res = sco.minimize(fun=lstsq, x0=x0, args=args, bounds=bounds)\n",
    "bf_pars = res.x\n",
    "\n",
    "print(\"Amplitude   : {: 13.5f} in Hz\".format(res.x[0]))\n",
    "print(\"Period (d)  : {: 13.5f} in days\".format(2 * np.pi / res.x[1]))\n",
    "print(\"Offset (MJD): {: 13.5f} in MJD\".format(res.x[2]))\n",
    "print(\"Avg. rate   : {: 13.5f} in Hz\".format(res.x[3]))\n",
    "\n",
    "# Define the rate function:\n",
    "def rate_fun(t):\n",
    "    \"\"\"\n",
    "    Returns the rate at a given time in MJD.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    t : array-like\n",
    "        Time in MJD.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    rate : array-like\n",
    "        The rate of background events in Hz.\n",
    "    \"\"\"\n",
    "    return f(t, res.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "secinday = 24 * 60 * 60\n",
    "def _prep_times(t, trange):\n",
    "    \"\"\"\n",
    "    Little wrapper to not DRY.\n",
    "    \"\"\"\n",
    "    t = np.atleast_1d(t)\n",
    "    trange = np.array(trange)\n",
    "    nsrc = len(t)\n",
    "    \n",
    "    # Make shape (nsources, 1) for the times\n",
    "    t = t.reshape(nsrc, 1)\n",
    "    \n",
    "    # If range is 1D (one for all) reshape it to (nsources, 2)\n",
    "    if len(trange.shape) == 1:\n",
    "        trange = np.repeat(trange.reshape(1, 2), repeats=nsrc, axis=0)\n",
    "        \n",
    "    # Prepare time window in MJD\n",
    "    trange = t + trange / secinday\n",
    "    \n",
    "    return t, trange\n",
    "\n",
    "def get_num_of_bg_events(t, trange, ntrials, pars):\n",
    "    \"\"\"\n",
    "    Draw number of background events per trial from a poisson distribution\n",
    "    with the mean of the fitted rate function.\n",
    "    Then draw nevents times via rejection sampling for the time dpeendent rate\n",
    "    function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    t : array-like\n",
    "        Times of the occurance of each source event in MJD.\n",
    "    trange : [float, float] or array_like, shape (len(t), 2)\n",
    "        Time window(s) in seconds relativ to the given time(s) t.\n",
    "        - If [float, float], the same window [lower, upper] is used for every\n",
    "          source.\n",
    "        - If array-like, lower [i, 0] and upper [i, 1] bounds of the time\n",
    "          window per source.\n",
    "    ntrials : int\n",
    "        Number of background trials we need the number of how many events to\n",
    "        inject for.\n",
    "    pars : array-like\n",
    "        Best fit parameters from the fit function used in its integral.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    nevents : array-like, shape (len(t), ntrials)\n",
    "        The number of events to inject for each trial for each source.\n",
    "    \"\"\"\n",
    "    # Integrate rate function analytially in desired interval\n",
    "    def rate_integral(trange, pars):\n",
    "        \"\"\"\n",
    "        Match with factor [secinday] = 24 * 60 * 60 s / MJD = 86400/(Hz*MJD)\n",
    "        in the last step.\n",
    "            [a], [d] = Hz, [b], [c], [ti] = MJD\n",
    "            [a / b] = Hz * MJD, [d * (t1 - t0)] = HZ * MJD\n",
    "        \"\"\"\n",
    "        a, b, c, d = pars\n",
    "        \n",
    "        t0 = np.atleast_2d(trange[:, 0]).reshape(len(trange), 1)\n",
    "        t1 = np.atleast_2d(trange[:, 1]).reshape(len(trange), 1)\n",
    "        \n",
    "        per = a / b * (np.cos(b * (t0 - c)) - np.cos(b * (t1 - c)))\n",
    "        lin = d * (t1 - t0)\n",
    "\n",
    "        return (per + lin) * secinday\n",
    "    \n",
    "    t, trange = _prep_times(t, trange)\n",
    "    \n",
    "    # Expectation is the integral in the time frame\n",
    "    expect = rate_integral(trange, pars)\n",
    "        \n",
    "    # Sample from poisson\n",
    "    nevts = np.random.poisson(lam=expect, size=(len(t), ntrials))\n",
    "      \n",
    "    return nevts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "start, stop = h[\"start_mjd\"], h[\"stop_mjd\"]\n",
    "\n",
    "nsrc = 5\n",
    "t = start[100:100 + nsrc]\n",
    "# Make different time windows to verify that more events are sampled in more time\n",
    "trange = np.array([[-10 * 5*i, 20 * 5*i] for i in range(1, nsrc + 1)])\n",
    "print(\"Time windows:\\n\", trange)\n",
    "\n",
    "ntrials = 10\n",
    "nevts = get_num_of_bg_events(t, trange, ntrials, res.x)\n",
    "# print(\"All evts, per src and trial:\\n\", nevts)\n",
    "\n",
    "# Total evts in all trials per src\n",
    "print(\"Total sampled evts per src:\\n\", np.sum(nevts, axis=1)[:, np.newaxis])\n",
    "\n",
    "# Compare to expectation\n",
    "print(\"Expected per trial per src:\\n\", np.diff(trange, axis=1) * res.x[3])\n",
    "print(\"Sampled per trial per src:\\n\", np.sum(nevts, axis=1)[:, np.newaxis] / ntrials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Now the sampling of random times in the time frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "First we want to see, that all BG injected events stay in the correct time frame and make a uniform distribution for small time frames.\n",
    "\n",
    "Then we make the time window really big and the events should follow the rate function PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_times_in_frame(t, trange, nsamples):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    t : float\n",
    "        Time of the occurance of the source event in MJD.\n",
    "    trange : [float, float]\n",
    "        Time window in seconds relativ to the given time t.\n",
    "    nsamples : array-like, type int, shape (len(t))\n",
    "        Number of events to inject per trial. Number of trials is given by\n",
    "        the length of nsamples.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    times : list, length len(t)\n",
    "        List of samples times in MJD of background events per source.\n",
    "        For each source i nsamples[i] times are drawn from the rate function.\n",
    "    \"\"\"\n",
    "    _pdf = rate_fun\n",
    "    \n",
    "    t, trange = _prep_times(t, trange)\n",
    "    \n",
    "    sample = []\n",
    "    nsamples = np.atleast_1d(nsamples)\n",
    "    \n",
    "    for i, ni in enumerate(nsamples):\n",
    "        sam, _ = rejection_sampling(_pdf, bounds=trange, n=ni)\n",
    "        sample.append(sam)\n",
    "        \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# First the small time frame\n",
    "# Arbitrary start date from data\n",
    "t0 = start[100]\n",
    "t0_sec = t0 * secinday\n",
    "\n",
    "# dt from t0 in seconds, clip at 4 sigma\n",
    "dt = 200\n",
    "nsig = 4.\n",
    "\n",
    "# Make t values for plotting in MJD around t0\n",
    "clip = np.clip(dt, 2, 30) * nsig\n",
    "plt_rng = [-clip, dt + clip]\n",
    "trange = plt_rng\n",
    "ntrials = 10000\n",
    "\n",
    "# Sample times\n",
    "nevts = get_num_of_bg_events(t=t0, trange=trange, ntrials=ntrials,\n",
    "                             pars=res.x)[0]\n",
    "times = get_times_in_frame(t0, trange, nevts)\n",
    "\n",
    "# Plot them in together with the PDFs\n",
    "def time_bg_pdf(t, t0, a, b):\n",
    "    # Normalize relative to t0 in seconds (first multiply avoids rounding?)\n",
    "    _t = t * secinday - t0 * secinday\n",
    "  \n",
    "    pdf = np.zeros_like(_t, dtype=np.float)\n",
    "    uni = (_t >= a) & (_t <= b)\n",
    "    pdf[uni] = 1. / (b - a)\n",
    "    return pdf\n",
    "\n",
    "def time_sig_pdf(t, t0, dt, nsig=4):\n",
    "    if dt < 0:\n",
    "        raise ValueError(\"dt must not be negative.\")\n",
    "\n",
    "    # Normalize relative to t0 in seconds (first multiply avoids rounding?)\n",
    "    _t = t * secinday - t0 * secinday\n",
    "    \n",
    "    # Constrain sig_t to [2, 30]s regardless of uniform time window\n",
    "    sig_t = np.clip(dt, 2, 30)\n",
    "    sig_t_clip = nsig * sig_t\n",
    "    gaus_norm = (np.sqrt(2 * np.pi) * sig_t)\n",
    "    \n",
    "    # Split in def regions gaus rising, uniform, gaus falling and zero\n",
    "    gr = (_t < 0) & (_t >= -sig_t_clip)\n",
    "    gf = (_t > dt) & (_t <= dt + sig_t_clip)\n",
    "    uni = (_t >= 0) & (_t <= dt)\n",
    "    \n",
    "    pdf = np.zeros_like(t, dtype=np.float)\n",
    "    pdf[gr] = scs.norm.pdf(_t[gr], loc=0, scale=sig_t)\n",
    "    pdf[gf] = scs.norm.pdf(_t[gf], loc=dt, scale=sig_t)\n",
    "    # Connect smoothly with the gaussians\n",
    "    pdf[uni] = 1. / gaus_norm\n",
    "    \n",
    "    # Normalize whole distribtuion\n",
    "    dcdf = (scs.norm.cdf(dt + sig_t_clip, loc=dt, scale=sig_t) -\n",
    "            scs.norm.cdf(-sig_t_clip, loc=0., scale=sig_t))\n",
    "    norm = dcdf + dt / gaus_norm\n",
    "    \n",
    "    return pdf / norm\n",
    "\n",
    "\n",
    "# Plot the pdfs\n",
    "t = np.linspace(t0_sec + plt_rng[0], t0_sec + plt_rng[1], 200) / secinday\n",
    "bg_pdf = time_bg_pdf(t, t0, -clip, dt + clip)\n",
    "sig_pdf = time_sig_pdf(t, t0, dt, nsig)\n",
    "\n",
    "# Plot in normalized time\n",
    "_t = t * secinday - t0 * secinday\n",
    "plt.plot(_t, bg_pdf, \"C0-\")\n",
    "plt.plot(_t, sig_pdf, \"C1-\")\n",
    "plt.axvline(dt, 0, 1, color=\"C7\", ls=\"--\")\n",
    "plt.axvline(0, 0, 1, color=\"C1\", ls=\"--\")\n",
    "\n",
    "# Plot injected events from all trials\n",
    "T = np.array([])\n",
    "for ti in times:\n",
    "    T = np.append(T, ti)  \n",
    "T = (T - t0) * secinday\n",
    "\n",
    "_ = plt.hist(T, bins=50, normed=True, color=dg, alpha=.25)\n",
    "\n",
    "plt.xlabel(\"Time relative to t0 in sec\")\n",
    "plt.ylim(0, None);\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig(\"./data/figs/bg_events_time_sampled_narrow.png\", dpi=200)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Now the really large time frame, over the whole time range\n",
    "t0 = start[0]\n",
    "t0_sec = t0 * secinday\n",
    "\n",
    "# dt from t0 in seconds, clip at 4 sigma\n",
    "dt = (stop[-1] - start[0]) * secinday\n",
    "nsig = 4.\n",
    "\n",
    "# Make t values for plotting in MJD around t0\n",
    "clip = np.clip(dt, 2, 30) * nsig\n",
    "plt_rng = [-clip, dt + clip]\n",
    "trange = plt_rng\n",
    "ntrials = 1\n",
    "\n",
    "# Sample times\n",
    "nevts = get_num_of_bg_events(t=t0, trange=trange, ntrials=ntrials,\n",
    "                             pars=res.x)[0]\n",
    "times = get_times_in_frame(t0, trange, nevts)\n",
    "\n",
    "# Plot injected events from all trials\n",
    "T = np.array([])\n",
    "for ti in times:\n",
    "    T = np.append(T, ti)  \n",
    "\n",
    "h, b = np.histogram(T, bins=1081)\n",
    "m = get_binmids([b])[0]\n",
    "scale = np.diff(b) * secinday * ntrials\n",
    "yerr = np.sqrt(h) / scale\n",
    "h = h / scale\n",
    "\n",
    "plt.errorbar(m, h, yerr=yerr, fmt=\",\")\n",
    "\n",
    "# Plot normalized rate function to compare\n",
    "t = np.linspace(start[0], stop[-1], 100)\n",
    "r = rate_fun(t=t)\n",
    "plt.plot(t, r, lw=2, zorder=5)\n",
    "plt.axhline(res.x[3], 0, 1, color=\"C1\", ls=\"--\", label=\"\", zorder=5)\n",
    "\n",
    "plt.xlim(start[0], stop[-1])\n",
    "plt.ylim(0, 0.009)\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig(\"./data/figs/bg_events_time_sampled_wide.png\", dpi=200)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Production Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test if the modules work correctly.\n",
    "They contain the same code as in the main test notebook, but can be used as classes.\n",
    "This should simplyfy production.\n",
    "\n",
    "Currently each submodul only does a very special task:\n",
    "\n",
    "- `bg_injector`: Samples (\"injects\") backgorund events for trials\n",
    "- `bg_rate_injector`: Samples (\"injects\") the number of BG events to be injected per trial.\n",
    "- `rate_function`: Describes the time depence of the background rate.\n",
    "- `llh`: Implements the likelihood function and signal and background PDFs.\n",
    "- `signal_injector`: Same as `bg_injector` but injecting signal evts from MC.\n",
    "- `analysis`: Main module pulling it all together, making trials, fitting llhs, provides methods for advanced tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "_exp = exp[exp[\"sigma\"] < np.deg2rad(20)]\n",
    "_logE = _exp[\"logE\"]\n",
    "_dec = _exp[\"dec\"]\n",
    "_sigma = _exp[\"sigma\"]\n",
    "\n",
    "sample = np.vstack((_logE, _dec, _sigma)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## BG Injector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Injects information for background-like events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "n_samples = 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Data Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_inj = BGInj.DataBGInjector()\n",
    "data_inj.fit(sample)\n",
    "data_sam = data_inj.sample(n_samples)\n",
    "\n",
    "xlabel = [\"logE\", \"sinDec\", \"logE\"]\n",
    "ylabel = [\"sinDec\", \"sigma\", \"sigma\"]\n",
    "for i, axes in enumerate([[0, 1], [1, 2], [0,2]]):\n",
    "    fig, (al, ar) = hlp.hist_comp(sample[:, axes], data_sam[:, axes])\n",
    "    al.set_xlabel(xlabel[i])\n",
    "    ar.set_xlabel(xlabel[i])\n",
    "    al.set_ylabel(ylabel[i])\n",
    "    ar.set_ylabel(ylabel[i])\n",
    "    al.set_title(\"Data\")\n",
    "    ar.set_title(\"Data sample: {} evts from original Data\".format(len(data_sam)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Adaptive Width KDE sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Assign model from CV, which has already evaluated adaptive kernels\n",
    "with open(\"data/awKDE_CV/CV10_glob_bw_alpha_EXP_IC86I_CUT_sig.ll.20_\" +\n",
    "          \"PARS_diag_True_pass2.pickle\", \"rb\") as f:\n",
    "    model_selector = pickle.load(f)\n",
    "    print(model_selector.best_params_)\n",
    "\n",
    "kde_inj = BGInj.KDEBGInjector()\n",
    "kde_inj.kde_model = model_selector.best_estimator_\n",
    "\n",
    "# We could still change the alpha, but the global bandwidth must stay fixed\n",
    "# kde_inj.kde_model.alpha = 0.3\n",
    "\n",
    "# Fit doesn't take long because all adaptive kernels are set\n",
    "bounds = np.array([[None, None], [-np.pi / 2. , np.pi / 2.], [0, None]])\n",
    "kde_inj.fit(sample, bounds)\n",
    "\n",
    "# Sample with bounds, because KDEs spillover\n",
    "bounds = np.array([[None, None], [-np.pi / 2. , np.pi / 2.], [0, None]])\n",
    "kde_sam = kde_inj.sample(n_samples)\n",
    "\n",
    "xlabel = [\"logE\", \"sinDec\", \"logE\"]\n",
    "ylabel = [\"sinDec\", \"sigma\", \"sigma\"]\n",
    "for i, axes in enumerate([[0, 1], [1, 2], [0,2]]):\n",
    "    fig, (al, ar) = hlp.hist_comp(sample[:, axes], kde_sam[:, axes])\n",
    "    al.set_xlabel(xlabel[i])\n",
    "    ar.set_xlabel(xlabel[i])\n",
    "    al.set_ylabel(ylabel[i])\n",
    "    ar.set_ylabel(ylabel[i])\n",
    "    al.set_title(\"Data\")\n",
    "    ar.set_title(\"KDE sample: {} evts\".format(len(kde_sam)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### GRBLLH style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# If False, only sample where data was\n",
    "# If True sample in global min/max bounding box\n",
    "minmax = True\n",
    "\n",
    "mrinj = BGInj.MRichmanBGInjector()\n",
    "ax0_bins, ax1_bins, ax2_bins = mrinj.fit(sample, nbins=10, minmax=minmax)\n",
    "mr_sam = mrinj.sample(n_samples=n_samples)\n",
    "\n",
    "xlabel = [\"logE\", \"sinDec\", \"logE\"]\n",
    "ylabel = [\"sinDec\", \"sigma\", \"sigma\"]\n",
    "for i, axes in enumerate([[0, 1], [1, 2], [0,2]]):\n",
    "    fig, (al, ar) = hlp.hist_comp(sample[:, axes], mr_sam[:, axes])\n",
    "    al.set_xlabel(xlabel[i])\n",
    "    ar.set_xlabel(xlabel[i])\n",
    "    al.set_ylabel(ylabel[i])\n",
    "    ar.set_ylabel(ylabel[i])\n",
    "    al.set_title(\"Data\")\n",
    "    ar.set_title(\"Pseudo MR sample: {} evts\".format(len(mr_sam)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Pseudo Data (uniform) sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "uni_inj = BGInj.UniformBGInjector()\n",
    "uni_sam = uni_inj.sample(n_samples)\n",
    "\n",
    "for axes in [[0, 1], [1, 2], [0,2]]:\n",
    "    fig, (al, ar) = hlp.hist_comp(sample[:, axes], uni_sam[:, axes])\n",
    "    al.set_title(\"Data\")\n",
    "    ar.set_title(\"Pseudo (uniform) sample: {} evts\".format(len(uni_sam)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## BG Rate Injector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This module injects times of background like events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Injector created from runlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Fit sinus to rates from detector runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# First create a rate function\n",
    "rate_func = RateFunc.Sinus1yrRateFunction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Now parse the rundict and make the fitted injector from that\n",
    "def filter_runs(run):\n",
    "    \"\"\"\n",
    "    Filter runs as stated in jfeintzig's doc.\n",
    "    \"\"\"\n",
    "    exclude_runs = [120028, 120029, 120030, 120087, 120156, 120157]\n",
    "    if ((run[\"good_i3\"] == True) & (run[\"good_it\"] == True) &\n",
    "        (run[\"run\"] not in exclude_runs)):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "# Let's create an injector using a goodrun list. This creates a run dict\n",
    "runlist=\"data/runlists/ic86-i-goodrunlist.json\"\n",
    "runlist_inj = BGRateInj.RunlistBGRateInjector(runlist, filter_runs, rate_func)\n",
    "\n",
    "# Fit function to exp times to runlist bins\n",
    "times = exp[\"timeMJD\"]\n",
    "rate_func = runlist_inj.fit(T=times, x0=None, remove_zero_runs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Plot runs\n",
    "rates = runlist_inj.rate_rec\n",
    "start_mjd = rates[\"start_mjd\"]\n",
    "stop_mjd = rates[\"stop_mjd\"]\n",
    "\n",
    "xerr = 0.5 * (stop_mjd - start_mjd)\n",
    "yerr = rates[\"rate_std\"]\n",
    "binmids = 0.5 * (stop_mjd + start_mjd)\n",
    "\n",
    "plt.errorbar(binmids, rates[\"rate\"], xerr=xerr, yerr=yerr, fmt=\",\")\n",
    "plt.ylim(0, None);\n",
    "\n",
    "# Plot fit\n",
    "t = np.linspace(start_mjd[0], stop_mjd[-1], 1000)\n",
    "y = rate_func(t)\n",
    "plt.plot(t, y, zorder=5)\n",
    "\n",
    "# Plot y shift dashed to see baseline or years average\n",
    "avg = runlist_inj.best_pars[2]\n",
    "plt.axhline(avg, 0, 1, color=\"C1\", ls=\"--\", label=\"\")\n",
    "\n",
    "plt.xlim(start_mjd[0], stop_mjd[-1])\n",
    "plt.xlabel(\"MJD\")\n",
    "plt.ylabel(\"Rate in Hz\")\n",
    "\n",
    "# plt.savefig(\"./data/figs/time_rate_sinus.png\", dpi=200)\n",
    "plt.ylim(0, 0.009)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Best fit params:\\n\", runlist_inj.best_pars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Sample many trials at once for a single src and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rates = runlist_inj.rate_rec\n",
    "start_mjd = rates[\"start_mjd\"]\n",
    "\n",
    "# Pick some random time and time frame\n",
    "t = np.random.choice(start_mjd, size=1)\n",
    "trange = np.array([-120, 220])\n",
    "\n",
    "ntrials = int(1e6)\n",
    "trials = runlist_inj.sample(t, trange, ntrials=ntrials)\n",
    "\n",
    "nevents = np.array(list(map(len, trials)))\n",
    "print(\"Sampled total of {:d} events in {:d} trials.\".format(\n",
    "        np.sum(nevents), ntrials))\n",
    "\n",
    "# Plot poisson distribution of nevents with expectation from integral\n",
    "expect = runlist_inj.best_estimator_integral(t, trange)\n",
    "_ = plt.hist(nevents, bins=np.arange(10), normed=True)\n",
    "plt.axvline(expect, 0, 1, color=\"C1\", ls=\"--\", lw=2)\n",
    "x = np.arange(0, 10)\n",
    "y = scs.poisson.pmf(x, mu=expect)\n",
    "_ = plt.plot(x, y, \"C1\", lw=2, drawstyle=\"steps-post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# First the small time frame\n",
    "# Arbitrary start date from data\n",
    "t0 = np.random.choice(start_mjd, size=1)\n",
    "t0_sec = t0 * secinday\n",
    "\n",
    "# dt from t0 in seconds, clip at 4 sigma\n",
    "dt = 200\n",
    "nsig = 4.\n",
    "\n",
    "# Make t values for plotting in MJD around t0\n",
    "clip = np.clip(dt, 2, 30) * nsig\n",
    "plt_rng = np.array([-clip, dt + clip])\n",
    "trange = plt_rng\n",
    "ntrials = 10000\n",
    "\n",
    "# Sample times\n",
    "trials = runlist_inj.sample(t0, trange, ntrials=ntrials)\n",
    "\n",
    "# Plot them in together with the PDFs\n",
    "def time_bg_pdf(t, t0, a, b):\n",
    "    # Normalize relative to t0 in seconds (first multiply avoids rounding?)\n",
    "    _t = t * secinday - t0 * secinday\n",
    "  \n",
    "    pdf = np.zeros_like(_t, dtype=np.float)\n",
    "    uni = (_t >= a) & (_t <= b)\n",
    "    pdf[uni] = 1. / (b - a)\n",
    "    return pdf\n",
    "\n",
    "def time_sig_pdf(t, t0, dt, nsig=4):\n",
    "    if dt < 0:\n",
    "        raise ValueError(\"dt must not be negative.\")\n",
    "\n",
    "    # Normalize relative to t0 in seconds (first multiply avoids rounding?)\n",
    "    _t = t * secinday - t0 * secinday\n",
    "    \n",
    "    # Constrain sig_t to [2, 30]s regardless of uniform time window\n",
    "    sig_t = np.clip(dt, 2, 30)\n",
    "    sig_t_clip = nsig * sig_t\n",
    "    gaus_norm = (np.sqrt(2 * np.pi) * sig_t)\n",
    "    \n",
    "    # Split in def regions gaus rising, uniform, gaus falling and zero\n",
    "    gr = (_t < 0) & (_t >= -sig_t_clip)\n",
    "    gf = (_t > dt) & (_t <= dt + sig_t_clip)\n",
    "    uni = (_t >= 0) & (_t <= dt)\n",
    "    \n",
    "    pdf = np.zeros_like(t, dtype=np.float)\n",
    "    pdf[gr] = scs.norm.pdf(_t[gr], loc=0, scale=sig_t)\n",
    "    pdf[gf] = scs.norm.pdf(_t[gf], loc=dt, scale=sig_t)\n",
    "    # Connect smoothly with the gaussians\n",
    "    pdf[uni] = 1. / gaus_norm\n",
    "    \n",
    "    # Normalize whole distribtuion\n",
    "    dcdf = (scs.norm.cdf(dt + sig_t_clip, loc=dt, scale=sig_t) -\n",
    "            scs.norm.cdf(-sig_t_clip, loc=0., scale=sig_t))\n",
    "    norm = dcdf + dt / gaus_norm\n",
    "    \n",
    "    return pdf / norm\n",
    "\n",
    "\n",
    "# Plot the pdfs\n",
    "t = np.linspace(t0_sec + plt_rng[0], t0_sec + plt_rng[1], 200) / secinday\n",
    "bg_pdf = time_bg_pdf(t, t0, -clip, dt + clip)\n",
    "sig_pdf = time_sig_pdf(t, t0, dt, nsig)\n",
    "\n",
    "# Plot in normalized time\n",
    "_t = t * secinday - t0 * secinday\n",
    "plt.plot(_t, bg_pdf, \"C0-\")\n",
    "plt.plot(_t, sig_pdf, \"C1-\")\n",
    "plt.axvline(dt, 0, 1, color=\"C3\", ls=\"--\")\n",
    "plt.axvline(0, 0, 1, color=\"C2\", ls=\"--\")\n",
    "\n",
    "# Plot injected events from all trials, relative times\n",
    "times = [i for arr in trials for i in arr] # Flatten out to single array\n",
    "times = (times - t0) * secinday\n",
    "_ = plt.hist(times, bins=50, normed=True, color=dg, alpha=.25)\n",
    "\n",
    "plt.xlabel(\"Time relative to t0 in sec\")\n",
    "plt.ylim(0, None);\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig(\"./data/figs/bg_events_time_sampled_narrow.png\", dpi=200)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Now the really large time frame, over the whole time range\n",
    "t0 = start_mjd[0]\n",
    "t0_sec = t0 * secinday\n",
    "\n",
    "# dt from t0 in seconds, clip at 4 sigma\n",
    "dt = (stop_mjd[-1] - start_mjd[0]) * secinday\n",
    "nsig = 4.\n",
    "\n",
    "# Make t values for plotting in MJD around t0\n",
    "clip = np.clip(dt, 2, 30) * nsig\n",
    "plt_rng = [-clip, dt + clip]\n",
    "trange = plt_rng\n",
    "ntrials = 1\n",
    "\n",
    "# Sample times\n",
    "trials = runlist_inj.sample(t0, trange, ntrials=ntrials)\n",
    "times = [i for arr in trials for i in arr] # Flatten out to single array\n",
    "\n",
    "h, b = np.histogram(times, bins=1081)\n",
    "m = get_binmids([b])[0]\n",
    "scale = np.diff(b) * secinday * ntrials\n",
    "yerr = np.sqrt(h) / scale\n",
    "h = h / scale\n",
    "\n",
    "plt.errorbar(m, h, yerr=yerr, fmt=\",\")\n",
    "\n",
    "# Plot normalized rate function to compare\n",
    "t = np.linspace(start_mjd[0], stop_mjd[-1], 100)\n",
    "r = runlist_inj.best_estimator(t)\n",
    "plt.plot(t, r, lw=2, zorder=5)\n",
    "plt.axhline(runlist_inj.best_pars[2], 0, 1, color=\"C1\",\n",
    "            ls=\"--\", label=\"\", zorder=5)\n",
    "\n",
    "plt.xlim(start_mjd[0], stop_mjd[-1])\n",
    "plt.ylim(0, 0.009)\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig(\"./data/figs/bg_events_time_sampled_wide.png\", dpi=200)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## BG Rate Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Test if fit, sample and integral works, with a simple example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### SinusRateFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define parameters for the test function\n",
    "period_days = 300.\n",
    "b = 2 * np.pi / period_days  # Period in 1/MJD\n",
    "c = 0  # t-Offset in MJD\n",
    "d = 1  # Rate offset in Hz = 1 evt/sec is average -> 86400 evts/day\n",
    "a = d / 2.  # Amplitude in Hz = +- 0.5 evts / per second\n",
    "pars = np.array([a, b, c, d])\n",
    "\n",
    "sinfun = RateFunc.SinusRateFunction()\n",
    "\n",
    "# Plot function\n",
    "t0, t1 = c, c + period_days\n",
    "t = np.linspace(0, t1, 200)  # In MJD days\n",
    "y = sinfun.fun(t, pars)\n",
    "\n",
    "_ = plt.plot(t, y, lw=2, label=\"fun\")\n",
    "\n",
    "# Plot integral\n",
    "intgrl = np.zeros_like(t)\n",
    "for i, ti in enumerate(t):\n",
    "    intgrl[i] = sinfun.integral(t=t0, trange=[t0, ti*secinday], pars=pars)\n",
    "    \n",
    "# Scale integral, we expect 24*3600=86400 evts/day * (period_days days)\n",
    "print(\"Expect   : \", secinday * t1)\n",
    "print(\"Integral : \", intgrl[-1])\n",
    "_ = plt.plot(t, intgrl / 1e7, lw=2, label=\"integral/1e7\")\n",
    "\n",
    "# Sample from whole range and scale normed hist with time scale to match rate\n",
    "nsam = int(1e4)\n",
    "sam = sinfun.sample(t=t0, trange=[t0, t1*secinday], pars=pars, n_samples=nsam)\n",
    "h, b = np.histogram(sam, range=[t0, t1], bins=50, density=True)\n",
    "m = get_binmids([b])[0]\n",
    "_ = plt.hist(m, bins=b, weights=h * (t1 - t0), color=\"C0\",\n",
    "             alpha=0.5, label=\"sampled\")\n",
    "\n",
    "# Finally fit the sampled points again\n",
    "runtime = (t1 - t0)\n",
    "p0 = None  # Test default args\n",
    "bf_pars = sinfun.fit(t=m, rate=h * (t1 - t0), rate_std=None, p0=p0)\n",
    "yfit = sinfun.fun(t, bf_pars)\n",
    "_ = plt.plot(t, yfit, lw=2, color=\"C3\", ls=\"--\", label=\"fitted\")\n",
    "p0 = sinfun._get_default_seed(t=m, rate=h * (t1 - t0),\n",
    "                              rate_std=np.ones_like(m))\n",
    "yseed = sinfun.fun(t, p0)\n",
    "_ = plt.plot(t, yseed, lw=2, color=\"C3\", ls=\"-\", alpha=0.3,\n",
    "             label=\"default seed\")\n",
    "\n",
    "plt.xlabel(\"time in MJD\")\n",
    "plt.ylabel(\"rate in Hz\")\n",
    "_ = plt.ylim(0, None)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig(\"data/figs/rate_function_test.png\", dpi=200)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Sinus1yrRateFunction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The same as above, but now with fixed period of 1 year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Define parameters for the test function\n",
    "period_days = 365.25\n",
    "c = 0  # t-Offset in MJD\n",
    "d = 1  # Rate offset in Hz = 1 evt/sec is average -> 86400 evts/day\n",
    "a = d / 2.  # Amplitude in Hz = +- 0.5 evts / per second\n",
    "pars = np.array([a, c, d])\n",
    "\n",
    "sinfun = RateFunc.Sinus1yrRateFunction()\n",
    "\n",
    "# Plot function\n",
    "t0, t1 = c, c + period_days\n",
    "t = np.linspace(0, t1, 200)  # In MJD days\n",
    "y = sinfun.fun(t, pars)\n",
    "\n",
    "_ = plt.plot(t, y, lw=2, label=\"fun\")\n",
    "\n",
    "# Plot integral\n",
    "intgrl = np.zeros_like(t)\n",
    "for i, ti in enumerate(t):\n",
    "    intgrl[i] = sinfun.integral(t=t0, trange=[t0, ti*secinday], pars=pars)\n",
    "    \n",
    "# Scale integral, we expect 24*3600=86400 evts/day * (period_days days)\n",
    "print(\"Expect   : \", secinday * t1)\n",
    "print(\"Integral : \", intgrl[-1])\n",
    "_ = plt.plot(t, intgrl / 1e7, lw=2, label=\"integral/1e7\")\n",
    "\n",
    "# Sample from whole range and scale normed hist with time scale to match rate\n",
    "nsam = int(1e4)\n",
    "sam = sinfun.sample(t=t0, trange=[t0, t1*secinday], pars=pars, n_samples=nsam)\n",
    "h, b = np.histogram(sam, range=[t0, t1], bins=50, density=True)\n",
    "m = get_binmids([b])[0]\n",
    "_ = plt.hist(m, bins=b, weights=h * (t1 - t0), color=\"C0\",\n",
    "             alpha=0.5, label=\"sampled\")\n",
    "\n",
    "# Finally fit the sampled points again\n",
    "runtime = (t1 - t0)\n",
    "p0 = None  # Test default args\n",
    "bf_pars = sinfun.fit(t=m, rate=h * (t1 - t0), rate_std=None, p0=p0)\n",
    "yfit = sinfun.fun(t, bf_pars)\n",
    "_ = plt.plot(t, yfit, lw=2, color=\"C3\", ls=\"--\", label=\"fitted\")\n",
    "p0 = sinfun._get_default_seed(t=m, rate=h * (t1 - t0),\n",
    "                              rate_std=np.ones_like(m))\n",
    "yseed = sinfun.fun(t, p0)\n",
    "_ = plt.plot(t, yseed, lw=2, color=\"C3\", ls=\"-\", alpha=0.3,\n",
    "             label=\"default seed\")\n",
    "\n",
    "plt.xlabel(\"time in MJD\")\n",
    "plt.ylabel(\"rate in Hz\")\n",
    "_ = plt.ylim(0, None)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig(\"data/figs/rate_function_test.png\", dpi=200)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### ConstantRateFunction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Use constant rate function but leave the sinus to see how the fit behaves.\n",
    "Otherwise it would be boring to just see 3 flat lines over another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define sinus parameters\n",
    "period_days = 365.25\n",
    "c = 0  # t-Offset in MJD\n",
    "d = 1  # Rate offset in Hz = 1 evt/sec is average -> 86400 evts/day\n",
    "a = d / 2.  # Amplitude in Hz = +- 0.5 evts / per second\n",
    "sinpars = np.array([a, c, d])\n",
    "\n",
    "# Same for the constant function.\n",
    "constpars = (d,)\n",
    "\n",
    "sinfun = RateFunc.Sinus1yrRateFunction()\n",
    "constfun = RateFunc.ConstantRateFunction()\n",
    "\n",
    "# Plot sinus and constant function\n",
    "t0, t1 = c, c + period_days\n",
    "t = np.linspace(0, t1, 200)  # In MJD days\n",
    "y = sinfun.fun(t, sinpars)\n",
    "yc = constfun.fun(t, constpars)\n",
    "\n",
    "_ = plt.plot(t, y, color=\"C0\", lw=2, ls=\"--\")\n",
    "_ = plt.plot(t, yc, lw=2, label=\"fun\")\n",
    "\n",
    "# Plot integral\n",
    "intgrl = np.zeros_like(t)\n",
    "for i, ti in enumerate(t):\n",
    "    intgrl[i] = constfun.integral(t=t0, trange=[t0, ti*secinday],\n",
    "                                  pars=constpars)\n",
    "    \n",
    "# Scale integral, we expect 24*3600=86400 evts/day * (period_days days)\n",
    "print(\"Expect   : \", secinday * t1)\n",
    "print(\"Integral : \", intgrl[-1])\n",
    "_ = plt.plot(t, intgrl / 1e7, lw=2, label=\"integral/1e7\")\n",
    "\n",
    "# Sample from whole range and scale normed hist with time scale to match rate\n",
    "nsam = int(1e4)\n",
    "sam = sinfun.sample(t=t0, trange=[t0, t1*secinday], pars=sinpars,\n",
    "                      n_samples=nsam)\n",
    "h, b = np.histogram(sam, range=[t0, t1], bins=50, density=True)\n",
    "m = get_binmids([b])[0]\n",
    "_ = plt.hist(m, bins=b, weights=h * (t1 - t0), color=\"C0\",\n",
    "             alpha=0.5, label=\"sampled\")\n",
    "\n",
    "# Finally fit the sampled points again\n",
    "runtime = (t1 - t0)\n",
    "p0 = None  # Test default args\n",
    "bf_pars = constfun.fit(t=m, rate=h * (t1 - t0), rate_std=None, p0=p0)\n",
    "yfit = constfun.fun(t, bf_pars)\n",
    "_ = plt.plot(t, yfit, lw=2, color=\"C3\", ls=\"--\", label=\"fitted\")\n",
    "p0 = constfun._get_default_seed(t=m, rate=h * (t1 - t0),\n",
    "                                rate_std=np.ones_like(m))\n",
    "yseed = constfun.fun(t, p0)\n",
    "_ = plt.plot(t, yseed, lw=2, color=\"C3\", ls=\"-\", alpha=0.3,\n",
    "             label=\"default seed\")\n",
    "\n",
    "plt.xlabel(\"time in MJD\")\n",
    "plt.ylabel(\"rate in Hz\")\n",
    "_ = plt.ylim(0, None)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig(\"data/figs/rate_function_test.png\", dpi=200)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### NonRateFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Just check that no errors are thrown and how fast it is\n",
    "nonfun = RateFunc.NonRateFunction()\n",
    "bf_pars = nonfun.fit(t=m, rate=h * (t1 - t0))\n",
    "%timeit nonfun.fun(t, bf_pars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the LLH module.\n",
    "\n",
    "It contains all functions for a specific LLH we want to use in our analysis.\n",
    "Currently GRBLLH is implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "sin_dec_bins = np.linspace(-1, 1, 50)\n",
    "\n",
    "min_logE = 1  #  min(np.amin(_exp[\"logE\"]), np.amin(mc[\"logE\"]))\n",
    "max_logE = 10 #  max(np.amax(_exp[\"logE\"]), np.amax(mc[\"logE\"]))\n",
    "logE_bins = np.linspace(min_logE, max_logE, 40)\n",
    "\n",
    "spatial_pdf_args = {\"bins\": sin_dec_bins, \"k\": 3, \"kent\": True}\n",
    "\n",
    "energy_pdf_args = {\"bins\": [sin_dec_bins, logE_bins],\n",
    "                   \"gamma\": 2., \"fillval\": \"col\", \"interpol_log\": False}\n",
    "\n",
    "time_pdf_args = {\"nsig\": 4., \"sigma_t_min\": 2., \"sigma_t_max\": 30.}\n",
    "\n",
    "grbllh = LLH.GRBLLH(X=_exp, MC=mc, srcs=None,\n",
    "                    spatial_pdf_args=spatial_pdf_args,\n",
    "                    energy_pdf_args=energy_pdf_args,\n",
    "                    time_pdf_args=time_pdf_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Time PDF Ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Reproduce the paper plot.\n",
    "\n",
    "Note that we get the PDFs for all srcs at once.\n",
    "Their times are just all the same here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Make a plot with ratios for different time windows as in the paper\n",
    "# dt from t0 in seconds, clip at 4 sigma\n",
    "dts = [[-1, 5], [-5, 50], [-20, 200]]\n",
    "nsrcs = len(dts)\n",
    "nsig = 4\n",
    "\n",
    "# Arbitrary start date from data\n",
    "t0 = np.repeat(np.random.choice(exp[\"timeMJD\"], size=1),\n",
    "               repeats=nsrcs).reshape(nsrcs, 1)\n",
    "t0_sec = t0[0] * secinday  # Only single number needed, t0s are all equal\n",
    "\n",
    "# Make t values for plotting in MJD around t0, to fit all in one plot\n",
    "max_dt, min_dt = np.amax(dts), np.amin(dts)\n",
    "dt_tot = max_dt - min_dt\n",
    "clip = np.clip(dt_tot, 2, 30) * nsig\n",
    "plt_range = np.array([min_dt - clip, max_dt + clip])\n",
    "t = np.linspace(t0_sec + 1.2 * plt_range[0],\n",
    "                t0_sec + 1.2 * plt_range[1], 1000) / secinday\n",
    "_t = t * secinday - t0 * secinday\n",
    "\n",
    "# Mark event time\n",
    "plt.axvline(0, 0, 1, c=\"k\", ls=\"--\", lw=2, alpha=0.8)\n",
    "\n",
    "# Get all at once\n",
    "SoB = grbllh._soverb_time(t=t, src_t=t0, dt=dts)\n",
    "\n",
    "colors = [\"C0\", \"C3\", \"C2\"]\n",
    "for i in range(len(SoB)):\n",
    "    # Plot seperately to give colors and labels\n",
    "    plt.plot(_t[i], SoB[i], lw=2, c=colors[i],\n",
    "             label=r\"$T_\\mathrm{{uni}}$: {:>3d}s, {:>3d}s\".format(*dts[i]))\n",
    "    # Fill uniform part, might look nicely\n",
    "    # fbtw = (_t > 0) & (_t < dt)\n",
    "    # plt.fill_between(_t[fbtw], 0, SoB[fbtw], color=\"C7\", alpha=0.1)\n",
    "\n",
    "# Plot stacked\n",
    "weights = np.ones(nsrcs).reshape(nsrcs, 1) / nsrcs\n",
    "stacked = np.sum(SoB * weights, axis=0) \n",
    "plt.plot(_t[0], stacked, ls=\"--\", c=\"k\", lw=2, label=\"stacked\", alpha=0.8)\n",
    "    \n",
    "# Make it look like the paper plot, but with slightly extended borders\n",
    "plt.xlim(1.2 * plt_range)\n",
    "plt.ylim(0, np.amax(SoB) * 1.05)\n",
    "plt.xlabel(\"t - t0 in sec\")\n",
    "plt.ylabel(\"S / B\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.grid(ls=\"--\", lw=1)\n",
    "\n",
    "# plt.savefig(\"./data/figs/time_pdf_ratio.png\", dpi=200)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Get the injection time window.\n",
    "This is needed for the injector, so only events in regions with non-zero PDF are injected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "grbllh.get_injection_trange(t0, dts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Compare manually\n",
    "dts = np.array([[-1, 5], [-5, 50], [-20, 200]], dtype=np.float)\n",
    "nsig, sig_min, sig_max = time_pdf_args.values()  # Beware if order is wrong :P\n",
    "clip = np.clip(np.diff(dts, axis=1), sig_min, sig_max) * nsig\n",
    "dts[:, 0] -= clip.reshape(len(dts))  # Same as flatten()\n",
    "dts[:, 1] += clip.flatten()\n",
    "\n",
    "dts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Spatial background spline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This is the same technique as used in skylab, but with an extra step of adding the outermost bin edges to the spline gridpoints.\n",
    "This way, the spline behaves reasonable at the edges and doesn't overshoot.\n",
    "\n",
    "We could extend this by using the KDE integrated over every variable and then fitting a spline to that.\n",
    "Or we could sample from the KDE and bin finely and fit a splien again.\n",
    "\n",
    "For now we leave only the option to use data directly.\n",
    "The spline fit is depending on the binning anyway.\n",
    "Only the finely binned KDE version could resolve that issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sin_dec = np.linspace(-1.05, 1.05, 200)\n",
    "y = np.exp(grbllh._spatial_bg_spl(sin_dec))\n",
    "_ = plt.hist(np.sin(_exp[\"dec\"]), bins=50, normed=True)\n",
    "plt.plot(sin_dec, y, lw=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Spatial background pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Should be identical to calling the spline directly, except that the BG PDF is normalized to the whole sphere.\n",
    "So we multiply the values by 2pi to account for that.\n",
    "\n",
    "Here we see the difference to just calling the spline directly: The PDF is zero outside the definition range, the spline extrapolated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "_ = plt.hist(np.sin(_exp[\"dec\"]), bins=grbllh.spatial_pdf_args[\"bins\"],\n",
    "             normed=True)\n",
    "sin_dec = np.linspace(-1.05, 1.05, 200)\n",
    "y = 2 * np.pi * grbllh._pdf_spatial_background(ev_sin_dec=sin_dec)\n",
    "plt.plot(sin_dec, y, lw=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Spatial signal PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Compare signal and BG pdf.\n",
    "\n",
    "First we create multiple sources and a single event and scan the event PDF by moving the event along the declination axis.\n",
    "All PDFs have the height, because the same sigma is used.\n",
    "\n",
    "Note that BG is here usually very small compared to the signal, because we sample the ev positions within 1 sigma around the source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "LOG = False\n",
    "\n",
    "nsrcs = 4\n",
    "# Choose the event sigma from data\n",
    "ev_sigma = np.random.choice(_exp[\"sigma\"], size=1)\n",
    "\n",
    "# Make nsrcs, same ra, but different dec. decs are distributed uniformly in\n",
    "# the range of the largest sigma from the events (for illustration only)\n",
    "src_dec = np.random.uniform(-ev_sigma, ev_sigma, size=nsrcs)\n",
    "src_ra = np.ones_like(src_dec) * np.pi\n",
    "plt_rnge = [np.amin(src_dec) - ev_sigma, np.amax(src_dec) + ev_sigma]\n",
    "\n",
    "# Scan signal PDF for event declination\n",
    "ev_dec = np.sin(np.linspace(plt_rnge[0], plt_rnge[1], 200))\n",
    "ev_sin_dec = np.sin(ev_dec)\n",
    "ev_ra = src_ra[0] * np.ones_like(ev_sin_dec)\n",
    "ev_sig = np.ones_like(ev_sin_dec) * ev_sigma\n",
    "\n",
    "# y has shape (nsrcs, nevts), where nevts are the ev_sin_dec values here (scan)\n",
    "y = grbllh._pdf_spatial_signal(src_ra, src_dec, ev_ra, ev_sin_dec, ev_sig)\n",
    "\n",
    "if LOG:\n",
    "    y = np.log10(y)\n",
    "\n",
    "plt.plot(ev_dec, y.T, lw=2)\n",
    "plt.vlines(src_dec, 0, np.amax(y), color=\"C7\", linestyles=\"--\", lw=2,\n",
    "           label=\"srcs pos\")\n",
    "\n",
    "# Plot BG PDF to compare\n",
    "bg = grbllh._pdf_spatial_background(ev_sin_dec=ev_sin_dec)\n",
    "plt.plot(ev_dec, bg, lw=2, label=\"BG\")\n",
    "\n",
    "plt.xlim(*plt_rnge)\n",
    "if LOG:\n",
    "    plt.ylim(1e-5, 1.1 * np.amax(y))\n",
    "else:\n",
    "    plt.ylim(0, 1.1 * np.amax(y))\n",
    "    \n",
    "    \n",
    "plt.xlabel(\"dec\")\n",
    "plt.ylabel(\"PDF per src\")\n",
    "plt.legend()\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here we use multiple events with different sigmas and scan again in declination by moving a single possible src position.\n",
    "We get different heights, because of the different sigmas.\n",
    "\n",
    "The PDFs each peak where the event position is.\n",
    "If we had a single source, we would just read off the values at that position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "LOG = True\n",
    "# Make nevts, same ra, but different dec. sigmas chosen from data\n",
    "nevts = 4\n",
    "ev_sigma = np.random.choice(_exp[\"sigma\"], size=nevts)\n",
    "# Sample some evt decs uniformly around the horizon with spread of the largest \n",
    "# sigma to get some variation\n",
    "max_sig = np.amax(ev_sigma)\n",
    "ev_dec = np.random.uniform(-max_sig, max_sig, size=nevts)\n",
    "ev_sin_dec = np.sin(ev_dec)\n",
    "ev_ra = np.ones_like(ev_dec) * np.pi\n",
    "\n",
    "# Plot margin PDF scanned for each src position for each event position\n",
    "src_dec = np.linspace(-2. * max_sig, 2 * max_sig, 200)\n",
    "src_ra = ev_ra[0] * np.ones_like(src_dec)\n",
    "\n",
    "# This has shape (nsrcs, nevts)\n",
    "y = grbllh._pdf_spatial_signal(src_ra, src_dec, ev_ra, ev_sin_dec, ev_sigma)\n",
    "\n",
    "if LOG:\n",
    "    y = np.log10(y)\n",
    "\n",
    "plt.plot(src_dec, y, lw=2)\n",
    "\n",
    "plt.vlines(ev_dec, 0, np.amax(y) * 1.1, color=\"C7\",\n",
    "           linestyles=\"--\", label=\"evts pos\")\n",
    "\n",
    "# Plot BG PDF to compare\n",
    "bg = grbllh._pdf_spatial_background(ev_sin_dec=np.sin(src_dec))\n",
    "plt.plot(src_dec, bg, lw=2, label=\"BG\")\n",
    "\n",
    "plt.xlim(src_dec[[0, -1]])\n",
    "if LOG:\n",
    "    plt.ylim(1e-5, 1.1 * np.amax(y))\n",
    "else:\n",
    "    plt.ylim(0, 1.1 * np.amax(y))\n",
    "    \n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Spatial PDF ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_dec_vs_signal(S, ev_dec, src_ra, src_dec, weights, ax=None):\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots(1, 1)\n",
    "    # Plot signal per source for each event\n",
    "    for i, (sra, sdec) in enumerate(zip(src_ra, src_dec)):\n",
    "        ax.plot(np.rad2deg(ev_dec), S[i], ls=\"-\")\n",
    "        ax.plot(np.rad2deg(sdec), -10, \"k|\")\n",
    "\n",
    "    # Simulate a simple stacking, one weight per source\n",
    "    ax.plot(np.rad2deg(ev_dec), np.sum(weights * S, axis=0) / np.sum(weights),\n",
    "             ls=\"--\", c=dg, label=\"stacked\")\n",
    "\n",
    "    ax.set_xlim([-1 + smin, smax + 1])\n",
    "    ax.set_xlabel(\"DEC in °\")\n",
    "    ax.set_ylabel(\"Signal pdf\")\n",
    "    ax.legend(loc=\"upper right\")\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We make 4 plots to test everything:\n",
    "\n",
    "1. [Top left] We place densely packed srcs at the declination range and scan the PDFs by varying the event declinations.\n",
    "   Sigma is fixed to 1 for illustration.\n",
    "   We expect just a row of gaussians along the dec range.\n",
    "   The stacked signal is the weighted sum of all signal contributions at a single event dec position.\n",
    "   \n",
    "2. [Bottom left] We plot just the background PDF and its inverse for the dec range.\n",
    "   The inverse PDF is what modulates the signal PDF.\n",
    "   \n",
    "3. [Top right] This modulation can be seen in this plot.\n",
    "   It is basically the same as the first one, but now it's signal over background.\n",
    "   So the signal peaks are modulated with the inverse BG PDF.\n",
    "   \n",
    "4. [Bottom right] This is the same plot as the third one, but this time we use the real data declination values instead of nicely spaced ones.\n",
    "   The effect is the same but not reall visible, because each event has a different sigma, so the PDFs all have different heights and widths.\n",
    "   It becomes more similar when using an 1° sigma for all events (just comment that line in)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Make srcs across the dec range. The hull of SoB should be shaped like the\n",
    "# 1/(sinDec BG distribtuion). With a single source we couldn't see that,\n",
    "# because it drops to zero far from the src position\n",
    "smin, smax, step = -90, +90, 10\n",
    "src_ra = np.deg2rad(np.arange(smin, smax + step, step))\n",
    "src_dec = np.deg2rad(np.arange(smin, smax + step, step))\n",
    "\n",
    "# Scan in dec by varying the evts dec\n",
    "ev_ra = np.deg2rad(np.linspace(smin, smax, 1000))\n",
    "ev_dec = np.deg2rad(np.linspace(smin, smax, 1000))\n",
    "ev_sin_dec = np.sin(ev_dec)\n",
    "ev_sig = np.deg2rad(np.ones_like(ev_ra))\n",
    "\n",
    "# Some pseudo weights to simulate stacking\n",
    "weights = np.arange(1, len(src_dec) + 1)[:, np.newaxis]\n",
    "\n",
    "fig, ((axtl, axtr), (axbl, axbr)) = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Signal only\n",
    "S = grbllh._pdf_spatial_signal(src_ra, src_dec, ev_ra, ev_sin_dec, ev_sig)\n",
    "_ = plot_dec_vs_signal(S, ev_dec, src_ra, src_dec, weights, ax=axtl)\n",
    "axtl.set_xlim(-90, 90)\n",
    "\n",
    "# Background only\n",
    "bins = grbllh.spatial_pdf_args[\"bins\"]\n",
    "h, b = np.histogram(np.sin(_exp[\"dec\"]), bins=bins, density=True)\n",
    "m = 0.5 * (b[:-1] + b[1:])\n",
    "_ = axbl.hist(m, bins=bins, weights=h / 2 / np.pi, alpha=0.5)\n",
    "_sin_dec = np.linspace(-1, 1, 1000)\n",
    "bg_pdf = grbllh._pdf_spatial_background(_sin_dec)\n",
    "axbl.plot(_sin_dec, bg_pdf, lw=2, label=\"pdf\")\n",
    "axbl.set_ylim(0, 0.2)\n",
    "# 1 / BG PDF on second axis\n",
    "axbl2 = axbl.twinx()\n",
    "axbl2.plot(_sin_dec, 1. / bg_pdf, c=\"C2\", lw=2, ls=\"--\", label=\"1/pdf\")\n",
    "axbl2.set_ylim(0, (1 / bg_pdf).max())\n",
    "axbl.set_xlabel(\"sinus DEC\")\n",
    "axbl.set_xlim(-1, 1)\n",
    "axbl.legend(loc=\"upper left\")\n",
    "axbl2.legend(loc=\"upper center\")\n",
    "\n",
    "# SoB on example + BG PDF\n",
    "SoB = grbllh._soverb_spatial(src_ra, src_dec, ev_ra, ev_sin_dec, ev_sig)\n",
    "weights = np.arange(1, len(src_dec) + 1)[:, np.newaxis]\n",
    "_ = plot_dec_vs_signal(SoB, ev_dec, src_ra, src_dec, weights, ax=axtr)\n",
    "axtr.plot(np.rad2deg(np.arcsin(_sin_dec)), bg_pdf, lw=3, label=\"BG pdf\", c=dg)\n",
    "axtr.set_xlim(-90, 90)\n",
    "axtr.set_yscale(\"log\")\n",
    "axtr.set_ylim(np.amin(bg_pdf), 1e5)\n",
    "axtr.legend(loc=\"upper left\")\n",
    "\n",
    "# Now with the real data. Sort first in dec to show with nice lines + BG PDF\n",
    "idx = np.argsort(exp[\"dec\"])\n",
    "ev_ra = exp[\"ra\"][idx]\n",
    "ev_dec = exp[\"dec\"][idx]\n",
    "ev_sin_dec = np.sin(ev_dec)\n",
    "ev_sig = exp[\"sigma\"][idx]\n",
    "# Comment in to match the simple example (all events have sigma 1°)\n",
    "# ev_sig = np.deg2rad(np.ones_like(ev_ra))\n",
    "SoB = grbllh._soverb_spatial(src_ra, src_dec, ev_ra, ev_sin_dec, ev_sig)\n",
    "\n",
    "_ = plot_dec_vs_signal(SoB, ev_dec, src_ra, src_dec, weights, ax=axbr)\n",
    "axbr.plot(np.rad2deg(np.arcsin(_sin_dec)), bg_pdf,\n",
    "          lw=3, label=\"BG pdf\", c=\"C0\")\n",
    "axbr.set_yscale(\"log\")\n",
    "axbr.set_ylim(np.amin(bg_pdf), 1e5)\n",
    "axbr.legend(loc=\"upper left\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Energy ratio spline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This is the creation of the signal over background ratio for the energy PDF.\n",
    "It is resolved in sinDec and logE to account for different positions on the sky and energies.\n",
    "\n",
    "Missing values, where no data or MC is present is filled with interpolation values, conttrolled by the \"fillval\" option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, (al, ar) = plt.subplots(1, 2, figsize=(14,5))\n",
    "\n",
    "energy_pdf_args = {\"bins\": [sin_dec_bins, logE_bins],\n",
    "                   \"gamma\": 2., \"fillval\": \"col\", \"interpol_log\": False}\n",
    "grbllh = LLH.GRBLLH(X=_exp, MC=mc, srcs=None,\n",
    "                    spatial_pdf_args=spatial_pdf_args,\n",
    "                    energy_pdf_args=energy_pdf_args,\n",
    "                    time_pdf_args=time_pdf_args)\n",
    "\n",
    "# Ratio spline with 'col' filling\n",
    "x = np.linspace(-1.1, 1.1, num=1000 + 1)\n",
    "y = np.linspace(0.5, 10.5, num=1000 + 1)\n",
    "XX, YY = np.meshgrid(x, y)\n",
    "xx, yy = map(np.ravel, [XX, YY])\n",
    "gpts = np.vstack((xx, yy)).T\n",
    "zz = np.exp(grbllh._energy_spl(gpts))\n",
    "ZZ = zz.reshape(XX.shape)\n",
    "# Plotting with hist creates strange effects... Use pcolormesh instead\n",
    "img = al.pcolormesh(XX, YY, ZZ, norm=LogNorm(), cmap=\"coolwarm\",\n",
    "                    vmin=1e-3, vmax=1e3)\n",
    "al.set_title(\"Spline interpolation: 'col'\")\n",
    "plt.colorbar(ax=al, mappable=img)\n",
    "\n",
    "# With 'minmax' filling. Note: The small values in the lower row are due to\n",
    "# plotting in log. We interpolate in linear space, so in log, the jump is\n",
    "# very steep for small values.\n",
    "energy_pdf_args = {\"bins\": [sin_dec_bins, logE_bins],\n",
    "                   \"gamma\": 2., \"fillval\": \"minmax\", \"interpol_log\": False}\n",
    "grbllh = LLH.GRBLLH(X=_exp, MC=mc, srcs=None,\n",
    "                    spatial_pdf_args=spatial_pdf_args,\n",
    "                    energy_pdf_args=energy_pdf_args,\n",
    "                    time_pdf_args=time_pdf_args)\n",
    "\n",
    "zz = np.exp(grbllh._energy_spl(gpts))\n",
    "ZZ = zz.reshape(XX.shape)\n",
    "img = ar.pcolormesh(XX, YY, ZZ, norm=LogNorm(), cmap=\"coolwarm\",\n",
    "                    vmin=1e-3, vmax=1e3)\n",
    "ar.set_title(\"Spline interpolation: 'minmax'\")\n",
    "plt.colorbar(ax=ar, mappable=img)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Energy PDF ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here we see again the difference to the direct spline evaluation.\n",
    "The ratio function set's values outside to zero probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, (al, ar) = plt.subplots(1, 2, figsize=(14,5))\n",
    "\n",
    "energy_pdf_args = {\"bins\": [sin_dec_bins, logE_bins],\n",
    "                   \"gamma\": 2., \"fillval\": \"col\", \"interpol_log\": True}\n",
    "grbllh = LLH.GRBLLH(X=_exp, MC=mc, srcs=None,\n",
    "                    spatial_pdf_args=spatial_pdf_args,\n",
    "                    energy_pdf_args=energy_pdf_args,\n",
    "                    time_pdf_args=time_pdf_args)\n",
    "\n",
    "# Ratio spline with 'col' filling\n",
    "x = np.linspace(-1.1, 1.1, num=1000 + 1)\n",
    "y = np.linspace(0.5, 10.5, num=1000 + 1)\n",
    "XX, YY = np.meshgrid(x, y)\n",
    "xx, yy = map(np.ravel, [XX, YY])\n",
    "gpts = np.vstack((xx, yy)).T\n",
    "zz = grbllh._soverb_energy(xx, yy)\n",
    "ZZ = zz.reshape(XX.shape)\n",
    "# Plotting with hist creates strange effects... Use pcolormesh instead\n",
    "img = al.pcolormesh(XX, YY, ZZ, norm=LogNorm(), cmap=\"coolwarm\",\n",
    "                    vmin=1e-3, vmax=1e3)\n",
    "al.set_title(\"Spline interpolation: 'col'\")\n",
    "plt.colorbar(ax=al, mappable=img)\n",
    "\n",
    "# With 'minmax' filling. Note: The small values in the lower row are due to\n",
    "# plotting in log. We interpolate in linear space, so in log, the jump is\n",
    "# very steep for small values.\n",
    "energy_pdf_args = {\"bins\": [sin_dec_bins, logE_bins],\n",
    "                   \"gamma\": 2., \"fillval\": \"minmax\", \"interpol_log\": True}\n",
    "grbllh = LLH.GRBLLH(X=_exp, MC=mc, srcs=None,\n",
    "                    spatial_pdf_args=spatial_pdf_args,\n",
    "                    energy_pdf_args=energy_pdf_args,\n",
    "                    time_pdf_args=time_pdf_args)\n",
    "\n",
    "zz = grbllh._soverb_energy(xx, yy)\n",
    "ZZ = zz.reshape(XX.shape)\n",
    "img = ar.pcolormesh(XX, YY, ZZ, norm=LogNorm(), cmap=\"coolwarm\",\n",
    "                    vmin=1e-3, vmax=1e3)\n",
    "ar.set_title(\"Spline interpolation: 'minmax'\")\n",
    "plt.colorbar(ax=ar, mappable=img)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Detector source weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We use the same spline method to create a spline describing the sinDec dependence of a signal MC weighted to a specific astrophysical flux modell (usually unbroken power law).\n",
    "\n",
    "Depending on the src position, we expect more or less signal from that src.\n",
    "This is equivalent to folding with the detector exposure function.\n",
    "\n",
    "Our stacking form is described by a multi position search where the signal term gets modified to:\n",
    "\n",
    "$$\n",
    "    S^\\text{tot} = \\sum_{j=1}^{N_\\text{srcs}} w_j S_{ij} \\quad\\text{with}\\quad\n",
    "    \\sum_j w_j = 1 \\quad\\text{with}\\quad w_j = w_j^\\text{theo}\\cdot w_j^\\text{det}\n",
    "$$\n",
    "\n",
    "The weights are a combination of the exposure weights and a-priori fixed intrinsic source weights, eg. from a known gamma flux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Small hack to change the gamma without recreating the grbllh object\n",
    "gamma_override = 2.13\n",
    "\n",
    "grbllh.energy_pdf_args[\"gamma\"] = gamma_override\n",
    "mc_sin_dec = np.sin(mc[\"dec\"])\n",
    "mc_bins = grbllh.energy_pdf_args[\"bins\"][0]\n",
    "mc_dict = {\"trueE\": mc[\"trueE\"], \"ow\": mc[\"ow\"]}\n",
    "\n",
    "grbllh._spatial_signal_spl = grbllh._create_sin_dec_spline(\n",
    "    sin_dec=mc_sin_dec, bins=mc_bins, mc=mc_dict)\n",
    "\n",
    "sin_dec = np.linspace(-1.05, 1.05, 200)\n",
    "y = np.exp(grbllh._spatial_signal_spl(sin_dec))\n",
    "\n",
    "# MC needs proper weighting\n",
    "gamma = grbllh.energy_pdf_args[\"gamma\"]\n",
    "mc_w = mc[\"ow\"] * mc[\"trueE\"]**(-gamma)\n",
    "mc_bins = energy_pdf_args[\"bins\"][0]\n",
    "h, b = np.histogram(np.sin(mc[\"dec\"]), bins=mc_bins, weights=mc_w, normed=True)\n",
    "\n",
    "# Smooth it, charge it, odd it, quick truncate it\n",
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.savgol_filter.html\n",
    "m = get_binmids([b])[0]\n",
    "redux = 10  # Window len is nearest odd number to (number of bins / redux)\n",
    "window_len = int(2 * np.floor((len(b) / redux) / 2) + 1)  # Must be odd\n",
    "_h = scsignal.savgol_filter(h, window_len, 3, mode=\"mirror\")\n",
    "plt.hist(m, bins=b, weights=_h, histtype=\"step\", lw=2, color=\"C1\", ls=\"--\")\n",
    "plt.hist(m, bins=b, weights=h, histtype=\"step\", lw=2, color=\"C3\")\n",
    "\n",
    "# Plot spline (fitted to unsmoothed)\n",
    "plt.plot(sin_dec, y, lw=2, color=\"C2\")\n",
    "\n",
    "# Get weights for some srcs\n",
    "src_sin_dec = np.linspace(-1, 1, 11)\n",
    "src_dec = np.arcsin(src_sin_dec)\n",
    "src_w_theo = np.ones_like(src_dec)\n",
    "w = grbllh.get_src_weights(src_dec=src_dec, src_w_theo=src_w_theo)\n",
    "\n",
    "# Revoke norm for plotting to see if weights are on the curve\n",
    "src_dec_w = np.exp(grbllh._spatial_signal_spl(src_sin_dec))\n",
    "_w = w * np.sum(src_dec_w * src_w_theo)\n",
    "\n",
    "plt.plot(src_sin_dec, _w, \"wo\", ms=7, mew=1.5, mec=\"k\")\n",
    "plt.vlines(np.sin(src_dec), 0, 1.05 * np.amax(y), colors=\"C7\",\n",
    "           lw=1, linestyles=\"--\")\n",
    "\n",
    "plt.xlabel(\"sin(dec)\")\n",
    "plt.ylabel(\"PDF\")\n",
    "plt.title(\"$\\gamma = {:.1f}$\".format(gamma))\n",
    "\n",
    "plt.xlim(-1, 1)\n",
    "plt.ylim(0, 1.05 * np.amax(y))\n",
    "plt.tight_layout()\n",
    "\n",
    "print(w)\n",
    "print(w.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here we just use some ascending theoretical weights in both directions.\n",
    "\n",
    "- w1 should resemble the sindec curve from above\n",
    "- w2 should rise overall to the right (less steep or reversed from w1)\n",
    "- w3 should fall overall to the right (steeper than w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "_src_dec = np.arcsin(np.linspace(-1, 1, 21))\n",
    "\n",
    "# Compare for different theoretical weights\n",
    "src_w_theo = np.ones_like(_src_dec)\n",
    "w1 = grbllh.get_src_weights(src_dec=_src_dec, src_w_theo=src_w_theo)\n",
    "\n",
    "src_w_theo = np.arange(len(_src_dec)) + 1\n",
    "w2 = grbllh.get_src_weights(src_dec=_src_dec, src_w_theo=src_w_theo)\n",
    "\n",
    "src_w_theo = (np.arange(len(_src_dec)) + 1)[::-1]\n",
    "w3 = grbllh.get_src_weights(src_dec=_src_dec, src_w_theo=src_w_theo)\n",
    "\n",
    "plt.plot(np.sin(_src_dec), w1, \"o\", label=\"theo = 1 (orig)\")\n",
    "plt.plot(np.sin(_src_dec), w2, \"o\", label=\"theo = arange\")\n",
    "plt.plot(np.sin(_src_dec), w3, \"o\", label=\"theo = arange[::-1]\")\n",
    "plt.legend(loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Check if weights are the same when using event density (skylab style) instead.\n",
    "It should make no difference because the weights are normalized anyway.\n",
    "\n",
    "Note: The normalization and the pivot will not be included in the end, because they are constant for every source and for every dataset/year, so they get normalized out anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Weight with numu diffuse 6yr flux norm, but same index as above\n",
    "index = gamma_override\n",
    "norm = 0.9 * 1e-18  # (GeV s sr cm^2)^-1, valid at 100 TeV = 1e5 GeV\n",
    "pivot = 1e5\n",
    "flux = norm * (mc[\"trueE\"] / pivot)**(-index)\n",
    "mc_w = mc[\"ow\"] * flux * livetime * secinday\n",
    "mc_bins = energy_pdf_args[\"bins\"][0]\n",
    "\n",
    "density = True\n",
    "h, b = np.histogram(np.sin(mc[\"dec\"]), bins=mc_bins, weights=mc_w,\n",
    "                    density=density)\n",
    "\n",
    "# Normalize (same as density=True)\n",
    "if not density:\n",
    "    h /= np.diff(b) * np.sum(h)\n",
    "\n",
    "# PDF * Number of total events = Event densitiy\n",
    "_h = h * mc_w.sum()  \n",
    "\n",
    "mids = get_binmids([mc_bins])[0]\n",
    "_ = plt.hist(mids, bins=mc_bins, weights=_h)\n",
    "\n",
    "plt.xlabel(\"sindec\")\n",
    "plt.ylabel(\"Event density Nevts / sindec\")\n",
    "\n",
    "# Total events by integrating _h: Ntot = sum_i (_h_i * diff(bins)_i)\n",
    "plt.title(\"Gamma = {:.2f}: Ntot = {:.2f}\".format(\n",
    "        index, np.sum(_h * np.diff(mc_bins))))\n",
    "\n",
    "# Quick check the weight, no spline involved, directly use hist vals, so the\n",
    "# Weights are not 100% percent equal. But equal enough to confirm\n",
    "# Hack to include the first data point, as digitize is exclusive...\n",
    "src_sin_dec[0] = -0.99\n",
    "idx = np.digitize(src_sin_dec, bins=mc_bins, right=True) - 1\n",
    "src_w_dec = _h[idx]\n",
    "src_w_theo = np.ones_like(src_dec)\n",
    "src_w = src_w_dec * src_w_theo / np.sum(src_w_dec * src_w_theo)\n",
    "\n",
    "plt.plot(src_sin_dec, src_w_dec, \"wo\", ms=7, mew=1.5, mec=\"k\")\n",
    "plt.tight_layout()\n",
    "\n",
    "print(\"Spline from PDF only\")\n",
    "print(w)\n",
    "\n",
    "print(\"\\nSpline from event density with livetime\")\n",
    "print(src_w.reshape(len(src_w), 1))\n",
    "\n",
    "print(\"\\nRatio\")\n",
    "print(w / src_w.reshape(len(src_w), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### ln-LLH ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Plot llh and gradient.\n",
    "The gradient is calculated analytically.\n",
    "With this test, we simply want to check, if the gradient is OK and the likelihood behaves correctly.\n",
    "\n",
    "First with only one source.\n",
    "\n",
    "Note: We test here \"super-signal-like\" events. Every event is exactly at the src position and every time is exactly in the rime window, where the ration is max- Only the energy is distributed as background.\n",
    "So only for really large time windows (really large) which have insanely high background rates we drop lower than the injected ns in our prediction.\n",
    "This is because the background term can only counter background-like events, which have a low signal over background ratio.\n",
    "For the events injected here, this rate is super high, so we always \"fit\" the exact amount of injected events.\n",
    "\n",
    "For such a setup for each event SoB is equal.\n",
    "So the gradient is zero at:\n",
    "\n",
    "\\begin{align}\n",
    "    0 &= -1 + \\sum_i \\frac{S}{n_b B}\\cdot \\frac{1}{n_s \\frac{S}{n_b B} + 1}\n",
    "       = -1 + N \\frac{S}{n_b B}\\cdot \\frac{1}{n_s \\frac{S}{n_b B} + 1} \\\\\n",
    "    \\Leftrightarrow \\frac{1}{N} &= \\frac{1}{n_s + \\frac{n_b B}{S}} \\\\\n",
    "    \\Leftrightarrow N &= n_s + \\frac{n_b B}{S}\n",
    "\\end{align}\n",
    "\n",
    "So now if the signal is super large (and that's what we ensured by using our super-signal-like events) the term $\\frac{n_b B}{S} \\rightarrow 0$ and we get $\\hat{n}_S = N$ which is exactly what we observe.\n",
    "\n",
    "Only if we set super high $n_B$, our $\\hat{n}_S$ shrinks as $\\frac{n_b B}{S}$ gets larger and larger and in the end $\\hat{n}_S$ even turns negative, when the 1 / SoB ratio is larger than N."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Make up some setup\n",
    "src_t = np.random.choice(_exp[\"timeMJD\"], size=1)\n",
    "dt = np.array([-20, 200])\n",
    "# Expected background with rate 5mHz, kind of realistic\n",
    "nb = 0.005 * np.diff(dt) * 1e6  # Uncomment to see ns shrink\n",
    "src_ra = np.deg2rad([180])  # Arbitrarily placed single source\n",
    "src_dec = np.deg2rad([10])\n",
    "args = {\"nb\": nb, \"src_t\": src_t, \"dt\": dt,\n",
    "        \"src_ra\": src_ra, \"src_dec\": src_dec,\n",
    "        \"src_w_theo\": np.ones_like(src_dec)}\n",
    "\n",
    "# Set the events artificially where the srcs are in space and nicely spaced\n",
    "# times inside the search window, where time sob is large. Otherwise the llh\n",
    "# is almost always peaked at 0\n",
    "N = 10\n",
    "mint, maxt = src_t + dt / secinday  # In MJD\n",
    "timeMJD = np.linspace(mint, maxt, N)\n",
    "X = np.random.choice(_exp, size=N)  # Only to copy the recarray structure\n",
    "X[\"timeMJD\"] = timeMJD\n",
    "X[\"ra\"] = np.ones_like(timeMJD) * src_ra\n",
    "X[\"sinDec\"] = np.ones_like(timeMJD) * np.sin(src_dec)\n",
    "X[\"sigma\"] = np.deg2rad(np.ones_like(timeMJD))\n",
    "\n",
    "# Scan a single LLH for the chosen data above\n",
    "n_ns = 500\n",
    "xmin, xmax = 0, 2 * N\n",
    "ns = np.linspace(xmin, xmax, n_ns)\n",
    "lnllh = np.empty(n_ns)\n",
    "lnllh_grad = np.empty(n_ns)\n",
    "for i in range(n_ns):\n",
    "    theta = {\"ns\": ns[i]}\n",
    "    lnllh[i], lnllh_grad[i] = grbllh.lnllh_ratio(X, theta, args)\n",
    "\n",
    "# Manual \"fit\" by scanning the maximum\n",
    "ns_max = ns[np.argmax(lnllh)]\n",
    "\n",
    "plt.plot(ns, lnllh)\n",
    "plt.xlim(xmin, xmax)\n",
    "plt.ylim(0, 1.05 * np.amax(lnllh))\n",
    "plt.axvline(ns_max, 0, 1, ls=\"--\", lw=2, color=\"C7\")\n",
    "plt.title(\"nb = {:.2f}. ns max = {:.2f}\".format(*nb, ns_max))\n",
    "plt.show()\n",
    "\n",
    "plt.plot(ns, lnllh_grad)\n",
    "plt.axhline(0, 0, 1, ls=\"--\", lw=2, color=\"C7\")\n",
    "plt.axvline(ns_max, 0, 1, ls=\"--\", lw=2, color=\"C7\")\n",
    "plt.xlim(xmin, xmax)\n",
    "plt.ylim(-5, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This time we use multiple sources, but all at the exact same location and with the exact same properties.\n",
    "We expect the very same result as in the single source case above, because the weighted sum of the signal terms reduces to\n",
    "\n",
    "\\begin{align}\n",
    "    S^\\text{tot} &= \\sum_{j=1}^{N_\\text{srcs}} w_j S_{ij}\n",
    "                 = S_{i} \\sum_{j=1}^{N_\\text{srcs}} \\frac{1}{N}\n",
    "                 = S_i \\\\\n",
    "    \\Lambda &= -2\\ln\\left(\\frac{\\mathcal{L}_0}{\\mathcal{L}_1}\\right)\n",
    "             = -n_S + \\sum_{i=1}^N\\ln\\left(\\frac{n_S S^\\text{tot}}{\\langle n_B\\rangle B_i} + 1\\right)\n",
    "             = -n_S + \\sum_{i=1}^N\\ln\\left(\\frac{n_S S_i}{\\langle n_B\\rangle B_i} + 1\\right)\n",
    "\\end{align}\n",
    "\n",
    "as all signal terms are exaxtly the same and no further background locations are introduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Repeat sources exactly as the single one from above\n",
    "nsrcs = 5\n",
    "_src_t = np.repeat([src_t,], repeats=nsrcs, axis=0)\n",
    "_dt = np.repeat(dt.reshape(1, 2), axis=0, repeats=nsrcs)\n",
    "# Attention here: 100% overlapping windows so total BG is unchanged. To work\n",
    "# in the stacking framework, we just split the expectation equally\n",
    "_nb = 0.005 * np.diff(_dt) / nsrcs * 1e6  # Uncomment to see ns shrink\n",
    "\n",
    "_src_ra = np.repeat([src_ra,], repeats=nsrcs, axis=0)\n",
    "_src_dec = np.repeat([src_dec,], repeats=nsrcs, axis=0)\n",
    "\n",
    "_args = {\"nb\": _nb, \"src_t\": _src_t, \"dt\": _dt,\n",
    "         \"src_ra\": _src_ra, \"src_dec\": _src_dec,\n",
    "         \"src_w_theo\": np.ones_like(_src_dec)}\n",
    "\n",
    "# Also use the very same events for all sources here\n",
    "_X = np.copy(X)\n",
    "\n",
    "# Scan a single LLH for the chosen data above\n",
    "n_ns = 500\n",
    "xmin, xmax = 0, 2 * N\n",
    "ns = np.linspace(xmin, xmax, n_ns)\n",
    "_lnllh = np.empty(n_ns)\n",
    "_lnllh_grad = np.empty(n_ns)\n",
    "for i in range(n_ns):\n",
    "    theta = {\"ns\": ns[i]}\n",
    "    _lnllh[i], _lnllh_grad[i] = grbllh.lnllh_ratio(_X, theta, _args)\n",
    "\n",
    "# Manual \"fit\" by scanning the maximum\n",
    "_ns_max = ns[np.argmax(_lnllh)]\n",
    "\n",
    "plt.plot(ns, _lnllh)\n",
    "plt.xlim(xmin, xmax)\n",
    "plt.ylim(0, 1.05 * np.amax(_lnllh))\n",
    "plt.axvline(_ns_max, 0, 1, ls=\"--\", lw=2, color=\"C7\")\n",
    "plt.title(\"nb = {:.2f} per source. ns max = {:.2f}\".format(*_nb[0], _ns_max))\n",
    "plt.show()\n",
    "\n",
    "plt.plot(ns, _lnllh_grad)\n",
    "plt.axhline(0, 0, 1, ls=\"--\", lw=2, color=\"C7\")\n",
    "plt.axvline(_ns_max, 0, 1, ls=\"--\", lw=2, color=\"C7\")\n",
    "plt.xlim(xmin, xmax)\n",
    "plt.ylim(-5, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now the almost same thing, but with changed right ascensions only.\n",
    "We distribute them equally around a fixed declination.\n",
    "Also everything else is left as before.\n",
    "\n",
    "This case is a bit more tricky, but a combination of the two cases above makes sure we still fit N events.\n",
    "\n",
    "We inject the same number of events (N) but we get nsrcs times the BG (because the windows don't overlap anymore).\n",
    "Each event only contributes to the window where it spatially is placed, so per source only N / nsrcs events (we choosed them so the number distribute nicely) have a SoB > 0.\n",
    "\n",
    "This means, that the total signal term is reduced by a factor of nsrcs, as the zero signal terms can't compensate the unaffected backgound which is still the same for all events.\n",
    "\n",
    "So even though our stacked signal term is reduced by a factor of nsrcs  we still get the same fit result, because the signal term is still huge and we still satisfy the condition $\\frac{n_b B}{S}\\rightarrow 0$.\n",
    "\n",
    "But we need slightly less cranked up background rate to let the best fit ns shrink as in the previous cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Repeat sources exactly as the single one from above\n",
    "nsrcs = 5\n",
    "_src_t = np.repeat([src_t,], repeats=nsrcs, axis=0)\n",
    "_dt = np.repeat(dt.reshape(1, 2), axis=0, repeats=nsrcs)\n",
    "# Windows don't overlap anymore, so use full BG for each window\n",
    "_nb = 0.005 * np.diff(_dt) * 1e6  # Test this to see ns shrink\n",
    "\n",
    "# Handpick to let windows not overlap\n",
    "_src_ra = np.deg2rad([0, 30, 60, 90, 120]).reshape(nsrcs, 1)\n",
    "_src_dec = np.repeat([src_dec,], repeats=nsrcs, axis=0)\n",
    "\n",
    "_args = {\"nb\": _nb, \"src_t\": _src_t, \"dt\": _dt,\n",
    "         \"src_ra\": _src_ra, \"src_dec\": _src_dec,\n",
    "         \"src_w_theo\": np.ones_like(_src_dec)}\n",
    "\n",
    "# We used 5 srcs and 10 events, so we just repeat the ras once\n",
    "# This is not very obvious on how to scale to arbirary Ns and nsrcs\n",
    "# I'm not very sure here, how many events to inject to exactly match the cases\n",
    "# above.\n",
    "# Here we just have 2 evts per window and still have ns of 10, even though\n",
    "# signal should get donwweighted to 1/5 of the two cases above per source.\n",
    "_X = np.copy(X)\n",
    "_X[\"ra\"] = np.repeat(_src_ra, repeats=2)\n",
    "\n",
    "# Scan a single LLH for the chosen data above\n",
    "n_ns = 500\n",
    "xmin, xmax = 0, 2 * N\n",
    "ns = np.linspace(xmin, xmax, n_ns)\n",
    "_lnllh = np.empty(n_ns)\n",
    "_lnllh_grad = np.empty(n_ns)\n",
    "for i in range(n_ns):\n",
    "    theta = {\"ns\": ns[i]}\n",
    "    _lnllh[i], _lnllh_grad[i] = grbllh.lnllh_ratio(_X, theta, _args)\n",
    "\n",
    "# Manual \"fit\" by scanning the maximum\n",
    "_ns_max = ns[np.argmax(_lnllh)]\n",
    "\n",
    "plt.plot(ns, _lnllh)\n",
    "plt.xlim(xmin, xmax)\n",
    "plt.ylim(0, 1.05 * np.amax(_lnllh))\n",
    "plt.axvline(_ns_max, 0, 1, ls=\"--\", lw=2, color=\"C7\")\n",
    "plt.title(\"nb = {:.2f} per source. ns max = {:.2f}\".format(*_nb[0], _ns_max))\n",
    "plt.show()\n",
    "\n",
    "plt.plot(ns, _lnllh_grad)\n",
    "plt.axhline(0, 0, 1, ls=\"--\", lw=2, color=\"C7\")\n",
    "plt.axvline(_ns_max, 0, 1, ls=\"--\", lw=2, color=\"C7\")\n",
    "plt.xlim(xmin, xmax)\n",
    "plt.ylim(-5, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gamma = energy_pdf_args[\"gamma\"]\n",
    "weights = mc[\"trueE\"]**(-gamma) * mc[\"ow\"]\n",
    "_ = plt.hist(mc[\"sinDec\"], bins=energy_pdf_args[\"bins\"][0], weights=weights)\n",
    "\n",
    "plt.xlabel(\"southern sky <- horizon -> northern sky\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit LLH paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_llh(ns, lnllh, lnllh_grad, ns_max, xmin, xmax):\n",
    "    fig, (al, ar) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    al.plot(ns, lnllh)\n",
    "    if ns_max == 0:\n",
    "        al.set_xlim(-1, 1)\n",
    "    else:\n",
    "        al.set_xlim(xmin, 2 * ns_max)\n",
    "    al.set_ylim(0, 1.05 * np.amax(lnllh))\n",
    "    al.axvline(ns_max, 0, 1, ls=\"--\", lw=2, color=\"C7\")\n",
    "    al.set_title(\"LLH\")\n",
    "\n",
    "    ar.plot(ns, lnllh_grad)\n",
    "    ar.axhline(0, 0, 1, ls=\"--\", lw=2, color=\"C7\")\n",
    "    ar.axvline(ns_max, 0, 1, ls=\"--\", lw=2, color=\"C7\")\n",
    "    if ns_max == 0:\n",
    "        al.set_xlim(-1, 1)\n",
    "    else:\n",
    "        al.set_xlim(xmin, 2 * ns_max)\n",
    "    ar.set_ylim(-5, 5)\n",
    "    ar.set_title(\"LLH gradient in ns\")\n",
    "    fig.tight_layout()\n",
    "    return fig, (al, ar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we test if the module simply wrapps the LLH module correctly.\n",
    "This should reproduce same results (not regarding random fluctuations of course) as in the section ln-llh ratio, as we test the same setup as above here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First make a likelihood we want to test with\n",
    "sin_dec_bins = np.linspace(-1, 1, 50)\n",
    "\n",
    "min_logE = 1  #  min(np.amin(_exp[\"logE\"]), np.amin(mc[\"logE\"]))\n",
    "max_logE = 10 #  max(np.amax(_exp[\"logE\"]), np.amax(mc[\"logE\"]))\n",
    "logE_bins = np.linspace(min_logE, max_logE, 40)\n",
    "\n",
    "spatial_pdf_args = {\"bins\": sin_dec_bins, \"k\": 3, \"kent\": True}\n",
    "energy_pdf_args = {\"bins\": [sin_dec_bins, logE_bins],\n",
    "                   \"gamma\": 2., \"fillval\": \"col\", \"interpol_log\": False}\n",
    "time_pdf_args = {\"nsig\": 4.}\n",
    "\n",
    "grbllh = LLH.GRBLLH(X=_exp, MC=mc,\n",
    "                    spatial_pdf_args=spatial_pdf_args,\n",
    "                    energy_pdf_args=energy_pdf_args,\n",
    "                    time_pdf_args=time_pdf_args)\n",
    "\n",
    "# Now we make a single source record array\n",
    "dt = np.atleast_2d([[-20, 200],])\n",
    "n_srcs = len(dt)\n",
    "nb = 0.005 * np.diff(dt)  # Rate 5mHz, kind of realistic\n",
    "\n",
    "dtype = [(\"t\", np.float), (\"dt0\", np.float), (\"dt1\", np.float),\n",
    "         (\"ra\", np.float), (\"dec\", np.float)]\n",
    "srcs = np.zeros((n_srcs, ), dtype=dtype)\n",
    "srcs[\"t\"] = np.random.choice(_exp[\"timeMJD\"], size=n_srcs)\n",
    "srcs[\"dt0\"] = dt[:, 0]\n",
    "srcs[\"dt1\"] = dt[:, 1]\n",
    "srcs[\"ra\"] = np.random.uniform(0, 2 * np.pi, n_srcs)\n",
    "srcs[\"dec\"] = np.arcsin(np.random.uniform(-1, 1, n_srcs))\n",
    "\n",
    "# And manually put it in an arg list of dicts\n",
    "args = n_srcs * [{}]\n",
    "for i in range(n_srcs):\n",
    "    args[i][\"nb\"] = nb[i]\n",
    "    args[i][\"dt\"] = [srcs[i][\"dt0\"], srcs[i][\"dt1\"]]\n",
    "    args[i][\"src_t\"] = srcs[i][\"t\"]\n",
    "    args[i][\"src_ra\"] = srcs[i][\"ra\"]\n",
    "    args[i][\"src_dec\"] = srcs[i][\"dec\"]\n",
    "\n",
    "# Build the analysis module\n",
    "ana = Analysis.TransientsAnalysis(srcs=srcs, llh=grbllh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Place events directly where the srcs are, to have a chance to see the LLh\n",
    "# take actual maximum values different from 0\n",
    "N = 100\n",
    "mint, maxt = srcs[\"t\"] + dt[0] / secinday\n",
    "timeMJD = np.linspace(mint, maxt, N)\n",
    "X = np.random.choice(_exp, size=N)a\n",
    "X[\"timeMJD\"] = timeMJD\n",
    "X[\"ra\"] = np.ones_like(timeMJD) * srcs[\"ra\"]\n",
    "X[\"dec\"] = np.ones_like(timeMJD) * srcs[\"dec\"]\n",
    "\n",
    "# Manual \"fit\" by scanning the maximum\n",
    "n_ns = 1000\n",
    "xmin, xmax = 0, 20  # Scan range, chosen after trial and error\n",
    "ns = np.linspace(xmin, xmax, n_ns)\n",
    "lnllh = np.zeros(n_ns)\n",
    "lnllh_grad = np.zeros(n_ns)\n",
    "for i in range(n_ns):\n",
    "    theta = {\"ns\": ns[i]}\n",
    "    for j in range(n_srcs):\n",
    "        f, g = ana.llh.lnllh_ratio(X, theta, args[j])\n",
    "        lnllh[i] += f\n",
    "        lnllh_grad[i] += g\n",
    "\n",
    "ns_max = ns[np.argmax(lnllh)]\n",
    "plot_llh(ns, lnllh, lnllh_grad, ns_max, xmin, xmax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Also let's quickly see, how the events are distributed within the PDF\n",
    "dt = args[0][\"dt\"]\n",
    "x = np.linspace(X[\"timeMJD\"][0] + 6 * dt[0] / secinday,\n",
    "                X[\"timeMJD\"][-1] + 1 * dt[1] / secinday, 10 * N)\n",
    "y = ana.llh._soverb_time(t=x, src_t=srcs[\"t\"], dt=dt)\n",
    "plt.plot(x, y)\n",
    "plt.vlines(X[\"timeMJD\"], 0, np.amax(y), colors=\"C7\", linestyles=\"-\",\n",
    "           alpha=0.2, lw=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test if we can do the same as above, but usign a real fitter this time to get the maximum.\n",
    "The LLH and the maximum should be identical to the one above (except for small errors in scanning vs fitting ns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Seed for the fitter\n",
    "theta0 = {\"ns\": 1}\n",
    "# Args must only contain \"nb\", everything else is used from self.srcs\n",
    "args = []\n",
    "for i in range(n_srcs):\n",
    "    args.append({\"nb\": nb[i]})\n",
    "\n",
    "res = ana.fit_lnllh_ratio_params(X, theta0, args)\n",
    "\n",
    "plot_llh(ns, lnllh, lnllh_grad, res.x, xmin, xmax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Multiple sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We only have to add more sources and the code from aboove should work the same, with some broadcasting effort to setup the events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# First make a likelihood we want to test with\n",
    "sin_dec_bins = np.linspace(-1, 1, 50)\n",
    "\n",
    "min_logE = 1  #  min(np.amin(_exp[\"logE\"]), np.amin(mc[\"logE\"]))\n",
    "max_logE = 10 #  max(np.amax(_exp[\"logE\"]), np.amax(mc[\"logE\"]))\n",
    "logE_bins = np.linspace(min_logE, max_logE, 40)\n",
    "\n",
    "spatial_pdf_args = {\"bins\": sin_dec_bins, \"k\": 3, \"kent\": True}\n",
    "energy_pdf_args = {\"bins\": [sin_dec_bins, logE_bins],\n",
    "                   \"gamma\": 2., \"fillval\": \"col\", \"interpol_log\": False}\n",
    "time_pdf_args = {\"nsig\": 4.}\n",
    "\n",
    "grbllh = LLH.GRBLLH(X=_exp, MC=mc,\n",
    "                    spatial_pdf_args=spatial_pdf_args,\n",
    "                    energy_pdf_args=energy_pdf_args,\n",
    "                    time_pdf_args=time_pdf_args)\n",
    "\n",
    "# Now we make a single source record array\n",
    "dt = np.atleast_2d([[-20, 200], [-5, 20], [0, 300]])\n",
    "n_srcs = len(dt)\n",
    "nb = 0.005 * np.diff(dt)  # Rate 5mHz, kind of realistic\n",
    "\n",
    "dtype = [(\"t\", np.float), (\"dt0\", np.float), (\"dt1\", np.float),\n",
    "         (\"ra\", np.float), (\"dec\", np.float)]\n",
    "srcs = np.zeros((n_srcs, ), dtype=dtype)\n",
    "srcs[\"t\"] = np.random.choice(_exp[\"timeMJD\"], size=n_srcs)\n",
    "srcs[\"dt0\"] = dt[:, 0]\n",
    "srcs[\"dt1\"] = dt[:, 1]\n",
    "srcs[\"ra\"] = np.random.uniform(0, 2 * np.pi, n_srcs)\n",
    "srcs[\"dec\"] = np.arcsin(np.random.uniform(-1, 1, n_srcs))\n",
    "\n",
    "# And manually put it in an arg list of dicts\n",
    "args = []\n",
    "for i in range(n_srcs):\n",
    "    args.append({\"nb\": nb[i]})\n",
    "    args[i][\"nb\"] = nb[i]\n",
    "    args[i][\"dt\"] = [srcs[i][\"dt0\"], srcs[i][\"dt1\"]]\n",
    "    args[i][\"src_t\"] = srcs[i][\"t\"]\n",
    "    args[i][\"src_ra\"] = srcs[i][\"ra\"]\n",
    "    args[i][\"src_dec\"] = srcs[i][\"dec\"]\n",
    "\n",
    "# Build the analysis module\n",
    "ana = Analysis.TransientsAnalysis(srcs=srcs, llh=grbllh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Place events directly where the srcs are, to have a chance to see the LLh\n",
    "# take actual maximum values different from 0\n",
    "N = 100\n",
    "# Make time for each src window\n",
    "mint, maxt = srcs[\"t\"] + dt[:, 0] / secinday, srcs[\"t\"] + dt[:, 1] / secinday\n",
    "timeMJD = np.empty((n_srcs, N), dtype=np.float)\n",
    "for j in range(n_srcs):\n",
    "    timeMJD[j] = np.linspace(mint[j], maxt[j], N)\n",
    "X = np.random.choice(_exp, size=(n_srcs, N))\n",
    "X[\"timeMJD\"] = timeMJD\n",
    "X[\"ra\"] = np.ones(N) * srcs[\"ra\"].reshape(n_srcs, 1)\n",
    "X[\"dec\"] = np.ones(N) * srcs[\"dec\"].reshape(n_srcs, 1)\n",
    "X = X.flatten()\n",
    "\n",
    "# Manual \"fit\" by scanning the maximum\n",
    "n_ns = 1000\n",
    "xmin, xmax = 0, 200  # Scan range, chosen after trial and error\n",
    "ns = np.linspace(xmin, xmax, n_ns)\n",
    "lnllh = np.zeros(n_ns)\n",
    "lnllh_grad = np.zeros(n_ns)\n",
    "\n",
    "# Weight ns with BG expectation to get proper ratios in the fit\n",
    "weights = np.array([arg[\"nb\"] for arg in args])\n",
    "if np.sum(weights) > 0:\n",
    "    weights /= np.sum(weights)\n",
    "else:\n",
    "    weights = np.ones_like(weights) / n_src\n",
    "    \n",
    "lnllh = np.zeros(n_ns, dtype=np.float)\n",
    "lnllh_grad = np.zeros((n_ns, 1), dtype=np.float)\n",
    "for i in range(n_ns):\n",
    "    for j in range(n_srcs):\n",
    "        theta = {\"ns\": ns[i] * weights[j]}\n",
    "        f, g = ana.llh.lnllh_ratio(X, theta, args[j])\n",
    "        lnllh[i] += f\n",
    "        lnllh_grad[i] += weights[j] * g\n",
    "\n",
    "ns_max = ns[np.argmax(lnllh)]\n",
    "plot_llh(ns, lnllh, lnllh_grad, ns_max, xmin, xmax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Again check using the class function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Seed for the fitter\n",
    "theta0 = {\"ns\": 1}\n",
    "# Args must only contain \"nb\", everything else is used from self.srcs\n",
    "args = []\n",
    "for i in range(n_srcs):\n",
    "    args.append({\"nb\": nb[i]})\n",
    "    \n",
    "res = ana.fit_lnllh_ratio_params(X, theta0, args, bounds=None)\n",
    "    \n",
    "plot_llh(ns, lnllh, lnllh_grad, res.x, xmin, xmax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pulls all from above together.\n",
    "\n",
    "We need all injectors and the LLH.\n",
    "Then we inject events from those injectors per trial and fit the LLH for each configuration.\n",
    "We end up with a test statistic, modeling our null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BG only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a bg rat einjector model\n",
    "def filter_runs(run):\n",
    "    \"\"\"\n",
    "    Filter runs as stated in jfeintzig's doc.\n",
    "    \"\"\"\n",
    "    exclude_runs = [120028, 120029, 120030, 120087, 120156, 120157]\n",
    "    if ((run[\"good_i3\"] == True) & (run[\"good_it\"] == True) &\n",
    "        (run[\"run\"] not in exclude_runs)):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Let's create an injector using a goodrun list.\n",
    "runlist=\"data/runlists/ic86-i-goodrunlist.json\"\n",
    "rate_func = RateFunc.Sinus1yrRateFunction()\n",
    "runlist_inj = BGRateInj.RunlistBGRateInjector(runlist, filter_runs, rate_func)\n",
    "\n",
    "# `fit` the injector to make it usable\n",
    "times = exp[\"timeMJD\"]\n",
    "rate_func = runlist_inj.fit(T=times, x0=None, remove_zero_runs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create our bg injector, here we use the data resampler\n",
    "data_inj = BGInj.DataBGInjector()\n",
    "data_inj.fit(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Least we need a LLH, use the GRBLLH\n",
    "sin_dec_bins = np.linspace(-1, 1, 50)\n",
    "\n",
    "# Choose borders and bins by eye\n",
    "min_logE = 1 \n",
    "max_logE = 10\n",
    "logE_bins = np.linspace(min_logE, max_logE, 40)\n",
    "\n",
    "spatial_pdf_args = {\"bins\": sin_dec_bins, \"k\": 3, \"kent\": True}\n",
    "energy_pdf_args = {\"bins\": [sin_dec_bins, logE_bins],\n",
    "                   \"gamma\": 2., \"fillval\": \"col\", \"interpol_log\": False}\n",
    "time_pdf_args = {\"nsig\": 4.}\n",
    "\n",
    "grbllh = LLH.GRBLLH(X=_exp, MC=mc,\n",
    "                    spatial_pdf_args=spatial_pdf_args,\n",
    "                    energy_pdf_args=energy_pdf_args,\n",
    "                    time_pdf_args=time_pdf_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create some srcs to test for\n",
    "dt = np.atleast_2d([[-20, 200], [-5, 20], [0, 300]])\n",
    "n_srcs = len(dt)\n",
    "\n",
    "dtype = [(\"t\", np.float), (\"dt0\", np.float), (\"dt1\", np.float),\n",
    "         (\"ra\", np.float), (\"dec\", np.float)]\n",
    "srcs = np.zeros((n_srcs, ), dtype=dtype)\n",
    "srcs[\"t\"] = np.random.choice(_exp[\"timeMJD\"], size=n_srcs)\n",
    "srcs[\"dt0\"] = dt[:, 0]\n",
    "srcs[\"dt1\"] = dt[:, 1]\n",
    "srcs[\"ra\"] = np.random.uniform(0, 2 * np.pi, n_srcs)\n",
    "srcs[\"dec\"] = np.arcsin(np.random.uniform(-1, 1, n_srcs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# And now the analysis object\n",
    "ana = Analysis.TransientsAnalysis(srcs=srcs, llh=grbllh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "self = ana\n",
    "\n",
    "_X = []\n",
    "times = []\n",
    "rnd_ra = []\n",
    "args = []\n",
    "\n",
    "for src_idx in range(len(srcs)):\n",
    "    # Samples times and thus number of bg expectated events\n",
    "    t = self.srcs[\"t\"][src_idx]\n",
    "    dt = [self.srcs[\"dt0\"][src_idx], self.srcs[\"dt1\"][src_idx]]\n",
    "    _times = runlist_inj.sample(t=t, trange=dt, ntrials=1)[0]\n",
    "    nb = len(_times)\n",
    "    times.append(_times)\n",
    "    args.append({\"nb\": nb})\n",
    "    # Sample rest of features\n",
    "    if nb > 0:\n",
    "        _X.append(data_inj.sample(n_samples=nb))\n",
    "        rnd_ra.append(np.random.uniform(0, 2. * np.pi, size=nb))\n",
    "\n",
    "names = [\"ra\", \"sinDec\", \"timeMJD\", \"logE\", \"sigma\"]\n",
    "dtype = [(n, t) for (n, t) in zip(names, len(names) * [np.float])]\n",
    "nb_tot = np.sum([d[\"nb\"] for d in args])\n",
    "X = np.empty((nb_tot, ), dtype=dtype)\n",
    "# Make output array in compatible format\n",
    "_X = flatten_list_of_1darrays(_X)\n",
    "X[\"ra\"] = flatten_list_of_1darrays(rnd_ra)\n",
    "X[\"sinDec\"] = np.sin(_X[:, 1])\n",
    "X[\"logE\"] = _X[:, 0]\n",
    "X[\"sigma\"] = _X[:, 2]\n",
    "X[\"timeMJD\"] = flatten_list_of_1darrays(times)\n",
    "\n",
    "nb_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ana.fit_lnllh_ratio_params(X, theta0={\"ns\": 1}, args=args)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

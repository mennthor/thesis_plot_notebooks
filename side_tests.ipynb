{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some more side tests to clarify / justify details, that would clutter the main test notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import helper as hlp\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mpldates\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.colors import LogNorm\n",
    "%matplotlib inline\n",
    "\n",
    "import scipy.interpolate as sci\n",
    "import scipy.optimize as sco\n",
    "\n",
    "import json\n",
    "import datetime\n",
    "import pickle\n",
    "from astropy.time import Time as astrotime\n",
    "from corner import corner\n",
    "\n",
    "import sklearn.neighbors as skn\n",
    "import sklearn.model_selection as skms  # Newer version of grid_search\n",
    "\n",
    "from corner_hist import corner_hist\n",
    "from anapymods3.plots.general import split_axis, get_binmids, hist_marginalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "\n",
    "Load IC86 data from epinat, which should be the usual IC86-I (2011) PS sample, but pull corrected and OneWeights corrected by number of events generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exp, mc, livetime = hlp.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data livetime comparison to v1.4\n",
    "\n",
    "Let's compare to the v1.4 list, as used by jfeintzig.\n",
    "Oddly we have 0.2 days less livetime as he had.\n",
    "The number of runs is correct though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# New livetime from iclive\n",
    "run_list = hlp.get_run_list()\n",
    "run_dict = hlp.get_run_dict(run_list)\n",
    "inc_run_arr, ic_livetime = hlp.get_good_runs(run_dict)\n",
    "\n",
    "print(\"Total runs from iclive     : \", len(inc_run_arr))\n",
    "print(\"IC86-I livetime from iclive: \", ic_livetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For comparison, also parse the v1.4 list\n",
    "# Should be: 1081 runs, with a total livetime of 332.61 days.\n",
    "with open(\"data/Prelim_IC86-I_v1.4a.txt\",'r') as f:\n",
    "    data = []\n",
    "    for line in f.readlines():\n",
    "        data.append(line.replace('\\n',''))\n",
    "        \n",
    "# Skip to beginning of run info\n",
    "data = data[73:]\n",
    "\n",
    "# Split at white space\n",
    "data = [d.split() for d in data]\n",
    "\n",
    "dtype = [(\"runID\", np.int), (\"duration\", np.float), (\"IT\", \"|S2\"),\n",
    "         (\"CONF\", \"|S7\"), (\"FLAG\", \"|S6\")]\n",
    "runlist = np.empty((len(data),), dtype=dtype)\n",
    "\n",
    "runlist[\"runID\"] = np.array([int(d[0]) for d in data])\n",
    "runlist[\"duration\"] = np.array([float(d[3]) for d in data])\n",
    "runlist[\"IT\"] = np.array([d[5] for d in data])\n",
    "runlist[\"CONF\"] = np.array([d[6] for d in data])\n",
    "runlist[\"FLAG\"] = np.array([d[7] for d in data])\n",
    "\n",
    "# Now filter: Include IT=it, CONF=full, FLAG=GOOD, exclude strange rate runs\n",
    "exclude_rate = [120028, 120029, 120030, 120087, 120156, 120157]\n",
    "itgood = runlist[\"IT\"] == b\"IT\"  # Somehow only bitwise comparison is non-empty\n",
    "confgood = runlist[\"CONF\"] == b\"full\"\n",
    "flaggood = runlist[\"FLAG\"] == b\"GOOD\"\n",
    "ratebad = np.in1d(runlist[\"runID\"], exclude_rate)\n",
    "\n",
    "include = itgood & confgood & flaggood & ~ratebad\n",
    "runlist_inc = runlist[include]\n",
    "\n",
    "# Get the livetime of the sample in days\n",
    "hoursindays = 24.\n",
    "old_livetime = np.sum(runlist_inc[\"duration\"]) / hoursindays\n",
    "\n",
    "print(\"Total runs from v1.4     : \", len(runlist_inc))\n",
    "print(\"Total livetime from v1.4 : \", old_livetime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see, if the 120 extra runs in the new runlist make up for the difference of about 10 days in livetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iclive_in_old = np.in1d(inc_run_arr[\"runID\"], runlist_inc[\"runID\"])\n",
    "not_in_old = inc_run_arr[~iclive_in_old]\n",
    "\n",
    "start = not_in_old[\"start_mjd\"]\n",
    "stop = not_in_old[\"stop_mjd\"]\n",
    "missing_livetime = np.sum(stop - start)\n",
    "\n",
    "print(\"\\nOfficial IC86-I PS livetime: \", livetime)\n",
    "print(\"Total livetime from v1.4   : \", old_livetime)\n",
    "print(\"IC86-I livetime from iclive: \", ic_livetime)\n",
    "\n",
    "print(\"\\nMissing runs in old: \", len(not_in_old))\n",
    "print(\"Livetime icliv - old :\", ic_livetime - old_livetime)\n",
    "\n",
    "print(\"\\nDiff from summing missing runs           : \", missing_livetime)\n",
    "print(\"New iclive livetime with same runs as old: \", ic_livetime - missing_livetime)\n",
    "\n",
    "print(\"\\nTotal rate over total livetime: \n",
    "      \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All runs from the new run list that zero events, make up for the missing runs in the old runlist, so this is consisting.\n",
    "\n",
    "Dont't know though, where the missing 0,2 days come from. Probably some runtimes have shifted a little making some extra livetime in the new list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Store events in bins with run borders\n",
    "exp_times = exp[\"timeMJD\"]\n",
    "start_mjd = inc_run_arr[\"start_mjd\"]\n",
    "stop_mjd = inc_run_arr[\"stop_mjd\"]\n",
    "\n",
    "tot = 0\n",
    "evts_in_run = {}\n",
    "for start, stop , runid in zip(start_mjd, stop_mjd, inc_run_arr[\"runID\"]):\n",
    "    mask = (exp_times >= start) & (exp_times < stop)\n",
    "    evts_in_run[runid] = exp[mask]\n",
    "    tot += np.sum(mask)\n",
    "    \n",
    "# Crosscheck, if we got all events and counted nothing double\n",
    "print(\"Do we have all events? \", tot == len(exp))\n",
    "print(\"  Events selected : \", tot)\n",
    "print(\"  Events in exp   : \", len(exp))\n",
    "\n",
    "# Create binmids and histogram values in each bin\n",
    "binmids = 0.5 * (start_mjd + stop_mjd)\n",
    "h = np.zeros(len(binmids), dtype=np.float)\n",
    "\n",
    "for i, evts in enumerate(evts_in_run.values()):\n",
    "    h[i] = len(evts)\n",
    "    \n",
    "m = (h == 0)\n",
    "print(\"Runs with 0 events :\", np.sum(m))\n",
    "print(\"Runtime in those runs: \", np.sum(inc_run_arr[\"stop_mjd\"][m] -\n",
    "                                        inc_run_arr[\"start_mjd\"][m]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = plt.hist2d(mjd, exp[\"dec\"], bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m = (h > 0)\n",
    "runtimes = inc_run_arr[\"stop_mjd\"][m] - inc_run_arr[\"start_mjd\"][m]\n",
    "_ = plt.plot(binmids[m], h[m] / runtimes / secinday, \"bo\")\n",
    "plt.xlabel(\"MJD\")\n",
    "plt.ylabel(\"Rate in Hz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time - Dec expectation\n",
    "\n",
    "Instead of just using the background rate dependent on the events time, we can make it \n",
    "dependent on the events position too.\n",
    "\n",
    "1. Technique is to use a spline fit to a histogram.\n",
    "   Robust and easy to average out the unwanted small scale fluctuations, but bin depent.\n",
    "2. Technique is to use 2D KDE\n",
    "   Use the same KDE technique to describe our background rate in 2 dimensions as we do with the per event PDF in 3D. Needs more work at the edges due to the hard cut in time. Again depends on bandwidth, smoothing is more difficult ot control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "time = exp[\"timeMJD\"]\n",
    "dec = exp[\"dec\"]\n",
    "\n",
    "# Normalize time to match scale \n",
    "time_norm = (time - np.amin(time)) / (np.amax(time) - np.amin(time))\n",
    "\n",
    "sample = np.vstack((time_norm, dec)).T\n",
    "_ = corner(sample, bins=[20, 50], plot_datapoints=False, plot_contours=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem with the KDE ist, that time is hard cut bounded at both sides.\n",
    "One way around this would be to mirror the times at the edges to ensure continuity and then cut of values outside the range when evaluating the PDF to get the background rate.\n",
    "This is OK, because time is somewhat periodic.\n",
    "\n",
    "Declination falls of to the sides, so theres no need to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kde = skn.KernelDensity(bandwidth=0.05, rtol=1e-8)\n",
    "kde.fit(sample)\n",
    "\n",
    "kde_sample = kde.sample(int(1e7))\n",
    "_ = corner(kde_sample, bins=[50, 50], plot_datapoints=False, plot_contours=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to properly calculate the rate per bin, when not the run bins are used?\n",
    "Assign each event a weight wich is rate per second per event.\n",
    "The weights are given binwise for each run.\n",
    "Summing over a larger bin gives a reduced total rate which is as expected when there are some missing runs in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exp_times = exp[\"timeMJD\"]\n",
    "start_mjd = inc_run_arr[\"start_mjd\"]\n",
    "stop_mjd = inc_run_arr[\"stop_mjd\"]\n",
    "\n",
    "weights = np.zeros(len(exp), dtype=np.float)\n",
    "for start, stop in zip(start_mjd, stop_mjd):\n",
    "    mask = (exp_times >= start) & (exp_times < stop)\n",
    "    secinday = 24. * 60. * 60.\n",
    "    run_livetime = (stop - start) * secinday\n",
    "    weights[mask] = 1 / (np.sum(mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mjd = exp[\"timeMJD\"]\n",
    "h, b = np.histogram(mjd, bins=100)\n",
    "m = get_binmids([b])[0]\n",
    "\n",
    "hn = h / (np.diff(b) * len(exp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(exp) / (365 * secinday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h, b = np.histogram(mjd, bins=100)\n",
    "\n",
    "h = h / (np.diff(b) * np.sum(h))\n",
    "m = get_binmids([b])[0]\n",
    "\n",
    "_ = plt.plot(m, h, \"bo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = plt.plot(m, hn, \"bo\")\n",
    "plt.ylim(0, 0.004)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's make the BG pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Justify the sigma cut\n",
    "\n",
    "Only few higher energy events from the sothern sky are excluded (see cut=10).\n",
    "But really bad reconstructed events tend to have higher energies (see cut=90).\n",
    "Still it should be OK to remove those > 10 because they have not so much spatial information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Show the leftover event s after a sigma cut\n",
    "sig_cut = 10\n",
    "m = exp[\"sigma\"] > np.deg2rad(sig_cut)\n",
    "\n",
    "_ = plt.hist2d(exp[\"logE\"][m], np.rad2deg(exp[\"dec\"][m]),\n",
    "               bins=30, cmap=\"inferno\")\n",
    "plt.colorbar()\n",
    "plt.title(\"Total Evts w sigma > {:d}°: {:d} ({:.3f}%)\".format(\n",
    "        sig_cut, np.sum(m), np.sum(m) / len(exp) * 100))\n",
    "plt.xlabel(\"logE\")\n",
    "plt.ylabel(\"dec in °\")\n",
    "plt.show()\n",
    "\n",
    "# Show the skewed sigma distribution with the cut applied and mean vs median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the marginalize_hist method.\n",
    "\n",
    "It should be equivalent to use one of the following methods to create a 1D histogram from the original 3D data pdf in logE, dec and sigma:\n",
    "\n",
    "1. Simply use the original 1D data in any variable, e.g. simply histogram logE\n",
    "2. Create the complete 3D histogram and marginalize by summing over remaining dimensions.\n",
    "\n",
    "When using unnormalized hists, 2. is simply summing up all other counts.\n",
    "\n",
    "When using normalized hists, we need to sum with respect to the binwidths in the current dimension to keep the normalization intact.\n",
    "This is only useful, when only the histogram is available and not the original sample.\n",
    "\n",
    "We want to compare if both methods are equivalent\n",
    "As we can see, all ratios are one, so methods are equal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_hist_ratio(h1, h2):\n",
    "    \"\"\"Return the ratio h1 / h2. Return 0 where h2 is 0.\"\"\"\n",
    "    m = (h2 > 0)\n",
    "    ratio = np.zeros_like(h1)\n",
    "    ratio[m] = h1[m] / h2[m]\n",
    "    return ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unnormalized\n",
    "First the unnormalized version. Simply sum over the other axes of the 3D hist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot each variable in a single plot and the ratios seperately\n",
    "fig, [[axtl, axtr], [axbl, axbr]] = plt.subplots(2, 2, figsize=(10, 8))\n",
    "\n",
    "# We also make a cut < 10° in sigma, because there are some outliers\n",
    "m = exp[\"sigma\"] <= np.deg2rad(10)\n",
    "sigma = np.rad2deg(exp[\"sigma\"][m])\n",
    "logE = exp[\"logE\"][m]\n",
    "dec = np.sin(exp[\"dec\"][m])\n",
    "\n",
    "logE_nbins = 50\n",
    "dec_nbins = 40\n",
    "sigma_nbins = 30\n",
    "\n",
    "# Make the 3D hist\n",
    "sample = np.vstack((logE, dec, sigma)).T\n",
    "nbins = [logE_nbins, dec_nbins, sigma_nbins]\n",
    "h, b = np.histogramdd(sample, bins=nbins,)\n",
    "\n",
    "# Get binmids for plotting\n",
    "m = get_binmids(b)\n",
    "\n",
    "# Common hist settings\n",
    "h1 = {\"lw\": 2, \"color\": \"k\", \"histtype\": \"step\"}\n",
    "h2 = {\"lw\": 2, \"color\": \"r\", \"histtype\": \"step\", \"alpha\": 0.5}\n",
    "\n",
    "# logE\n",
    "logE_h, logE_b, _ = axtl.hist(logE, bins=logE_nbins, **h1)\n",
    "logE_hm = np.sum(h, axis=(1, 2))\n",
    "_ = axtl.hist(m[0], bins=b[0], weights=logE_hm, **h2)\n",
    "# Ratio plot below\n",
    "axtl_sec = split_axis(axtl, \"bottom\", \"20%\", cbar=False)\n",
    "axtl_sec.hist(m[0], b[0], weights=make_hist_ratio(logE_h, logE_hm), **h2)\n",
    "axtl_sec.axhline(1, 0, 1, color=\"k\")\n",
    "axtl_sec.set_ylim(0, 2)\n",
    "\n",
    "# dec\n",
    "dec_h, dec_b, _ = axbl.hist(dec, bins=dec_nbins, **h1)\n",
    "dec_hm = np.sum(h, axis=(0, 2))\n",
    "_ = axbl.hist(m[1], bins=b[1], weights=dec_hm, **h2)\n",
    "\n",
    "axbl_sec = split_axis(axbl, \"bottom\", \"20%\", cbar=None)\n",
    "axbl_sec.hist(m[1], b[1], weights=make_hist_ratio(dec_h, dec_hm), **h2)\n",
    "axbl_sec.axhline(1, 0, 1, color=\"k\")\n",
    "axbl_sec.set_ylim(0, 2)\n",
    "\n",
    "# sigma\n",
    "sigma_h, sigma_b, _ = axtr.hist(sigma, bins=sigma_nbins, **h1)\n",
    "sigma_hm = np.sum(h, axis=(0, 1))\n",
    "_ = axtr.hist(m[2], bins=b[2], weights=sigma_hm, **h2)\n",
    "\n",
    "axtr_sec = split_axis(axtr, \"bottom\", \"20%\", cbar=None)\n",
    "axtr_sec.hist(m[2], b[2], weights=make_hist_ratio(sigma_h, sigma_hm), **h2)\n",
    "axtr_sec.axhline(1, 0, 1, color=\"k\")\n",
    "axtr_sec.set_ylim(0, 2)\n",
    "\n",
    "axbr.set_visible(False)\n",
    "\n",
    "fig.suptitle(\"Black: 1D, Red: Margin\", fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalized\n",
    "Sum over the other axes of the 3D hist and multiply by bin widths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot each variable in a single plot and the ratios seperately\n",
    "fig, [[axtl, axtr], [axbl, axbr]] = plt.subplots(2, 2, figsize=(10, 8))\n",
    "\n",
    "# Now make it normed\n",
    "h, b = np.histogramdd(sample, bins=nbins, normed=True)\n",
    "\n",
    "# Get binmids for plotting\n",
    "m = get_binmids(b)\n",
    "\n",
    "# logE\n",
    "logE_h, logE_b, _ = axtl.hist(logE, bins=logE_nbins, normed=True, **h1)\n",
    "logE_hm = hist_marginalize(h=h, bins=b, axes=(1, 2))[0]\n",
    "_ = axtl.hist(m[0], bins=b[0], weights=logE_hm, **h2)\n",
    "# Ratio plot below\n",
    "axtl_sec = split_axis(axtl, \"bottom\", \"20%\", cbar=False)\n",
    "axtl_sec.hist(m[0], b[0], weights=make_hist_ratio(logE_h, logE_hm), **h2)\n",
    "axtl_sec.axhline(1, 0, 1, color=\"k\")\n",
    "axtl_sec.set_ylim(0, 2)\n",
    "\n",
    "# dec\n",
    "dec_h, dec_b, _ = axbl.hist(dec, bins=dec_nbins, normed=True, **h1)\n",
    "dec_hm = hist_marginalize(h=h, bins=b, axes=(0, 2))[0]\n",
    "_ = axbl.hist(m[1], bins=b[1], weights=dec_hm, **h2)\n",
    "\n",
    "axbl_sec = split_axis(axbl, \"bottom\", \"20%\", cbar=None)\n",
    "axbl_sec.hist(m[1], b[1], weights=make_hist_ratio(dec_h, dec_hm), **h2)\n",
    "axbl_sec.axhline(1, 0, 1, color=\"k\")\n",
    "axbl_sec.set_ylim(0, 2)\n",
    "\n",
    "# sigma\n",
    "sigma_h, sigma_b, _ = axtr.hist(sigma, bins=sigma_nbins, normed=True, **h1)\n",
    "sigma_hm = hist_marginalize(h=h, bins=b, axes=(0, 1))[0]\n",
    "_ = axtr.hist(m[2], bins=b[2], weights=sigma_hm, **h2)\n",
    "\n",
    "axtr_sec = split_axis(axtr, \"bottom\", \"20%\", cbar=None)\n",
    "axtr_sec.hist(m[2], b[2], weights=make_hist_ratio(sigma_h, sigma_hm), **h2)\n",
    "axtr_sec.axhline(1, 0, 1, color=\"k\")\n",
    "axtr_sec.set_ylim(0, 2)\n",
    "\n",
    "axbr.set_visible(False)\n",
    "\n",
    "fig.suptitle(\"Black: 1D, Red: Margin\", fontsize=15);"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

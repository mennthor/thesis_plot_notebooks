{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some more side tests to clarify / justify details, that would clutter the main test notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "import helper as hlp\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mpldates\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.colors import LogNorm\n",
    "%matplotlib inline\n",
    "\n",
    "import scipy.interpolate as sci\n",
    "import scipy.optimize as sco\n",
    "import scipy.integrate as scint\n",
    "import scipy.stats as scs\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import json\n",
    "import datetime\n",
    "import pickle\n",
    "from astropy.time import Time as astrotime\n",
    "from corner import corner\n",
    "\n",
    "import sklearn.neighbors as skn\n",
    "import sklearn.model_selection as skms  # Newer version of grid_search\n",
    "\n",
    "from corner_hist import corner_hist\n",
    "from anapymods3.plots.general import split_axis, get_binmids, hist_marginalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Load IC86 data from epinat, which should be the usual IC86-I (2011) PS sample, but pull corrected and OneWeights corrected by number of events generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "exp, mc, livetime = hlp.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Data livetime comparison to v1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's compare to the v1.4 list, as used by jfeintzig.\n",
    "Oddly we have 0.2 days less livetime as he had.\n",
    "The number of runs is correct though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# New livetime from iclive\n",
    "run_list = hlp.get_run_list()\n",
    "run_dict = hlp.get_run_dict(run_list)\n",
    "inc_run_arr, ic_livetime = hlp.get_good_runs(run_dict)\n",
    "\n",
    "print(\"Total runs from iclive     : \", len(inc_run_arr))\n",
    "print(\"IC86-I livetime from iclive: \", ic_livetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# For comparison, also parse the v1.4 list\n",
    "# Should be: 1081 runs, with a total livetime of 332.61 days.\n",
    "with open(\"data/Prelim_IC86-I_v1.4a.txt\",'r') as f:\n",
    "    data = []\n",
    "    for line in f.readlines():\n",
    "        data.append(line.replace('\\n',''))\n",
    "        \n",
    "# Skip to beginning of run info\n",
    "data = data[73:]\n",
    "\n",
    "# Split at white space\n",
    "data = [d.split() for d in data]\n",
    "\n",
    "dtype = [(\"runID\", np.int), (\"duration\", np.float), (\"IT\", \"|S2\"),\n",
    "         (\"CONF\", \"|S7\"), (\"FLAG\", \"|S6\")]\n",
    "runlist = np.empty((len(data),), dtype=dtype)\n",
    "\n",
    "runlist[\"runID\"] = np.array([int(d[0]) for d in data])\n",
    "runlist[\"duration\"] = np.array([float(d[3]) for d in data])\n",
    "runlist[\"IT\"] = np.array([d[5] for d in data])\n",
    "runlist[\"CONF\"] = np.array([d[6] for d in data])\n",
    "runlist[\"FLAG\"] = np.array([d[7] for d in data])\n",
    "\n",
    "# Now filter: Include IT=it, CONF=full, FLAG=GOOD, exclude strange rate runs\n",
    "exclude_rate = [120028, 120029, 120030, 120087, 120156, 120157]\n",
    "itgood = runlist[\"IT\"] == b\"IT\"  # Somehow only bitwise comparison is non-empty\n",
    "confgood = runlist[\"CONF\"] == b\"full\"\n",
    "flaggood = runlist[\"FLAG\"] == b\"GOOD\"\n",
    "ratebad = np.in1d(runlist[\"runID\"], exclude_rate)\n",
    "\n",
    "include = itgood & confgood & flaggood & ~ratebad\n",
    "runlist_inc = runlist[include]\n",
    "\n",
    "# Get the livetime of the sample in days\n",
    "hoursindays = 24.\n",
    "secinday = hoursindays * 60. * 60.\n",
    "old_livetime = np.sum(runlist_inc[\"duration\"]) / hoursindays\n",
    "\n",
    "print(\"Total runs from v1.4     : \", len(runlist_inc))\n",
    "print(\"Total livetime from v1.4 : \", old_livetime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's see, if the 120 extra runs in the new runlist make up for the difference of about 10 days in livetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "iclive_in_old = np.in1d(inc_run_arr[\"runID\"], runlist_inc[\"runID\"])\n",
    "not_in_old = inc_run_arr[~iclive_in_old]\n",
    "\n",
    "start = not_in_old[\"start_mjd\"]\n",
    "stop = not_in_old[\"stop_mjd\"]\n",
    "missing_livetime = np.sum(stop - start)\n",
    "\n",
    "print(\"\\nOfficial IC86-I PS livetime: \", livetime)\n",
    "print(\"Total livetime from v1.4   : \", old_livetime)\n",
    "print(\"IC86-I livetime from iclive: \", ic_livetime)\n",
    "\n",
    "print(\"\\nMissing runs in old: \", len(not_in_old))\n",
    "print(\"Livetime icliv - old :\", ic_livetime - old_livetime)\n",
    "\n",
    "print(\"\\nDiff from summing missing runs           : \", missing_livetime)\n",
    "print(\"New iclive livetime with same runs as old: \",\n",
    "      ic_livetime - missing_livetime)\n",
    "\n",
    "print(\"\\nTotal rate [Hz] over total livetime: \",\n",
    "      len(exp) / (livetime * secinday))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "All runs from the new run list that zero events, make up for the missing runs in the old runlist, so this is consisting.\n",
    "\n",
    "Dont't know though, where the missing 0,2 days come from. Probably some runtimes have shifted a little making some extra livetime in the new list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Store events in bins with run borders\n",
    "exp_times = exp[\"timeMJD\"]\n",
    "start_mjd = inc_run_arr[\"start_mjd\"]\n",
    "stop_mjd = inc_run_arr[\"stop_mjd\"]\n",
    "\n",
    "tot = 0\n",
    "evts_in_run = {}\n",
    "for start, stop , runid in zip(start_mjd, stop_mjd, inc_run_arr[\"runID\"]):\n",
    "    mask = (exp_times >= start) & (exp_times < stop)\n",
    "    evts_in_run[runid] = exp[mask]\n",
    "    tot += np.sum(mask)\n",
    "    \n",
    "# Crosscheck, if we got all events and counted nothing double\n",
    "print(\"Do we have all events? \", tot == len(exp))\n",
    "print(\"  Events selected : \", tot)\n",
    "print(\"  Events in exp   : \", len(exp))\n",
    "\n",
    "# Create binmids and histogram values in each bin\n",
    "binmids = 0.5 * (start_mjd + stop_mjd)\n",
    "h = np.zeros(len(binmids), dtype=np.float)\n",
    "\n",
    "for i, evts in enumerate(evts_in_run.values()):\n",
    "    h[i] = len(evts)\n",
    "    \n",
    "m = (h > 0)\n",
    "print(\"Runs with 0 events :\", np.sum(~m))\n",
    "print(\"Runtime in those runs: \", np.sum(inc_run_arr[\"stop_mjd\"][~m] -\n",
    "                                        inc_run_arr[\"start_mjd\"][~m]))\n",
    "\n",
    "# Remove all zero event runs (artifacts from new run list) and calc the rate\n",
    "stop_mjd, start_mjd = stop_mjd[m], start_mjd[m]\n",
    "h = h[m] / ((stop_mjd - start_mjd) * secinday)\n",
    "binmids = binmids[m]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Time dependent rate function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Note: I think it is unnecessary to use a time and declination dependent rate. The spatial part is injected from the data BG from KDE anyways. So we just need to have the rate to determine how much events we inject allsky.**\n",
    "\n",
    "Rate ist time dependent because of seasonal variation.\n",
    "We take this varariation into account by fitting a priodic function to the time resolved rate.\n",
    "\n",
    "The data is built by calculating the rate in each run as seen before.\n",
    "This rate is correctly normalized and smoothes local fluctuations.\n",
    "\n",
    "### Peridoc function with a weighted least squares fit\n",
    "\n",
    "See side_test for comparison to spline fits.\n",
    "The function is a simple sinus scalable by 4 parameters to fit the shape of the rates:\n",
    "\n",
    "$$\n",
    "    f(x) = a\\cdot \\sin(b\\cdot(x - c)) + d\n",
    "$$\n",
    "\n",
    "The least squares loss function is\n",
    "\n",
    "$$\n",
    "    R = \\sum_i (w_i(y_i - f(x_i)))^2\n",
    "$$\n",
    "\n",
    "Weights are standard deviations from poisson histogram error.\n",
    "\n",
    "$$\n",
    "    w_i = \\frac{1}{\\sigma_i}\n",
    "$$\n",
    "\n",
    "Seed values are estimated from plot rate vs time.\n",
    "\n",
    "- Period should be 365 days (MJD) because we have one year of data so we choose $b0 = 2\\pi/365$.\n",
    "- Amplitude is about $a_0=-0.0005$, because sinus seems to start with negative values.\n",
    "- The x-offset is choose as the first start date, to get the right order of magnitude.\n",
    "- The y-axis intersection $d$ schould be close to the weighted average, so we take this as a seed.\n",
    "\n",
    "The bounds are motivated as follows (and if we don't hit them, it's OK to use them).\n",
    "\n",
    "- Amplitude $a$ should be positive, this also resolves a degenracy between a-axis offset.\n",
    "- The period $b$ should scatter around one year, a period larger than +-1 half a year is unphysical.\n",
    "- The x-offset $c$ cannot be greater than the initial +- the period because we have a periodic function.\n",
    "- The y-axis offset $d$ is arbitrarily constrained, but as seen from the plot it should not exceed 0.1. \n",
    "\n",
    "## Proposed was something like this\n",
    "\n",
    "Rate ist time dependent because of seasonal variation and delination dependent because the detector acceptance is declination dependent.\n",
    "A correletation should not exist or be very small.\n",
    "\n",
    "So we express the rate in depence of time and decliantion as\n",
    "\n",
    "$$\n",
    "    R(t,\\delta) = R_T(t)\\cdot R_\\delta(t)\n",
    "$$\n",
    "\n",
    "with independent parts in declination and time each.\n",
    "\n",
    "The time parts is constructed by fitting a periodic function.\n",
    "Then we seperatly fit a spline to the total $\\sin(\\delta)$ distribtuion and normalizes it over the declination range of the sample.\n",
    "For a given time and declination the smooth function $R(t, \\delta)$ gives the correct detector rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Splinefit to sinDec\n",
    "\n",
    "First we fit a spline to the sinDec distribtuion for all events.\n",
    "This is equivalent to what is done in skylab to estimate the per signal background PDF from data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# This is equivalent to skylab's baclground PDF construction\n",
    "sinDec_bins = 25\n",
    "sinDec_range= [-1, 1]\n",
    "sinDec_hist, sinDec_bins = np.histogram(exp[\"sinDec\"], density=True,\n",
    "                                        bins=sinDec_bins, range=sinDec_range)\n",
    "\n",
    "m = get_binmids([sinDec_bins])[0]\n",
    "\n",
    "if np.any(sinDec_hist <= 0.):\n",
    "    raise ValueError((\"Declination hist bins empty, this must not happen. \"\n",
    "                      +\"Empty bin idx: {}\".format(\n",
    "                          np.arange(len(m))[sinDec_hist <= 0.])))\n",
    "\n",
    "# Fit to logarithm, to avoid ringing. Raise err if evaluated outside range\n",
    "sinDec_spline = sci.InterpolatedUnivariateSpline(m, np.log(sinDec_hist),\n",
    "                                                 k=3, ext=\"extrapolate\")\n",
    "\n",
    "# Normalize to area on whole sky = 1, so norm = 2pi * integral(exp(spl))\n",
    "def sinDec_pdf_(x):\n",
    "    return np.exp(sinDec_spline(x))\n",
    "\n",
    "norm = scint.quad(sinDec_pdf_, -1, 1)[0] * 2. * np.pi\n",
    "\n",
    "def sinDec_pdf(x):\n",
    "    return (np.exp(sinDec_spline(x))) / norm\n",
    "\n",
    "print(\"SinDec pdf has area on 4pi = \", scint.quad(\n",
    "    sinDec_pdf, -1, 1)[0] * 2 * np.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x = np.linspace(sinDec_bins[0], sinDec_bins[-1], 100)\n",
    "y = sinDec_pdf(x)\n",
    "\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Try to hist dec dependence of single run\n",
    "# As expected we have not enough statistic to see anything\n",
    "run = 50\n",
    "start = start_mjd[run]\n",
    "stop = stop_mjd[run]\n",
    "mask = (exp_times >= start) & (exp_times < stop)\n",
    "_sinDec_run = np.sin(exp[\"dec\"][mask])\n",
    "\n",
    "plt.hist(_sinDec_run, range=[-1, 1], bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Spline to rate distribution\n",
    "\n",
    "Choosing spline weights according to scipy.interpolate.UnivariateSpline manual:\n",
    "\n",
    "    If None (default), s = len(w) which should be a good value if 1/w[i] is an estimate of\n",
    "    the standard deviation of y[i].\n",
    "\n",
    "Internally it is doing a weighted least squares fit with $\\sum_i(w_i(y_i-\\text{spl}(x_i)))^2 \\leq s$.\n",
    "We leave $s$ as the default because we have an estimate for the stddevs: $\\sigma_i = \\sqrt{h_i}$.\n",
    "To match the definition of the weights we use:\n",
    "\n",
    "$$\n",
    "    \\frac{1}{w_i} \\stackrel{!}{=} \\sigma_i = \\sqrt{h_i} \\Leftrightarrow w_i \n",
    "                               = \\frac{1}{\\sqrt{h_i}}\n",
    "$$\n",
    "\n",
    "Because we scaled $h_i$ to get the rate in events per s, we need to scale the errors too:\n",
    "\n",
    "$$\n",
    "    \\tilde{h}_i = \\frac{h_i}{s_i} \\Rightarrow \\tilde{\\sigma}_i = \\frac{\\sqrt{h_i}}{s_i} \n",
    "                = \\frac{\\sqrt{s_i\\tilde{h}_i}}{s} \n",
    "                = \\sqrt{\\frac{\\tilde{h}_i}{s_i}} = \\frac{1}{w_i}\n",
    "$$\n",
    "\n",
    "The smoothing condition `s` chooses the support knots based on the weights.\n",
    "Because we have some oscilating pattern due to to seasonal variations (periode ~1yr) a quadratic spline function is not enough.\n",
    "So we choose the next higher order, a cubic spline, which is able to oscilate up and down exatcly once.\n",
    "\n",
    "**Note:** If a weight is zero, the corresponding point doesn't contribute at all.\n",
    "So we might consider using $w_i = \\sigma_i$ instead.\n",
    "Then point woth high poisson statsitics are preferred over low statistic bins.\n",
    "It doesn't seem to make a huge difference though.\n",
    "\n",
    "Below we try both weights and the unweighted case.\n",
    "For the 'correctly' weighted case with $w_i = 1. / \\sigma_i$ the spline oscillates strongly.\n",
    "So we better try a true perdiodic function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# h is already scaled, so we need to scale the errors too\n",
    "yerr = np.sqrt(h) / np.sqrt((stop_mjd - start_mjd) * secinday)\n",
    "w = 1. / yerr\n",
    "rate_spline = sci.UnivariateSpline(binmids, h, k=3, w=w,\n",
    "                                   s=None, ext=\"extrapolate\")\n",
    "\n",
    "rate_spline_inv = sci.UnivariateSpline(binmids, h, k=3, w=1. / w,\n",
    "                                       s=None, ext=\"extrapolate\")\n",
    "\n",
    "rate_spline_unw = sci.UnivariateSpline(binmids, h, k=3, w=None,\n",
    "                                       s=None, ext=\"extrapolate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Plot runs\n",
    "xerr = 0.5 * (stop_mjd - start_mjd)\n",
    "plt.errorbar(binmids, h, xerr=0, yerr=yerr, fmt=\",\")\n",
    "plt.ylim(0, None);\n",
    "\n",
    "# Plot spline\n",
    "x = np.linspace(start_mjd[0], stop_mjd[-1], 200)\n",
    "y = rate_spline(x)\n",
    "plt.plot(x, y, zorder=5, lw=2, color=\"k\", label=\"w=1/std\")\n",
    "\n",
    "# Plot weighted average. Weights are variance to resemble stddev weighted\n",
    "# least squares fit\n",
    "avg = np.average(h, weights=yerr**2)\n",
    "plt.axhline(avg, 0, 1, color=\"k\", ls=\"--\", zorder=5)\n",
    "\n",
    "# Plot unweighted mean and spline\n",
    "y = rate_spline_unw(x)\n",
    "plt.plot(x, y, zorder=5, lw=2, color=\"r\")\n",
    "\n",
    "avg = np.mean(h)\n",
    "plt.axhline(avg, 0, 1, color=\"r\", ls=\"--\", zorder=5, label=\"w=1\")\n",
    "\n",
    "# Plot with inverse weights\n",
    "y = rate_spline_inv(x)\n",
    "plt.plot(x, y, zorder=5, lw=2, color=\"g\", label=\"w=std\")\n",
    "avg = np.average(h, weights=1. / yerr**2)\n",
    "plt.axhline(avg, 0, 1, color=\"g\", ls=\"--\", zorder=5)\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"splines.png\", dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Periodic function fit\n",
    "\n",
    "Try a peridoc function with a weighted least squares fit.\n",
    "\n",
    "$$\n",
    "    f(x) = a\\cdot \\sin(b\\cdot(x - c)) + d\n",
    "$$\n",
    "\n",
    "The least squares loss function is\n",
    "\n",
    "$$\n",
    "    R = \\sum_i (w_i(y_i - f(x_i)))^2\n",
    "$$\n",
    "\n",
    "Weights are standard deviations from poisson histogram error.\n",
    "\n",
    "$$\n",
    "    w_i = \\frac{1}{\\sigma_i}\n",
    "$$\n",
    "\n",
    "Seed values are estimated from plot rate vs time.\n",
    "Period should be 365 days (MJD) because we have one year of data so we choose $b0 = 2\\pi/365$.\n",
    "Amplitude is about $a_0=-0.0005$, because sinus seems to start with negative values.\n",
    "The x-offset is choose as the first start date, to get the right order of magnitude.\n",
    "The y-axis intersection $d$ schould be close to the weighted average, so we take this as a seed.\n",
    "\n",
    "The bounds are motivated as follows (and if we don't hit them, it's OK to use them).\n",
    "Amplitude $a$ should be positive, this also resolves a degenracy between a-axis offset.\n",
    "The period $b$ should scatter around one year, a period larger than +-1 half a year is unphysical.\n",
    "The x-offset $c$ cannot be greater than the initial +- the period because we have a periodic function.\n",
    "The y-axis offset $d$ is arbitrarily constrained, but as seen from the plot it should not exceed 0.1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def f(x, args):\n",
    "    a, b, c, d = args\n",
    "    return a * np.sin(b * (x - c)) + d\n",
    "\n",
    "def lstsq(pars, *args):\n",
    "    \"\"\"\n",
    "    Weighted leastsquares min sum((wi * (yi - fi))**2)\n",
    "    \"\"\"\n",
    "    # data x,y-values and weights are fixed\n",
    "    x, y, w = args[0], args[1], args[2]\n",
    "    # Params get fitted\n",
    "    a, b, c, d = pars[0], pars[1], pars[2], pars[3]\n",
    "    # Target function\n",
    "    f = a * np.sin(b * (x - c)) + d\n",
    "    # Least squares loss\n",
    "    return np.sum((w * (y - f))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Seed values from consideration above.\n",
    "a0 = -0.0005\n",
    "b0 = 2. * np.pi / 365.\n",
    "c0 = np.amin(start_mjd)\n",
    "d0 = np.average(h, weights=yerr**2)\n",
    "\n",
    "x0 = [a0, b0, c0, d0]\n",
    "# Bounds as explained above\n",
    "bounds = [[None, None], [0.5 * b0, 1.5 * b0], [c0 - b0, c0 + b0, ], [0, 0.01]]\n",
    "# x, y values, weights\n",
    "args = (binmids, h, 1. / yerr)\n",
    "\n",
    "res = sco.minimize(fun=lstsq, x0=x0, args=args, bounds=bounds)\n",
    "\n",
    "for i, name in enumerate([\"Amplitude a\", \"Period b\", \"x-Shift c\", \"y-axis d\"]):\n",
    "    print(name, \" : \", res.x[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Plot runs\n",
    "xerr = 0.5 * (stop_mjd - start_mjd)\n",
    "plt.errorbar(binmids, h, xerr=0, yerr=yerr, fmt=\",\")\n",
    "plt.ylim(0, None);\n",
    "\n",
    "# Plot fit\n",
    "pars = res.x\n",
    "x = np.linspace(start_mjd[0], stop_mjd[-1], 1000)\n",
    "y = f(x, pars)\n",
    "plt.plot(x, y, zorder=5)\n",
    "\n",
    "# Plot y shift dashed to see baseline or years average\n",
    "plt.axhline(res.x[3], 0, 1, color=\"C1\", ls=\"--\")\n",
    "\n",
    "plt.xlim(start_mjd[0], stop_mjd[-1])\n",
    "\n",
    "plt.savefig(\"sinfit.png\", dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Combine both to make a time-dec rate function\n",
    "\n",
    "Multiply the rate function of time with the pdf in sinDec.\n",
    "This gives the rate per solid angle.\n",
    "Integrated over the whole sphere, we recover the total rate that time.\n",
    "Integrating further over the whole time range, regarding the deadtimes of the detector, we recover the number of total events in all runs in this sample.\n",
    "We can approximate this by using the fitted y-axis offset, which is approximatly the mean and multiply with the livetitme.\n",
    "We recover the number of total events to good approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(\"Number of events from approx : \", res.x[3] * livetime * secinday)\n",
    "print(\"True number of events        : \", len(exp))\n",
    "\n",
    "# Simply integrating doesn't respect the downtimes\n",
    "wrong = scint.quad(f, start_mjd[0], stop_mjd[-1], args=res.x)[0] * secinday\n",
    "print(\"Integrating over whole year  : \", wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Function of time, sinDec and right-ascension to get the rate at that point.\n",
    "def time_sinDec_rate(sinDec, t):\n",
    "    return sinDec_pdf(sinDec) * f(t, res.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# This should yield ~1. The ratio of the fitted average d and the integral\n",
    "# of the rate function over the whole sky at a time approximately at rate=d\n",
    "_i = 2. * np.pi * scint.quad(time_sinDec_rate, -1, 1, args=55700)[0] / res.x[3]\n",
    "print(\"1D and mukltiply by 2pi : \",_i)\n",
    "\n",
    "# We can also use a 2D integrator to integrate RA as well (same result)\n",
    "def fullsky_rate(ra, sinDec, t):\n",
    "    return sinDec_pdf(sinDec) * f(t, res.x)\n",
    "_i = scint.dblquad(fullsky_rate, -1, 1, lambda x: 0, lambda x: 2.*np.pi,\n",
    "                   args=(55700,))[0] / res.x[3]\n",
    "print(\"2D over dec and ra      : \", _i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Time PDF ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Background in uniformly distributed in the time window.\n",
    "Signal distribtution is falling off gaussian-like at both edges so normalization is different.\n",
    "So the ratio S/B is simply the the signal pdf divided by the uniform normalization $1 / (t_1 - t_0)$ in the time frame.\n",
    "\n",
    "To get finite support we truncate the gaussian edges at n sigma.\n",
    "Though arbitrarliy introducet to smoothly run to zero, the concrete cutoff of the doesn't really matter (so say 4, 5, 6 sigma, etc).\n",
    "This is because in the LLH we get the product of $\\langle b_B \\rangle B_i$.\n",
    "A larger cutoff make the normalization of the BG pdf larger, but in the same time makes the number of expected BG event get higher in the same linear fashion.\n",
    "So as long as we choose a cutoff which ensures that $S \\approx 0$ outside, we're good to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "secinday = 24. * 60. * 60.\n",
    "\n",
    "def time_bg_pdf(t, t0, a, b):\n",
    "    \"\"\"\n",
    "    BG is uniform for t in [t0 + a, t0 + b] and 0 outside.\n",
    "    \n",
    "    Times t and t0 are given in MJD, the range is given relative to t0\n",
    "    in seconds. t are the times we return pdf values for, t0 is the time of\n",
    "    the source event around which the time frame is defined.\n",
    "    \n",
    "    The PDF is normed to time in seconds!\n",
    "    \"\"\"\n",
    "    # Normalize relative to t0 in seconds (first multiply avoids rounding?)\n",
    "    _t = t * secinday - t0 * secinday\n",
    "  \n",
    "    pdf = np.zeros_like(_t, dtype=np.float)\n",
    "    uni = (_t >= a) & (_t <= b)\n",
    "    pdf[uni] = 1. / (b - a)\n",
    "    return pdf\n",
    "\n",
    "def time_sig_pdf(t, t0, dt, nsig=4):\n",
    "    \"\"\"\n",
    "    Signal falls of with gaussian with sigma = dt outside uniform range dt.\n",
    "    \n",
    "    Times t, t0 are in MJD, dt is in seconds.\n",
    "    t are the times we return pdf values for, t0 is the time of the source\n",
    "    event around which the time frame is defined.\n",
    "    dt is the time window starting from t0 in which signal is uniform.\n",
    "    \n",
    "    The PDF is normed to time in seconds!\n",
    "    \"\"\"\n",
    "    if dt < 0:\n",
    "        raise ValueError(\"dt must not be negative.\")\n",
    "\n",
    "    # Normalize relative to t0 in seconds (first multiply avoids rounding?)\n",
    "    _t = t * secinday - t0 * secinday\n",
    "    \n",
    "    # Constrain sig_t to [2, 30]s regardless of uniform time window\n",
    "    sig_t = np.clip(dt, 2, 30)\n",
    "    sig_t_clip = nsig * sig_t\n",
    "    gaus_norm = (np.sqrt(2 * np.pi) * sig_t)\n",
    "    \n",
    "    # Split in def regions gaus rising, uniform, gaus falling and zero\n",
    "    gr = (_t < 0) & (_t >= -sig_t_clip)\n",
    "    gf = (_t > dt) & (_t <= dt + sig_t_clip)\n",
    "    uni = (_t >= 0) & (_t <= dt)\n",
    "    \n",
    "    pdf = np.zeros_like(t, dtype=np.float)\n",
    "    pdf[gr] = scs.norm.pdf(_t[gr], loc=0, scale=sig_t)\n",
    "    pdf[gf] = scs.norm.pdf(_t[gf], loc=dt, scale=sig_t)\n",
    "    # Connect smoothly with the gaussians\n",
    "    pdf[uni] = 1. / gaus_norm\n",
    "    \n",
    "    # Normalize whole distribtuion\n",
    "    dcdf = (scs.norm.cdf(dt + sig_t_clip, loc=dt, scale=sig_t) -\n",
    "            scs.norm.cdf(-sig_t_clip, loc=0., scale=sig_t))\n",
    "    norm = dcdf + dt / gaus_norm\n",
    "    \n",
    "    return pdf / norm\n",
    "\n",
    "def time_soverb(t, t0, dt, nsig):\n",
    "    \"\"\"\n",
    "    Time signal over background PDF.\n",
    "    \n",
    "    Signal and background PDFs are each normalized over seconds.\n",
    "    Signal PDF has gaussian edges to smoothly let it fall of to zero, the\n",
    "    stddev is dt when dt is in [2, 30]s, otherwise the nearest edge.\n",
    "\n",
    "    To ensure finite support, the edges are truncated after nsig * dt.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    t : array-like\n",
    "        Times given in MJD for which we want to evaluate the ratio.\n",
    "    t0 : float\n",
    "        Time of the source event.\n",
    "    dt : float\n",
    "        Time window in seconds starting from t0 in which the signal pdf is\n",
    "        assumed to be uniform. Must not be negative.\n",
    "    nsig : float\n",
    "        Clip the gaussian edges at nsig * dt\n",
    "    \"\"\"\n",
    "    if dt < 0:\n",
    "        raise ValueError(\"dt must not be negative.\")\n",
    "\n",
    "    secinday = 24. * 60. * 60.\n",
    "\n",
    "    # Normalize relative to t0 in seconds (first multiply avoids rounding?)\n",
    "    _t = t * secinday - t0 * secinday\n",
    "   \n",
    "    # Create signal PDF\n",
    "    # Constrain sig_t to [2, 30]s regardless of uniform time window\n",
    "    sig_t = np.clip(dt, 2, 30)\n",
    "    sig_t_clip = nsig * sig_t\n",
    "    gaus_norm = (np.sqrt(2 * np.pi) * sig_t)\n",
    "    \n",
    "    # Split in def regions gaus rising, uniform, gaus falling\n",
    "    gr = (_t < 0) & (_t >= -sig_t_clip)\n",
    "    gf = (_t > dt) & (_t <= dt + sig_t_clip)\n",
    "    uni = (_t >= 0) & (_t <= dt)\n",
    "    \n",
    "    pdf = np.zeros_like(t, dtype=np.float)\n",
    "    pdf[gr] = scs.norm.pdf(_t[gr], loc=0, scale=sig_t)\n",
    "    pdf[gf] = scs.norm.pdf(_t[gf], loc=dt, scale=sig_t)\n",
    "    # Connect smoothly with the gaussians\n",
    "    pdf[uni] = 1. / gaus_norm\n",
    "    \n",
    "    # Normalize signal distribtuion\n",
    "    dcdf = (scs.norm.cdf(dt + sig_t_clip, loc=dt, scale=sig_t) -\n",
    "            scs.norm.cdf(-sig_t_clip, loc=0., scale=sig_t))\n",
    "    norm = dcdf + dt / gaus_norm\n",
    "    pdf /= norm\n",
    "    \n",
    "    # Calculate the ratio\n",
    "    bg_pdf = 1. / (dt + 2 * sig_t_clip)\n",
    "    ratio = pdf / bg_pdf\n",
    "    return ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Arbitrary start date from data\n",
    "t0 = start_mjd[100]\n",
    "t0_sec = t0 * secinday\n",
    "\n",
    "# dt from t0 in seconds, clip at 4 sigma\n",
    "dt = 0\n",
    "nsig = 4.\n",
    "\n",
    "# Make t values for plotting in MJD around t0\n",
    "clip = np.clip(dt, 2, 30) * nsig\n",
    "plt_rng = [-clip, dt + clip]\n",
    "t = np.linspace(t0_sec + plt_rng[0], t0_sec + plt_rng[1], 200) / secinday\n",
    "\n",
    "bg_pdf = time_bg_pdf(t, t0, -clip, dt + clip)\n",
    "sig_pdf = time_sig_pdf(t, t0, dt, nsig)\n",
    "\n",
    "# Plot in normalized time\n",
    "_t = t * secinday - t0 * secinday\n",
    "plt.plot(_t, bg_pdf, \"C0-\")\n",
    "plt.plot(_t, sig_pdf, \"C1-\")\n",
    "plt.axvline(dt, 0, 1, color=\"C7\", ls=\"--\")\n",
    "plt.axvline(0, 0, 1, color=\"C1\", ls=\"--\")\n",
    "\n",
    "plt.xlabel(\"Time relative to t0 in sec\")\n",
    "plt.ylim(0, None);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Integrate both pdf over time range to show they are correctly normalized\n",
    "# Note that PDFs are defined in second so we multiply by secinday \n",
    "bg_int = scint.quad(time_bg_pdf, t[0], t[-1],\n",
    "                    args=(t0, -clip, dt + clip))[0] * secinday\n",
    "sig_int = scint.quad(time_sig_pdf, t[0], t[-1],\n",
    "                    args=(t0, dt, nsig))[0] * secinday\n",
    "\n",
    "print(\"BG integral     : \", bg_int)\n",
    "print(\"Signal integral : \", sig_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Make a plot with ratios for different time windows as in the paper\n",
    "# Arbitrary start date from data\n",
    "t0 = start_mjd[100]\n",
    "t0_sec = t0 * secinday\n",
    "\n",
    "# dt from t0 in seconds, clip at 4 sigma\n",
    "dts = [5, 50, 200]\n",
    "nsig = 4\n",
    "\n",
    "# Make t values for plotting in MJD around t0, fitting all in one plot\n",
    "max_dt = np.amax(dts)\n",
    "clip = np.clip(max_dt, 2, 30) * nsig\n",
    "plt_rng = np.array([-clip, max_dt + clip])\n",
    "t = np.linspace(t0_sec + 1.2 *plt_rng[0],\n",
    "                t0_sec + 1.2 * plt_rng[1], 1000) / secinday\n",
    "_t = t * secinday - t0 * secinday\n",
    "\n",
    "# Mark event time\n",
    "plt.axvline(0, 0, 1, c=\"#353132\", ls=\"--\", lw=2)\n",
    "\n",
    "colors = [\"C0\", \"C3\", \"C2\"]\n",
    "for i, dt in enumerate(dts):\n",
    "    # Plot ratio S/B\n",
    "    SoB = time_soverb(t, t0, dt, nsig)\n",
    "    plt.plot(_t, SoB, lw=2, c=colors[i],\n",
    "             label=r\"$T_\\mathrm{{uni}}$: {:>3d}s\".format(dt))\n",
    "    # Fill uniform part, might look nicely\n",
    "    # fbtw = (_t > 0) & (_t < dt)\n",
    "    # plt.fill_between(_t[fbtw], 0, SoB[fbtw], color=\"C7\", alpha=0.1)\n",
    "\n",
    "# Make it look like the paper plot, but with slightly extended borders, to\n",
    "# nothing breaks outside the total time frame\n",
    "plt.xlim(1.2 * plt_rng)\n",
    "plt.ylim(0, 3)\n",
    "plt.xlabel(\"t - t0 in sec\")\n",
    "plt.ylabel(\"S / B\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's make the BG pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Marginalize KDE by integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Instead of sampling and reducing to 2D histograms, we can try to truly integrate one dimension of the KDE to be able to plot also the tails of the distribution, where events usually end up only in large samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# KDE CV is running on cluster and pickles the GridSearchCV\n",
    "fname = \"./kde_cv/KDE_model_selector_20_exp_IC86_I_followup_2nd_pass.pickle\"\n",
    "with open(fname, \"rb\") as f:\n",
    "    model_selector = pickle.load(f)\n",
    "\n",
    "kde = model_selector.best_estimator_\n",
    "bw = model_selector.best_params_[\"bandwidth\"]\n",
    "print(\"Best bandwidth : {:.3f}\".format(bw))\n",
    "\n",
    "# We maybe just want to stick with the slightly overfitting kernel to\n",
    "# be as close as possible to data\n",
    "OVERFIT = True\n",
    "if OVERFIT:\n",
    "    bw = 0.1\n",
    "    kde = skn.KernelDensity(bandwidth=bw, kernel=\"gaussian\", rtol=1e-8)\n",
    "print(\"Used bandwidth : {:.3f}\".format(bw))\n",
    "\n",
    "# KDE sample must be cut in sigma before fitting, similar to range in hist\n",
    "_exp = exp[exp[\"sigma\"] <= np.deg2rad(5)]\n",
    "\n",
    "fac_logE = 1.5\n",
    "fac_dec = 2.5\n",
    "fac_sigma = 2.\n",
    "\n",
    "_logE = fac_logE * _exp[\"logE\"]\n",
    "_sigma = fac_sigma * np.rad2deg(_exp[\"sigma\"])\n",
    "_dec = fac_dec * _exp[\"dec\"]\n",
    "\n",
    "kde_sample = np.vstack((_logE, _dec, _sigma)).T\n",
    "\n",
    "# Fit KDE best model to sample\n",
    "kde.fit(kde_sample)\n",
    "\n",
    "# Make some samples\n",
    "nsamples_kde = int(1e7)\n",
    "bg_samples = kde.sample(n_samples=nsamples_kde)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 1D case\n",
    "\n",
    "Integrate out 2 axis with a double integral to show a 1D margin distribution.\n",
    "This take super long, 5 Minutes per point.\n",
    "But it can be parallelized pretty simple if needed.\n",
    "Code to create the values is on phobos.\n",
    "\n",
    "**Resumee:** Sampling many values and simply bin in 1D is much better, needs less time and is probably more accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Compare to data hist\n",
    "_ = plt.hist(exp[\"logE\"] * fac_logE, bins=100, normed=True, label=\"data\")\n",
    "\n",
    "# Compare 'true' integration with\n",
    "h, b = np.histogram(bg_samples[:, 0], bins=200, range=[2, 10], normed=True)\n",
    "m = 0.5 * (b[:-1] + b[1:])\n",
    "_ = plt.plot(m, h, label=\"sample\")\n",
    "\n",
    "bins_and_vals = np.load(\"data/2d_integrate_kde/bins_and_vals.npy\")\n",
    "x = bins_and_vals[0]\n",
    "vals = bins_and_vals[1]\n",
    "_ = plt.plot(x, vals, label=\"integrated\")\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig(\"./kde.png\", dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 2D PDF\n",
    "\n",
    "Integrate out only one axis (here sigma) to show the 2D marginalized PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Choose axis in which the PDF is differential\n",
    "xax = 0  # logE\n",
    "yax = 1  # dec\n",
    "xrng = np.array([2, 7]) * fac_logE\n",
    "yrng = np.array([-np.pi / 2., np.pi / 2.]) * fac_dec\n",
    "\n",
    "# Integrate range\n",
    "irng = np.array([0, 5]) * fac_sigma\n",
    "\n",
    "# Create the integration grid on the bin mids\n",
    "nbinsx, nbinsy = 10, 10\n",
    "xbins = np.linspace(2, 10, nbinsx + 1)\n",
    "ybins = np.linspace(-np.pi, np.pi, nbinsy + 1)\n",
    "x, y = get_binmids([xbins, ybins])\n",
    "xx, yy = map(np.ravel, np.meshgrid(x, y))\n",
    "\n",
    "# Scans x line by x line with incresing y\n",
    "grid_pts = np.vstack((xx, yy)).T\n",
    "\n",
    "def pdf(x, *args):\n",
    "    # Make single to point to evaluate the KDE at\n",
    "    xgridpt, ygridpt = args\n",
    "    pt = [xgridpt, ygridpt, x]\n",
    "    pt = np.array(pt)[np.newaxis, :]\n",
    "    \n",
    "    return np.exp(kde.score_samples(X=pt))\n",
    "    \n",
    "# Just integrate over last axis at every gridpoint\n",
    "integrals = np.zeros(len(grid_pts))\n",
    "for i, gp in tqdm(enumerate(grid_pts)):\n",
    "    intgrl = scint.quad(pdf, irng[0], irng[1], args=(gp[0], gp[1]))\n",
    "    integrals[i] = intgrl[0]\n",
    "    \n",
    "    \n",
    "# Compare hist from sample to integration in the same bins\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "_, _, _, img0 = ax[0].hist2d(bg_samples[:, xax], bg_samples[:, yax],\n",
    "                             bins=[xbins, ybins], normed=True)\n",
    "\n",
    "# Don't multiply with binwidth. The midpoint is already the \"average\".\n",
    "# A better estimation would be to integrate over the bin and divide by the\n",
    "# binwidths, but it would yield a similar result.\n",
    "_, _, _, img1 = ax[1].hist2d(xx, yy, bins=[xbins, ybins],\n",
    "                             weights=integrals)\n",
    "\n",
    "cax0 = split_axis(ax[0], \"right\", cbar=True)\n",
    "cax1 = split_axis(ax[1], \"right\", cbar=True)\n",
    "\n",
    "plt.colorbar(cax=cax0, mappable=img0)\n",
    "plt.colorbar(cax=cax1, mappable=img1)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Justify the sigma cut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Only few higher energy events from the sothern sky are excluded (see cut=10).\n",
    "But really bad reconstructed events tend to have higher energies (see cut=90).\n",
    "Still it should be OK to remove those > 10 because they have not so much spatial information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Show the leftover event s after a sigma cut\n",
    "sig_cut = 10\n",
    "m = exp[\"sigma\"] > np.deg2rad(sig_cut)\n",
    "\n",
    "_ = plt.hist2d(exp[\"logE\"][m], np.rad2deg(exp[\"dec\"][m]),\n",
    "               bins=30, cmap=\"inferno\")\n",
    "plt.colorbar()\n",
    "plt.title(\"Total Evts w sigma > {:d}°: {:d} ({:.3f}%)\".format(\n",
    "        sig_cut, np.sum(m), np.sum(m) / len(exp) * 100))\n",
    "plt.xlabel(\"logE\")\n",
    "plt.ylabel(\"dec in °\")\n",
    "plt.show()\n",
    "\n",
    "# Show the skewed sigma distribution with the cut applied and mean vs median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Test the marginalize_hist method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "It should be equivalent to use one of the following methods to create a 1D histogram from the original 3D data pdf in logE, dec and sigma:\n",
    "\n",
    "1. Simply use the original 1D data in any variable, e.g. simply histogram logE\n",
    "2. Create the complete 3D histogram and marginalize by summing over remaining dimensions.\n",
    "\n",
    "When using unnormalized hists, 2. is simply summing up all other counts.\n",
    "\n",
    "When using normalized hists, we need to sum with respect to the binwidths in the current dimension to keep the normalization intact.\n",
    "This is only useful, when only the histogram is available and not the original sample.\n",
    "\n",
    "We want to compare if both methods are equivalent\n",
    "As we can see, all ratios are one, so methods are equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def make_hist_ratio(h1, h2):\n",
    "    \"\"\"Return the ratio h1 / h2. Return 0 where h2 is 0.\"\"\"\n",
    "    m = (h2 > 0)\n",
    "    ratio = np.zeros_like(h1)\n",
    "    ratio[m] = h1[m] / h2[m]\n",
    "    return ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Unnormalized\n",
    "First the unnormalized version. Simply sum over the other axes of the 3D hist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Plot each variable in a single plot and the ratios seperately\n",
    "fig, [[axtl, axtr], [axbl, axbr]] = plt.subplots(2, 2, figsize=(10, 8))\n",
    "\n",
    "# We also make a cut < 10° in sigma, because there are some outliers\n",
    "m = exp[\"sigma\"] <= np.deg2rad(10)\n",
    "sigma = np.rad2deg(exp[\"sigma\"][m])\n",
    "logE = exp[\"logE\"][m]\n",
    "dec = np.sin(exp[\"dec\"][m])\n",
    "\n",
    "logE_nbins = 50\n",
    "dec_nbins = 40\n",
    "sigma_nbins = 30\n",
    "\n",
    "# Make the 3D hist\n",
    "sample = np.vstack((logE, dec, sigma)).T\n",
    "nbins = [logE_nbins, dec_nbins, sigma_nbins]\n",
    "h, b = np.histogramdd(sample, bins=nbins,)\n",
    "\n",
    "# Get binmids for plotting\n",
    "m = get_binmids(b)\n",
    "\n",
    "# Common hist settings\n",
    "h1 = {\"lw\": 2, \"color\": \"k\", \"histtype\": \"step\"}\n",
    "h2 = {\"lw\": 2, \"color\": \"r\", \"histtype\": \"step\", \"alpha\": 0.5}\n",
    "\n",
    "# logE\n",
    "logE_h, logE_b, _ = axtl.hist(logE, bins=logE_nbins, **h1)\n",
    "logE_hm = np.sum(h, axis=(1, 2))\n",
    "_ = axtl.hist(m[0], bins=b[0], weights=logE_hm, **h2)\n",
    "# Ratio plot below\n",
    "axtl_sec = split_axis(axtl, \"bottom\", \"20%\", cbar=False)\n",
    "axtl_sec.hist(m[0], b[0], weights=make_hist_ratio(logE_h, logE_hm), **h2)\n",
    "axtl_sec.axhline(1, 0, 1, color=\"k\")\n",
    "axtl_sec.set_ylim(0, 2)\n",
    "\n",
    "# dec\n",
    "dec_h, dec_b, _ = axbl.hist(dec, bins=dec_nbins, **h1)\n",
    "dec_hm = np.sum(h, axis=(0, 2))\n",
    "_ = axbl.hist(m[1], bins=b[1], weights=dec_hm, **h2)\n",
    "\n",
    "axbl_sec = split_axis(axbl, \"bottom\", \"20%\", cbar=None)\n",
    "axbl_sec.hist(m[1], b[1], weights=make_hist_ratio(dec_h, dec_hm), **h2)\n",
    "axbl_sec.axhline(1, 0, 1, color=\"k\")\n",
    "axbl_sec.set_ylim(0, 2)\n",
    "\n",
    "# sigma\n",
    "sigma_h, sigma_b, _ = axtr.hist(sigma, bins=sigma_nbins, **h1)\n",
    "sigma_hm = np.sum(h, axis=(0, 1))\n",
    "_ = axtr.hist(m[2], bins=b[2], weights=sigma_hm, **h2)\n",
    "\n",
    "axtr_sec = split_axis(axtr, \"bottom\", \"20%\", cbar=None)\n",
    "axtr_sec.hist(m[2], b[2], weights=make_hist_ratio(sigma_h, sigma_hm), **h2)\n",
    "axtr_sec.axhline(1, 0, 1, color=\"k\")\n",
    "axtr_sec.set_ylim(0, 2)\n",
    "\n",
    "axbr.set_visible(False)\n",
    "\n",
    "fig.suptitle(\"Black: 1D, Red: Margin\", fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Normalized\n",
    "Sum over the other axes of the 3D hist and multiply by bin widths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Plot each variable in a single plot and the ratios seperately\n",
    "fig, [[axtl, axtr], [axbl, axbr]] = plt.subplots(2, 2, figsize=(10, 8))\n",
    "\n",
    "# Now make it normed\n",
    "h, b = np.histogramdd(sample, bins=nbins, normed=True)\n",
    "\n",
    "# Get binmids for plotting\n",
    "m = get_binmids(b)\n",
    "\n",
    "# logE\n",
    "logE_h, logE_b, _ = axtl.hist(logE, bins=logE_nbins, normed=True, **h1)\n",
    "logE_hm = hist_marginalize(h=h, bins=b, axes=(1, 2))[0]\n",
    "_ = axtl.hist(m[0], bins=b[0], weights=logE_hm, **h2)\n",
    "# Ratio plot below\n",
    "axtl_sec = split_axis(axtl, \"bottom\", \"20%\", cbar=False)\n",
    "axtl_sec.hist(m[0], b[0], weights=make_hist_ratio(logE_h, logE_hm), **h2)\n",
    "axtl_sec.axhline(1, 0, 1, color=\"k\")\n",
    "axtl_sec.set_ylim(0, 2)\n",
    "\n",
    "# dec\n",
    "dec_h, dec_b, _ = axbl.hist(dec, bins=dec_nbins, normed=True, **h1)\n",
    "dec_hm = hist_marginalize(h=h, bins=b, axes=(0, 2))[0]\n",
    "_ = axbl.hist(m[1], bins=b[1], weights=dec_hm, **h2)\n",
    "\n",
    "axbl_sec = split_axis(axbl, \"bottom\", \"20%\", cbar=None)\n",
    "axbl_sec.hist(m[1], b[1], weights=make_hist_ratio(dec_h, dec_hm), **h2)\n",
    "axbl_sec.axhline(1, 0, 1, color=\"k\")\n",
    "axbl_sec.set_ylim(0, 2)\n",
    "\n",
    "# sigma\n",
    "sigma_h, sigma_b, _ = axtr.hist(sigma, bins=sigma_nbins, normed=True, **h1)\n",
    "sigma_hm = hist_marginalize(h=h, bins=b, axes=(0, 1))[0]\n",
    "_ = axtr.hist(m[2], bins=b[2], weights=sigma_hm, **h2)\n",
    "\n",
    "axtr_sec = split_axis(axtr, \"bottom\", \"20%\", cbar=None)\n",
    "axtr_sec.hist(m[2], b[2], weights=make_hist_ratio(sigma_h, sigma_hm), **h2)\n",
    "axtr_sec.axhline(1, 0, 1, color=\"k\")\n",
    "axtr_sec.set_ylim(0, 2)\n",
    "\n",
    "axbr.set_visible(False)\n",
    "\n",
    "fig.suptitle(\"Black: 1D, Red: Margin\", fontsize=15);"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
